"""
Jetson Nano ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ ë²¤ì¹˜ë§ˆí¬
- Base Model (ì›ë³¸)
- Base Model + FP16 ì–‘ìí™”
- Base Model + Pruning
- Base Model + Pruning + Fine-tuning
- Base Model + Pruning + Fine-tuning + FP16
"""

import os
import gc
import json
import time
import torch
import numpy as np
import matplotlib.pyplot as plt
import matplotlib
import psutil
import sys
import warnings
warnings.filterwarnings("ignore")

# â˜… ì˜ë¬¸ í°íŠ¸ë§Œ ì‚¬ìš© (í•œê¸€ ê¹¨ì§ ë°©ì§€)
matplotlib.rcParams['font.family'] = 'DejaVu Sans'
matplotlib.rcParams['axes.unicode_minus'] = False

# í”„ë¡œì íŠ¸ ëª¨ë“ˆ
from src.utils.memory_safe_import import load_model_class
from src.utils.model_utils import get_model_size_mb

# ============================================================================
# ì„¤ì •
# ============================================================================
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
NUM_RUNS = 50  # ê° ëª¨ë¸ë‹¹ ì¶”ë¡  íšŸìˆ˜
WARMUP_RUNS = 5  # ì›Œë°ì—… íšŸìˆ˜
# ============================================================================
# í™˜ê²½ ì„¤ì • (CRITICAL - í¬ë˜ì‹œ ë°©ì§€)
# ============================================================================
print("âš™ï¸  í™˜ê²½ ì„¤ì • ì¤‘...", file=sys.stderr)
torch.backends.cudnn.enabled = False  # ë¶ˆì•ˆì •ì„± ë°©ì§€
torch.backends.cudnn.benchmark = True # ì…ë ¥ í¬ê¸°ê°€ ê³ ì •(224x224)ì´ë¯€ë¡œ í•„ìˆ˜

# CPU/GPU ë””ë°”ì´ìŠ¤ ìë™ ê°ì§€ ë° ê°•ì œ ì„¤ì •
if torch.cuda.is_available():
    device = torch.device("cuda")
    print("ğŸš€ ë””ë°”ì´ìŠ¤: GPU (NVIDIA Maxwell) ê°€ì† ëª¨ë“œ", file=sys.stderr)
else:
    device = torch.device("cpu")
    print("ğŸ“ ë””ë°”ì´ìŠ¤: CPU (ê²½ê³ : ì„±ëŠ¥ì´ ë‚®ì„ ìˆ˜ ìˆìŒ)", file=sys.stderr)

# ìŠ¤ë ˆë“œ ìµœì í™”
torch.set_num_threads(4)
torch.set_num_interop_threads(4)

sys.modules['numpy._core'] = np.core
sys.modules['numpy._core.multiarray'] = np.core.multiarray
dtypes = torch.float32

print("ğŸ“Š Jetson Nano ëª¨ë¸ ë¹„êµ ë²¤ì¹˜ë§ˆí¬")
print("=" * 70)
print("ë””ë°”ì´ìŠ¤: {}".format(device))
print("=" * 70 + "\n")

# ============================================================================
# ëª¨ë¸ ê²½ë¡œ ì„¤ì •
# ============================================================================
MODELS_CONFIG = {
    'Base Model': {
        'path': 'models/lightweight_captioning_model.pth',
        'quantize': False,
        'pruned': False,
        'finetuned': False,
    },
    'Base Model + FP16': {
        'path': 'models/lightweight_captioning_model.pth',
        'quantize': True,
        'pruned': False,
        'finetuned': False,
    },
    'Base Model + Pruning': {
        'path': 'pruning_results/Pruning_epoch_1_checkpoint.pt',
        'quantize': False,
        'pruned': True,
        'finetuned': False,
    },
    'Base Model + Pruning + FT': {
        'path': 'pruning_results/Pruning_epoch_1_checkpoint.pt',
        'quantize': False,
        'pruned': True,
        'finetuned': True,
    },
    'Base Model + Pruning + FT + FP16': {
        'path': 'pruning_results/Pruning_epoch_1_checkpoint.pt',
        'quantize': True,
        'pruned': True,
        'finetuned': True,
    },
}

# ============================================================================
# ëª¨ë¸ ë¡œë“œ í•¨ìˆ˜
# ============================================================================
def load_model_with_config(config):
    """ì„¤ì •ì— ë”°ë¼ ëª¨ë¸ ë¡œë“œ"""
    model_path = config['path']
    
    if not os.path.exists(model_path):
        print("âŒ ëª¨ë¸ íŒŒì¼ ì—†ìŒ: {}".format(model_path))
        return None, None, None
    
    try:
        # ëª¨ë¸ í´ë˜ìŠ¤ ë¡œë“œ
        Model = load_model_class()
        
        # ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ
        checkpoint = torch.load(model_path, map_location=device)
        
        if isinstance(checkpoint, dict) and 'model_state_dict' in checkpoint:
            word_map = checkpoint.get('word_map')
            rev_word_map = checkpoint.get('rev_word_map')
            vocab_size = checkpoint.get('vocab_size')
            state_dict = checkpoint['model_state_dict']
            
            # state_dictì—ì„œ ëª¨ë¸ í¬ê¸° ì¶”ì¶œ
            decoder_dim = checkpoint.get('decoder_dim', 512)
            attention_dim = checkpoint.get('attention_dim', 256)
            
            # state_dictì—ì„œ ì‹¤ì œ í¬ê¸° ì¶”ì¶œ
            if 'decoder.decode_step.weight_ih' in state_dict:
                actual_size = state_dict['decoder.decode_step.weight_ih'].shape[0]
                decoder_dim = actual_size // 3
            
            if 'decoder.encoder_att.weight' in state_dict:
                attention_dim = state_dict['decoder.encoder_att.weight'].shape[0]
            
            # ëª¨ë¸ ìƒì„±
            gc.collect()
            model = Model(
                vocab_size=vocab_size,
                embed_dim=300,
                decoder_dim=decoder_dim,
                attention_dim=attention_dim
            )
            del Model
            
            # ê°€ì¤‘ì¹˜ ë¡œë“œ
            try:
                model.load_state_dict(state_dict, strict=True)
            except:
                model.load_state_dict(state_dict, strict=False)
            
            model = model.to(device)
            model.eval()
            
            # â˜… FP16 ì–‘ìí™” ì ìš©
            if config['quantize']:
                model = model.half()
            
            return model, word_map, rev_word_map
        else:
            print("âŒ ì˜ëª»ëœ ëª¨ë¸ íŒŒì¼ í˜•ì‹")
            return None, None, None
            
    except Exception as e:
        print("âŒ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {}".format(e))
        import traceback
        traceback.print_exc()
        return None, None, None

# ============================================================================
# FLOPs ê³„ì‚° í•¨ìˆ˜ (íŒŒë¼ë¯¸í„° ê¸°ë°˜ ì¶”ì •)
# ============================================================================
def calculate_flops(model):
    """
    ëª¨ë¸ì˜ FLOPs ê³„ì‚° (íŒŒë¼ë¯¸í„° ê¸°ë°˜ ì¶”ì •)
    - ì¸ì½”ë”(MobileNetV3): params Ã— 2
    - ë””ì½”ë”(GRU+Attention): params Ã— 2
    """
    try:
        param_count = sum(p.numel() for p in model.parameters())
        # ì…ë ¥: (1, 3, 224, 224)
        # ì¸ì½”ë”: ì•½ 2.5M params â†’ ì•½ 600M FLOPs
        # ë””ì½”ë”(seq_len=50): ì•½ 0.5M params â†’ ì•½ 50M FLOPs
        # ì´í•©: ì•½ 650M FLOPs â‰ˆ 2.0 Ã— params
        estimated_flops = param_count * 2.0 / 1e6  # Millions
        return float(estimated_flops)
    except Exception as e:
        print("FLOPs ê³„ì‚° ì˜¤ë¥˜: {}".format(e))
        return 0.0

# ============================================================================
# ì„±ëŠ¥ ì¸¡ì • í•¨ìˆ˜
# ============================================================================
class BenchmarkMetrics:
    """ëª¨ë¸ ì„±ëŠ¥ ë©”íŠ¸ë¦­"""
    def __init__(self):
        self.inference_times = []
        self.memory_usage = []
        self.process = psutil.Process(os.getpid())
    
    def record_inference(self, inf_time):
        """ì¶”ë¡  ì‹œê°„ ê¸°ë¡"""
        self.inference_times.append(inf_time)
    
    def record_memory(self):
        """ë©”ëª¨ë¦¬ ê¸°ë¡"""
        mem = self.process.memory_info().rss / 1024 / 1024
        self.memory_usage.append(mem)
    
    def get_stats(self):
        """í†µê³„ ê³„ì‚°"""
        if not self.inference_times:
            return None
        
        times = np.array(self.inference_times)
        
        return {
            'mean_latency_ms': float(np.mean(times)),
            'median_latency_ms': float(np.median(times)),
            'min_latency_ms': float(np.min(times)),
            'max_latency_ms': float(np.max(times)),
            'std_latency_ms': float(np.std(times)),
            'cpu_memory_mb': float(np.mean(self.memory_usage) if self.memory_usage else 0),
            'total_params': 0,
            'model_size_mb': 0,
            'flops_millions': 0,
        }

def benchmark_model(model, word_map, rev_word_map, model_name, config):
    """ë‹¨ì¼ ëª¨ë¸ ë²¤ì¹˜ë§ˆí¬"""
    print("\nBenchmarking: {}".format(model_name))
    print("-" * 70)
    
    metrics = BenchmarkMetrics()
    
    # ë”ë¯¸ ì…ë ¥ ìƒì„±
    dummy_input = torch.randn(1, 3, 224, 224).to(device)
    if config['quantize']:
        dummy_input = dummy_input.half()
    
    # ì›Œë°ì—…
    print("  Warming up...", end='')
    with torch.no_grad():
        for _ in range(WARMUP_RUNS):
            _ = model.encoder(dummy_input)
    if device.type == 'cuda':
        torch.cuda.synchronize()
    print(" Done")
    
    # ë³¸ ë²¤ì¹˜ë§ˆí¬
    print("  Running {} iterations...".format(NUM_RUNS))
    
    # ë”ë¯¸ ì´ë¯¸ì§€ ìƒì„±
    dummy_frame = np.random.randint(0, 256, (480, 640, 3), dtype=np.uint8)
    
    with torch.no_grad():
        for i in range(NUM_RUNS):
            # ì „ì²˜ë¦¬
            img = dummy_frame.copy()
            img = np.transpose(img, (2, 0, 1)).astype(np.float32) / 255.0
            img_tensor = torch.from_numpy(img).unsqueeze(0).to(device)
            
            if config['quantize']:
                img_tensor = img_tensor.half()
            
            # ì¶”ë¡ 
            metrics.record_memory()
            start = time.time()
            
            try:
                _ = model.generate(img_tensor, word_map, rev_word_map, max_len=50, device=device)
            except:
                # generateê°€ ì—†ìœ¼ë©´ encoderë§Œ ì‹¤í–‰
                features = model.encoder(img_tensor)
                if hasattr(model, 'decoder'):
                    batch_size = features.size(0)
                    channel = features.size(1)
                    features_flat = features.view(batch_size, channel, -1).permute(0, 2, 1)
            
            if device.type == 'cuda':
                torch.cuda.synchronize()
            
            inference_time = (time.time() - start) * 1000
            metrics.record_inference(inference_time)
            
            if (i + 1) % 10 == 0:
                print("    [{}/{}] Done".format(i + 1, NUM_RUNS))
    
    # í†µê³„ ê³„ì‚°
    stats = metrics.get_stats()
    
    # ëª¨ë¸ í¬ê¸° ì •ë³´
    param_count = sum(p.numel() for p in model.parameters())
    stats['total_params'] = param_count
    stats['model_size_mb'] = get_model_size_mb(model)
    
    # FLOPs ê³„ì‚°
    flops_millions = calculate_flops(model)
    stats['flops_millions'] = flops_millions
    
    print("\n Results:")
    print("    - Latency: {:.2f} ms".format(stats['mean_latency_ms']))
    print("    - Token Time: {:.3f} ms".format(stats['mean_latency_ms'] / 50.0))
    print("    - Memory: {:.1f} MB".format(stats['cpu_memory_mb']))
    print("    - Size: {:.2f} MB".format(stats['model_size_mb']))
    print("    - Params: {:,}".format(param_count))
    print("    - FLOPs: {:.1f}M".format(flops_millions))
    
    return stats

# ============================================================================
# ë©”ì¸ ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰
# ============================================================================
def main():
    results = {}
    
    print("\n" + "=" * 70)
    print("Starting Model Performance Comparison")
    print("=" * 70)
    
    for model_name, config in MODELS_CONFIG.items():
        print("\n\n[{}/{}] {}".format(
            list(MODELS_CONFIG.keys()).index(model_name) + 1,
            len(MODELS_CONFIG),
            model_name
        ))
        
        # ëª¨ë¸ ë¡œë“œ
        model, word_map, rev_word_map = load_model_with_config(config)
        
        if model is None:
            print("Model load failed, skipping")
            continue
        
        try:
            # ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰
            stats = benchmark_model(model, word_map, rev_word_map, model_name, config)
            results[model_name] = stats
            
        except Exception as e:
            print("Benchmark failed: {}".format(e))
            import traceback
            traceback.print_exc()
        
        finally:
            # ë©”ëª¨ë¦¬ ì •ë¦¬
            del model
            gc.collect()
    
    # ê²°ê³¼ ìš”ì•½
    print("\n\n" + "=" * 70)
    print("Benchmark Results Summary")
    print("=" * 70)
    
    for idx, (model_name, stats) in enumerate(results.items()):
        print("\nModel {}:".format(idx + 1))
        print("  - Latency: {:.2f} ms".format(stats['mean_latency_ms']))
        print("  - Token Time: {:.3f} ms".format(stats['mean_latency_ms'] / 50.0))
        print("  - Memory: {:.1f} MB".format(stats['cpu_memory_mb']))
        print("  - Size: {:.2f} MB".format(stats['model_size_mb']))
        print("  - FLOPs: {:.1f}M".format(stats['flops_millions']))
    
    # ê·¸ë˜í”„ ìƒì„±
    print("\n\n Generating graphs...")
    plot_comparison(results)
    
    # ê²°ê³¼ ì €ì¥
    with open('benchmark_results.json', 'w') as f:
        json.dump(results, f, indent=2)
    print("Results saved: benchmark_results.json")

# ============================================================================
# ê·¸ë˜í”„ ìƒì„± í•¨ìˆ˜
# ============================================================================
def plot_comparison(results):
    """ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ ê·¸ë˜í”„"""
    if not results:
        print("Results not found")
        return
    
    model_names = list(results.keys())
    
    # ë°ì´í„° ì¶”ì¶œ
    latencies = [results[m]['mean_latency_ms'] for m in model_names]
    memory_usage = [results[m]['cpu_memory_mb'] for m in model_names]
    model_sizes = [results[m]['model_size_mb'] for m in model_names]
    param_counts = [results[m]['total_params'] / 1e6 for m in model_names]  # Million
    flops_values = [results[m]['flops_millions'] for m in model_names]
    
    # â˜… FPS â†’ Token Timeìœ¼ë¡œ ë³€ê²½ (50 tokens ê¸°ì¤€)
    token_time = [lat / 50.0 for lat in latencies]  # ms per token
    
    # ê·¸ë˜í”„ ìƒì„±
    fig, axes = plt.subplots(2, 3, figsize=(16, 10))
    fig.suptitle('Jetson Nano Model Performance Comparison', fontsize=16, fontweight='bold')
    
    # ìƒ‰ìƒ ì„¤ì •
    colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd']
    
    # 1. ì¶”ë¡  ì§€ì—°ì‹œê°„ (ì „ì²´ ë¬¸ì¥)
    axes[0, 0].bar(range(len(model_names)), latencies, color=colors, alpha=0.8)
    axes[0, 0].set_ylabel('Time (ms)', fontsize=11, fontweight='bold')
    axes[0, 0].set_title('1. Full Inference Time', fontsize=12, fontweight='bold')
    axes[0, 0].set_xticks(range(len(model_names)))
    axes[0, 0].set_xticklabels(['Model ' + str(i+1) for i in range(len(model_names))], fontsize=9)
    axes[0, 0].grid(axis='y', alpha=0.3)
    for i, v in enumerate(latencies):
        axes[0, 0].text(i, v + 1, '{:.1f}ms'.format(v), ha='center', fontsize=10, fontweight='bold')
    
    # 2. Tokenë‹¹ ì†Œìš”ì‹œê°„ (â˜…ë³€ê²½)
    axes[0, 1].bar(range(len(model_names)), token_time, color=colors, alpha=0.8)
    axes[0, 1].set_ylabel('Time (ms)', fontsize=11, fontweight='bold')
    axes[0, 1].set_title('2. Time Per Token', fontsize=12, fontweight='bold')
    axes[0, 1].set_xticks(range(len(model_names)))
    axes[0, 1].set_xticklabels(['Model ' + str(i+1) for i in range(len(model_names))], fontsize=9)
    axes[0, 1].grid(axis='y', alpha=0.3)
    for i, v in enumerate(token_time):
        axes[0, 1].text(i, v + 0.02, '{:.3f}ms'.format(v), ha='center', fontsize=10, fontweight='bold')
    
    # 3. CPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰
    axes[0, 2].bar(range(len(model_names)), memory_usage, color=colors, alpha=0.8)
    axes[0, 2].set_ylabel('Memory (MB)', fontsize=11, fontweight='bold')
    axes[0, 2].set_title('3. CPU Memory Usage', fontsize=12, fontweight='bold')
    axes[0, 2].set_xticks(range(len(model_names)))
    axes[0, 2].set_xticklabels(['Model ' + str(i+1) for i in range(len(model_names))], fontsize=9)
    axes[0, 2].grid(axis='y', alpha=0.3)
    for i, v in enumerate(memory_usage):
        axes[0, 2].text(i, v + 10, '{:.0f}MB'.format(v), ha='center', fontsize=10, fontweight='bold')
    
    # 4. ëª¨ë¸ í¬ê¸°
    axes[1, 0].bar(range(len(model_names)), model_sizes, color=colors, alpha=0.8)
    axes[1, 0].set_ylabel('Size (MB)', fontsize=11, fontweight='bold')
    axes[1, 0].set_title('4. Model File Size', fontsize=12, fontweight='bold')
    axes[1, 0].set_xticks(range(len(model_names)))
    axes[1, 0].set_xticklabels(['Model ' + str(i+1) for i in range(len(model_names))], fontsize=9)
    axes[1, 0].grid(axis='y', alpha=0.3)
    for i, v in enumerate(model_sizes):
        axes[1, 0].text(i, v + 0.5, '{:.2f}MB'.format(v), ha='center', fontsize=10, fontweight='bold')
    
    # 5. íŒŒë¼ë¯¸í„° ê°œìˆ˜
    axes[1, 1].bar(range(len(model_names)), param_counts, color=colors, alpha=0.8)
    axes[1, 1].set_ylabel('Parameters (M)', fontsize=11, fontweight='bold')
    axes[1, 1].set_title('5. Total Parameters', fontsize=12, fontweight='bold')
    axes[1, 1].set_xticks(range(len(model_names)))
    axes[1, 1].set_xticklabels(['Model ' + str(i+1) for i in range(len(model_names))], fontsize=9)
    axes[1, 1].grid(axis='y', alpha=0.3)
    for i, v in enumerate(param_counts):
        axes[1, 1].text(i, v + 0.1, '{:.1f}M'.format(v), ha='center', fontsize=10, fontweight='bold')
    
    # 6. FLOPs
    axes[1, 2].bar(range(len(model_names)), flops_values, color=colors, alpha=0.8)
    axes[1, 2].set_ylabel('FLOPs (M)', fontsize=11, fontweight='bold')
    axes[1, 2].set_title('6. Floating Point Operations', fontsize=12, fontweight='bold')
    axes[1, 2].set_xticks(range(len(model_names)))
    axes[1, 2].set_xticklabels(['Model ' + str(i+1) for i in range(len(model_names))], fontsize=9)
    axes[1, 2].grid(axis='y', alpha=0.3)
    for i, v in enumerate(flops_values):
        axes[1, 2].text(i, v + 50, '{:.0f}M'.format(v), ha='center', fontsize=10, fontweight='bold')
    
    plt.tight_layout(rect=[0, 0.03, 1, 0.95])
    
    # ì €ì¥
    output_path = 'benchmark_comparison.png'
    plt.savefig(output_path, dpi=150, bbox_inches='tight')
    print("Graph saved: {}".format(output_path))
    
    # ìƒì„¸ í…Œì´ë¸”
    plot_comparison_table(results)
    
    plt.show()

def plot_comparison_table(results):
    """ìƒì„¸ ë¹„êµ í…Œì´ë¸”"""
    fig, ax = plt.subplots(figsize=(16, 6))
    ax.axis('off')
    
    # í…Œì´ë¸” ë°ì´í„°
    model_names = list(results.keys())
    table_data = []
    
    for idx, model_name in enumerate(model_names):
        stats = results[model_name]
        table_data.append([
            'Model {}'.format(idx + 1),
            '{:.2f}ms'.format(stats['mean_latency_ms']),
            '{:.3f}ms'.format(stats['mean_latency_ms'] / 50.0),  # Token time
            '{:.1f}MB'.format(stats['cpu_memory_mb']),
            '{:.2f}MB'.format(stats['model_size_mb']),
            '{:.1f}M'.format(stats['total_params'] / 1e6),
            '{:.0f}M'.format(stats['flops_millions']),
        ])
    
    # í…Œì´ë¸” ìƒì„±
    col_labels = ['Model', 'Latency', 'Token Time', 'Memory', 'Size', 'Params', 'FLOPs']
    
    table = ax.table(
        cellText=table_data,
        colLabels=col_labels,
        cellLoc='center',
        loc='center',
        colWidths=[0.12, 0.12, 0.14, 0.13, 0.12, 0.12, 0.12]
    )
    
    table.auto_set_font_size(False)
    table.set_fontsize(10)
    table.scale(1, 2.5)
    
    # í—¤ë” ìŠ¤íƒ€ì¼
    for i in range(len(col_labels)):
        table[(0, i)].set_facecolor('#4CAF50')
        table[(0, i)].set_text_props(weight='bold', color='white')
    
    # í–‰ ìƒ‰ìƒ
    colors = ['#E8F5E9', '#FFF3E0', '#E3F2FD', '#FCE4EC', '#F3E5F5']
    for i in range(len(table_data)):
        for j in range(len(col_labels)):
            table[(i + 1, j)].set_facecolor(colors[i % len(colors)])
    
    plt.title('Jetson Nano Model Performance Details', fontsize=14, fontweight='bold', pad=20)
    plt.savefig('benchmark_comparison_table.png', dpi=150, bbox_inches='tight')
    print("Table saved: benchmark_comparison_table.png")

if __name__ == "__main__":
    main()
