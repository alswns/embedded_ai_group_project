"""
Jetson Nano ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ ë²¤ì¹˜ë§ˆí¬
- Base Model (ì›ë³¸)
- Base Model + FP16 ì–‘ìí™”
- Base Model + Pruning
- Base Model + Pruning + Fine-tuning
- Base Model + Pruning + Fine-tuning + FP16
"""

import os
import gc
import json
import time
import torch
import numpy as np
import matplotlib.pyplot as plt
import matplotlib
import psutil
import sys
import warnings
import cv2  
warnings.filterwarnings("ignore")

# â˜… ì˜ë¬¸ í°íŠ¸ë§Œ ì‚¬ìš© (í•œê¸€ ê¹¨ì§ ë°©ì§€)
matplotlib.rcParams['font.family'] = 'DejaVu Sans'
matplotlib.rcParams['axes.unicode_minus'] = False

# í”„ë¡œì íŠ¸ ëª¨ë“ˆ
from src.utils.memory_safe_import import load_model_class
from src.utils.model_utils import get_model_size_mb
from src.utils.metrics import calculate_meteor as calculate_meteor_score


# ============================================================================
# ì„¤ì •
# ============================================================================
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
NUM_RUNS = 100  # ê° ëª¨ë¸ë‹¹ ì¶”ë¡  íšŸìˆ˜
WARMUP_RUNS = 5  # ì›Œë°ì—… íšŸìˆ˜
# ============================================================================
# í™˜ê²½ ì„¤ì • (CRITICAL - í¬ë˜ì‹œ ë°©ì§€)
# ============================================================================
print("âš™ï¸  í™˜ê²½ ì„¤ì • ì¤‘...", file=sys.stderr)
torch.backends.cudnn.enabled = False  # ë¶ˆì•ˆì •ì„± ë°©ì§€
torch.backends.cudnn.benchmark = True # ì…ë ¥ í¬ê¸°ê°€ ê³ ì •(224x224)ì´ë¯€ë¡œ í•„ìˆ˜

# CPU/GPU ë””ë°”ì´ìŠ¤ ìë™ ê°ì§€ ë° ê°•ì œ ì„¤ì •
if torch.cuda.is_available():
    device = torch.device("cuda")
    print("ğŸš€ ë””ë°”ì´ìŠ¤: GPU (NVIDIA Maxwell) ê°€ì† ëª¨ë“œ", file=sys.stderr)
else:
    device = torch.device("cpu")
    print("ğŸ“ ë””ë°”ì´ìŠ¤: CPU (ê²½ê³ : ì„±ëŠ¥ì´ ë‚®ì„ ìˆ˜ ìˆìŒ)", file=sys.stderr)

# ìŠ¤ë ˆë“œ ìµœì í™”
torch.set_num_threads(4)
torch.set_num_interop_threads(4)

sys.modules['numpy._core'] = np.core
sys.modules['numpy._core.multiarray'] = np.core.multiarray
dtypes = torch.float32

print("ğŸ“Š Jetson Nano ëª¨ë¸ ë¹„êµ ë²¤ì¹˜ë§ˆí¬")
print("=" * 70)
print("ë””ë°”ì´ìŠ¤: {}".format(device))
print("=" * 70 + "\n")

# ============================================================================
# ëª¨ë¸ ê²½ë¡œ ì„¤ì •
# ============================================================================
MODELS_CONFIG = {
    'Base Model': {
        'path': 'models/lightweight_captioning_model.pth',
        'quantize': False,
        'pruned': False,
        'finetuned': False,
    },
    'Base Model + FP16': {
        'path': 'models/lightweight_captioning_model.pth',
        'quantize': True,
        'pruned': False,
        'finetuned': False,
    },
    'Base Model + Pruning + FT': {
        'path': 'pruning_results/Pruning_epoch_1_checkpoint.pt',
        'quantize': False,
        'pruned': True,
        'finetuned': True,
    },
    'Base Model + Pruning + FT + FP16': {
        'path': 'pruning_results/Pruning_epoch_1_checkpoint.pt',
        'quantize': True,
        'pruned': True,
        'finetuned': True,
    },
}

# ============================================================================
# ëª¨ë¸ ë¡œë“œ í•¨ìˆ˜
# ============================================================================
def preprocess_image_optimized(frame, quantize=False):
    """
    Jetson Nano ìµœì í™” ì „ì²˜ë¦¬:
    1. PIL ì œê±° (ëŠë¦¼) -> OpenCV ì‚¬ìš© (ë¹ ë¦„)
    2. CPU ì—°ì‚° ìµœì†Œí™” -> GPUë¡œ ë°”ë¡œ ì—…ë¡œë“œ
    3. ëª¨ë¸ ì„¤ì •ì— ë§ê²Œ dtype ìë™ ë³€í™˜
    """
    # 1. OpenCV ë¦¬ì‚¬ì´ì¦ˆ (CPU ë¶€í•˜ ê°ì†Œ)
    img = cv2.resize(frame, (224, 224), interpolation=cv2.INTER_LINEAR)
    
    # 2. BGR -> RGB ë° ì •ê·œí™” (Numpy)
    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB).astype(np.float32) / 255.0
    
    # 3. ì •ê·œí™” (Mean/Std)
    img -= np.array([0.485, 0.456, 0.406], dtype=np.float32)
    img /= np.array([0.229, 0.224, 0.225], dtype=np.float32)
    
    # 4. (H, W, C) -> (C, H, W)
    img = np.transpose(img, (2, 0, 1))
    
    # 5. Tensor ë³€í™˜ ë° GPU ì—…ë¡œë“œ
    image_tensor = torch.from_numpy(img).unsqueeze(0).to(device)
    
    # â˜… í•µì‹¬: ì„¤ì •ì— ë”°ë¼ dtype ë³€í™˜
    if quantize:
        return image_tensor.half()  # FP16
    else:
        return image_tensor.float()  # FP32


def load_model_with_config(config):
    """ì„¤ì •ì— ë”°ë¼ ëª¨ë¸ ë¡œë“œ"""
    model_path = config['path']
    
    if not os.path.exists(model_path):
        print("âŒ ëª¨ë¸ íŒŒì¼ ì—†ìŒ: {}".format(model_path))
        return None, None, None
    
    try:
        # ëª¨ë¸ í´ë˜ìŠ¤ ë¡œë“œ
        Model = load_model_class()
        
        # ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ
        checkpoint = torch.load(model_path, map_location=device, weights_only=False)
        
        if isinstance(checkpoint, dict) and 'model_state_dict' in checkpoint:
            word_map = checkpoint.get('word_map')
            rev_word_map = checkpoint.get('rev_word_map')
            vocab_size = checkpoint.get('vocab_size')
            state_dict = checkpoint['model_state_dict']
            
            # state_dictì—ì„œ ëª¨ë¸ í¬ê¸° ì¶”ì¶œ
            decoder_dim = checkpoint.get('decoder_dim', 512)
            attention_dim = checkpoint.get('attention_dim', 256)
            
            # state_dictì—ì„œ ì‹¤ì œ í¬ê¸° ì¶”ì¶œ
            if 'decoder.decode_step.weight_ih' in state_dict:
                actual_size = state_dict['decoder.decode_step.weight_ih'].shape[0]
                decoder_dim = actual_size // 3
            
            if 'decoder.encoder_att.weight' in state_dict:
                attention_dim = state_dict['decoder.encoder_att.weight'].shape[0]
            
            # ëª¨ë¸ ìƒì„±
            gc.collect()
            model = Model(
                vocab_size=vocab_size,
                embed_dim=300,
                decoder_dim=decoder_dim,
                attention_dim=attention_dim
            )
            del Model
            
            # ê°€ì¤‘ì¹˜ ë¡œë“œ
            try:
                model.load_state_dict(state_dict, strict=True)
            except:
                model.load_state_dict(state_dict, strict=False)
            
            model = model.to(device)
            model.eval()
            
            # â˜… FP16 ì–‘ìí™” ì ìš©
            if config['quantize']:
                model = model.half()
            
            return model, word_map, rev_word_map
        else:
            print("âŒ ì˜ëª»ëœ ëª¨ë¸ íŒŒì¼ í˜•ì‹")
            return None, None, None
            
    except Exception as e:
        print("âŒ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {}".format(e))
        import traceback
        traceback.print_exc()
        return None, None, None


def load_test_dataset(test_dir='test'):
    """
    Test ë°ì´í„°ì…‹ ë¡œë“œ (ì´ë¯¸ì§€ + ì°¸ì¡° ìº¡ì…˜)
    """
    images = []
    captions = {}
    
    captions_file = os.path.join(test_dir, 'captions.txt')
    images_dir = os.path.join(test_dir, 'images')
    
    if not os.path.exists(captions_file):
        print("Test dataset not found")
        return [], {}
    
    # ìº¡ì…˜ ë¡œë“œ (ì‰¼í‘œë¡œ êµ¬ë¶„ëœ í˜•ì‹)
    with open(captions_file, 'r', encoding='utf-8') as f:
        for line in f:
            line = line.strip()
            if not line:
                continue
            # í˜•ì‹: image_name.jpg,caption text
            parts = line.split(',', 1)  # ì²« ë²ˆì§¸ ì‰¼í‘œë¡œë§Œ ë¶„ë¦¬
            if len(parts) >= 2:
                img_name = parts[0].strip()
                caption = parts[1].strip()
                captions[img_name] = caption
    
    # ì´ë¯¸ì§€ ê²½ë¡œ ë¡œë“œ
    if os.path.exists(images_dir):
        images = sorted([f for f in os.listdir(images_dir) if f.endswith(('.jpg', '.png', '.jpeg'))])
        images = [os.path.join(images_dir, img) for img in images if img in captions]
    
    print("Test dataset loaded: {} images".format(len(images)))
    return images, captions


    
def calculate_meteor_from_test_data(model, word_map, rev_word_map, num_samples=100):
    """
    Test ë°ì´í„°ì…‹ìœ¼ë¡œ METEOR ì ìˆ˜ ê³„ì‚°
    """
    test_images, test_captions = load_test_dataset()
    if not test_images:
        print("  No test images found")
        return 0.0
    
    # ìƒ˜í”Œ ì„ íƒ (ì²˜ìŒ num_samplesê°œ)
    sample_images = test_images[:min(num_samples, len(test_images))]
    meteor_scores = []
    
    print("  Computing METEOR on {} samples...".format(len(sample_images)))
    
    with torch.no_grad():
        for idx, img_path in enumerate(sample_images):
            try:
                # ì´ë¯¸ì§€ ë¡œë“œ (OpenCV ì‚¬ìš©)
                img = cv2.imread(img_path)
                if img is None:
                    continue
                
                # ìµœì í™”ëœ ì „ì²˜ë¦¬ í•¨ìˆ˜ ì‚¬ìš©
                img_tensor = preprocess_image_optimized(img)
                
                # ìº¡ì…˜ ìƒì„±
                generated_caption = None
                try:
                    generated_caption = model.generate(
                        img_tensor, word_map, rev_word_map,
                        max_len=50, device=device
                    )
                except Exception as e:
                    continue
                
                # ì°¸ì¡° ìº¡ì…˜
                img_name = os.path.basename(img_path)
                reference_caption = test_captions.get(img_name, '')
                
                if reference_caption and generated_caption:
                    score = calculate_meteor_score(generated_caption, reference_caption)
                    # scoreê°€ Noneì´ ì•„ë‹ˆê³  ìˆ«ìì¸ì§€ í™•ì¸
                    if score is not None and isinstance(score, (int, float)):
                        # 0~1 ë²”ìœ„ë¡œ í´ë¦½
                        score = max(0.0, min(1.0, float(score)))
                        meteor_scores.append(score)
                    
                    if (idx + 1) % 5 == 0 and meteor_scores:
                        current_avg = float(np.mean(meteor_scores))
                        print("    [{}/{}] METEOR: {:.4f}".format(idx + 1, len(sample_images), current_avg))
                    
            except Exception as e:
                continue
    
    # í‰ê·  METEOR ì ìˆ˜
    if meteor_scores:
        avg_meteor = float(np.mean(meteor_scores))
        print("  Average METEOR: {:.4f}".format(avg_meteor))
        return avg_meteor
    else:
        print("  No valid METEOR scores computed")
        return 0.0

def calculate_flops(model):
    """
    ëª¨ë¸ì˜ FLOPs ê³„ì‚° (íŒŒë¼ë¯¸í„° ê¸°ë°˜ ì¶”ì •)
    - ì¸ì½”ë”(MobileNetV3): params Ã— 2
    - ë””ì½”ë”(GRU+Attention): params Ã— 2
    """
    try:
        param_count = sum(p.numel() for p in model.parameters())
        # ì…ë ¥: (1, 3, 224, 224)
        # ì¸ì½”ë”: ì•½ 2.5M params â†’ ì•½ 600M FLOPs
        # ë””ì½”ë”(seq_len=50): ì•½ 0.5M params â†’ ì•½ 50M FLOPs
        # ì´í•©: ì•½ 650M FLOPs â‰ˆ 2.0 Ã— params
        estimated_flops = param_count * 2.0 / 1e6  # Millions
        return float(estimated_flops)
    except Exception as e:
        print("FLOPs ê³„ì‚° ì˜¤ë¥˜: {}".format(e))
        return 0.0

# ============================================================================
# ì„±ëŠ¥ ì¸¡ì • í•¨ìˆ˜
# ============================================================================
class BenchmarkMetrics:
    """ëª¨ë¸ ì„±ëŠ¥ ë©”íŠ¸ë¦­"""
    def __init__(self):
        self.inference_times = []
        self.cpu_memory_usage = []
        self.gpu_memory_usage = []
        self.process = psutil.Process(os.getpid())
    
    def record_inference(self, inf_time):
        """ì¶”ë¡  ì‹œê°„ ê¸°ë¡"""
        self.inference_times.append(inf_time)
    
    def record_memory(self):
        """ë©”ëª¨ë¦¬ ê¸°ë¡ (CPU + GPU)"""
        cpu_mem = self.process.memory_info().rss / 1024 / 1024
        self.cpu_memory_usage.append(cpu_mem)
        
        # GPU ë©”ëª¨ë¦¬ ê¸°ë¡
        if device.type == 'cuda':
            try:
                gpu_mem = torch.cuda.memory_allocated() / 1024 / 1024
                self.gpu_memory_usage.append(gpu_mem)
            except:
                self.gpu_memory_usage.append(0)
        else:
            self.gpu_memory_usage.append(0)
    
    def get_stats(self):
        """í†µê³„ ê³„ì‚°"""
        if not self.inference_times:
            return None
        
        times = np.array(self.inference_times)
        cpu_mem = np.mean(self.cpu_memory_usage) if self.cpu_memory_usage else 0
        gpu_mem = np.mean(self.gpu_memory_usage) if self.gpu_memory_usage else 0
        total_mem = cpu_mem + gpu_mem
        
        return {
            'mean_latency_ms': float(np.mean(times)),
            'median_latency_ms': float(np.median(times)),
            'min_latency_ms': float(np.min(times)),
            'max_latency_ms': float(np.max(times)),
            'std_latency_ms': float(np.std(times)),
            'cpu_memory_mb': float(cpu_mem),
            'gpu_memory_mb': float(gpu_mem),
            'total_memory_mb': float(total_mem),
            'total_params': 0,
            'model_size_mb': 0,
            'flops_millions': 0,
            'meteor_score': 0.0,
        }

def benchmark_model(model, word_map, rev_word_map, model_name, config):
    """ë‹¨ì¼ ëª¨ë¸ ë²¤ì¹˜ë§ˆí¬"""
    print("\nBenchmarking: {}".format(model_name))
    print("-" * 70)
    
    metrics = BenchmarkMetrics()
    
    # Test ë°ì´í„°ì…‹ ë¡œë“œ (ì‹¤ì œ ì´ë¯¸ì§€)
    test_images, test_captions = load_test_dataset()
    if not test_images:
        print("Test dataset not found, cannot benchmark")
        return None
    
    # ë”ë¯¸ ì…ë ¥ ìƒì„± (ì›Œë°ì—…ìš©)
    dummy_input = torch.randn(1, 3, 224, 224).to(device)
    if config['quantize']:
        dummy_input = dummy_input.half()
    
    # ì›Œë°ì—…
    print("  Warming up...", end='')
    with torch.no_grad():
        for _ in range(WARMUP_RUNS):
            _ = model.encoder(dummy_input)
    if device.type == 'cuda':
        torch.cuda.synchronize()
    print(" Done")
    
    # ë³¸ ë²¤ì¹˜ë§ˆí¬ (ì‹¤ì œ ì´ë¯¸ì§€ ì‚¬ìš© + METEOR ë™ì‹œ ì¸¡ì •)
    print("  Running {} iterations with real images (measuring METEOR)...".format(NUM_RUNS))
    
    meteor_scores_benchmark = []  # ë²¤ì¹˜ë§ˆí¬ ì¤‘ METEOR ì ìˆ˜ ì €ì¥
    
    with torch.no_grad():
        for i in range(NUM_RUNS):
            # ì‹¤ì œ ì´ë¯¸ì§€ ë¡œë“œ (ìˆœí™˜)
            img_idx = i % len(test_images)
            img_path = test_images[img_idx]
            
            try:
                # ì´ë¯¸ì§€ ë¡œë“œ (OpenCV ì‚¬ìš©)
                img = cv2.imread(img_path)
                if img is None:
                    print("    Failed to load image: {}".format(img_path))
                    continue
                
                # ìµœì í™”ëœ ì „ì²˜ë¦¬ í•¨ìˆ˜ ì‚¬ìš© (configì— ë§ê²Œ dtype ì ìš©)
                img_tensor = preprocess_image_optimized(img, quantize=config['quantize'])
                
                # ì¶”ë¡  ë° ë©”ëª¨ë¦¬/ì‹œê°„ ì¸¡ì •
                metrics.record_memory()
                start = time.time()
                
                generated_caption = None
                try:
                    generated_caption = model.generate(img_tensor, word_map, rev_word_map, max_len=50, device=device)
                except:
                    # generateê°€ ì—†ìœ¼ë©´ encoderë§Œ ì‹¤í–‰
                    features = model.encoder(img_tensor)
                    if hasattr(model, 'decoder'):
                        batch_size = features.size(0)
                        channel = features.size(1)
                        features_flat = features.view(batch_size, channel, -1).permute(0, 2, 1)
                
                if device.type == 'cuda':
                    torch.cuda.synchronize()
                
                inference_time = (time.time() - start) * 1000
                metrics.record_inference(inference_time)
                
                # â˜… ë²¤ì¹˜ë§ˆí¬ ì¤‘ METEOR ì ìˆ˜ ê³„ì‚°
                if generated_caption:
                    img_name = os.path.basename(img_path)
                    reference_caption = test_captions.get(img_name, '')
                    if reference_caption:
                        score = calculate_meteor_score(generated_caption, reference_caption)
                        # scoreê°€ Noneì´ ì•„ë‹ˆê³  ìˆ«ìì¸ì§€ í™•ì¸
                        if score is not None and isinstance(score, (int, float)):
                            # 0~1 ë²”ìœ„ë¡œ í´ë¦½
                            score = max(0.0, min(1.0, float(score)))
                            meteor_scores_benchmark.append(score)
                
                if (i + 1) % 10 == 0:
                    if meteor_scores_benchmark:
                        avg_meteor = float(np.mean(meteor_scores_benchmark))
                        print("    [{}/{}] Done | METEOR: {:.4f}".format(i + 1, NUM_RUNS, avg_meteor))
                    else:
                        print("    [{}/{}] Done".format(i + 1, NUM_RUNS))
                    
            except Exception as e:
                print("    Error processing image {}: {}".format(img_path, str(e)))
                continue
    
    # í†µê³„ ê³„ì‚°
    stats = metrics.get_stats()
    
    if stats is None:
        print("  No valid benchmark data collected")
        return None
    
    # ëª¨ë¸ í¬ê¸° ì •ë³´
    param_count = sum(p.numel() for p in model.parameters())
    stats['model_name']=model_name
    stats['total_params'] = param_count
    stats['model_size_mb'] = get_model_size_mb(model)
    
    # FLOPs ê³„ì‚°
    flops_millions = calculate_flops(model)
    stats['flops_millions'] = flops_millions
    
    # â˜… METEOR ì ìˆ˜ (ë²¤ì¹˜ë§ˆí¬ ì¤‘ ìˆ˜ì§‘í•œ ì ìˆ˜ ì‚¬ìš©)
    if meteor_scores_benchmark:
        avg_meteor = float(np.mean(meteor_scores_benchmark))
        stats['meteor_score'] = avg_meteor
        print("  Benchmark METEOR: {:.4f} (from {} samples)".format(avg_meteor, len(meteor_scores_benchmark)))
    else:
        # ë²¤ì¹˜ë§ˆí¬ ì¤‘ METEOR ëª» ì¸¡ì •í–ˆìœ¼ë©´ ë³„ë„ ê³„ì‚°
        print("  Calculating METEOR score separately...")
        meteor_score = calculate_meteor_from_test_data(model, word_map, rev_word_map, num_samples=20)
        stats['meteor_score'] = meteor_score
    
    print("\n Results:")
    print("    - Latency: {:.2f} ms".format(stats['mean_latency_ms']))
    print("    - Token Time: {:.3f} ms".format(stats['mean_latency_ms'] / 50.0))
    print("    - CPU Memory: {:.1f} MB".format(stats['cpu_memory_mb']))
    print("    - GPU Memory: {:.1f} MB".format(stats['gpu_memory_mb']))
    print("    - Total Memory: {:.1f} MB".format(stats['total_memory_mb']))
    print("    - Size: {:.2f} MB".format(stats['model_size_mb']))
    print("    - Params: {:,}".format(param_count))
    print("    - FLOPs: {:.1f}M".format(flops_millions))
    print("    - METEOR: {:.4f}".format(stats['meteor_score']))
    
    return stats

# ============================================================================
# ë©”ì¸ ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰
# ============================================================================
def main():
    results = {}
    
    print("\n" + "=" * 70)
    print("Starting Model Performance Comparison")
    print("=" * 70)
    
    for model_name, config in MODELS_CONFIG.items():
        print("\n\n[{}/{}] {}".format(
            list(MODELS_CONFIG.keys()).index(model_name) + 1,
            len(MODELS_CONFIG),
            model_name
        ))
        
        # ëª¨ë¸ ë¡œë“œ
        model, word_map, rev_word_map = load_model_with_config(config)
        
        if model is None:
            print("Model load failed, skipping")
            continue
        
        try:
            # ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰
            stats = benchmark_model(model, word_map, rev_word_map, model_name, config)
            results[model_name] = stats
            
        except Exception as e:
            print("Benchmark failed: {}".format(e))
            import traceback
            traceback.print_exc()
        
        finally:
            # ë©”ëª¨ë¦¬ ì •ë¦¬
            del model
            gc.collect()
    
    # ê²°ê³¼ ìš”ì•½
    print("\n\n" + "=" * 70)
    print("Benchmark Results Summary")
    print("=" * 70)
    
    for idx, (model_name, stats) in enumerate(results.items()):
        print("\nModel {}:".format(idx + 1))
        print("  - Latency: {:.2f} ms".format(stats['mean_latency_ms']))
        print("  - Token Time: {:.3f} ms".format(stats['mean_latency_ms'] / 50.0))
        print("  - Memory: {:.1f} MB".format(stats['cpu_memory_mb']))
        print("  - Size: {:.2f} MB".format(stats['model_size_mb']))
        print("  - FLOPs: {:.1f}M".format(stats['flops_millions']))
    
    # ê·¸ë˜í”„ ìƒì„±
    print("\n\n Generating graphs...")
    plot_comparison(results)
    
    # ê²°ê³¼ ì €ì¥
    with open('benchmark_results.json', 'w') as f:
        json.dump(results, f, indent=2)
    print("Results saved: benchmark_results.json")

# ============================================================================
# ê·¸ë˜í”„ ìƒì„± í•¨ìˆ˜
# ============================================================================
def plot_comparison(results):
    """ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ ê·¸ë˜í”„"""
    if not results:
        print("Results not found")
        return
    
    model_names = list(results.keys())
    
    # ë°ì´í„° ì¶”ì¶œ
    latencies = [results[m]['mean_latency_ms'] for m in model_names]
    cpu_memory = [results[m]['cpu_memory_mb'] for m in model_names]
    gpu_memory = [results[m]['gpu_memory_mb'] for m in model_names]
    total_memory = [results[m]['total_memory_mb'] for m in model_names]
    model_sizes = [results[m]['model_size_mb'] for m in model_names]
    param_counts = [results[m]['total_params'] / 1e6 for m in model_names]
    flops_values = [results[m]['flops_millions'] for m in model_names]
    meteor_scores = [results[m]['meteor_score'] for m in model_names]
    model_names=[results[m]['model_name'] for m in model_names]
    token_time = [lat / 50.0 for lat in latencies]
    
    # ê·¸ë˜í”„ ìƒì„± (2x4 = 8ê°œ ê·¸ë˜í”„)
    fig, axes = plt.subplots(2, 4, figsize=(20, 10))
    fig.suptitle('Jetson Nano Model Performance Comparison', fontsize=16, fontweight='bold')
    
    colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd']
    
    # Xì¶• ë¼ë²¨ ë‹¨ìˆœí™” (ê¸€ì ì˜ë¦¼ ë°©ì§€)
    x_labels = ['M{}'.format(i+1) for i in range(len(model_names))]
    
    # 1. ì¶”ë¡  ì§€ì—°ì‹œê°„
    axes[0, 0].bar(range(len(model_names)), latencies, color=colors, alpha=0.8)
    axes[0, 0].set_ylabel('Time (ms)', fontsize=11, fontweight='bold')
    axes[0, 0].set_title('1. Full Inference Time', fontsize=12, fontweight='bold')
    axes[0, 0].set_xticks(range(len(model_names)))
    axes[0, 0].set_xticklabels(x_labels, fontsize=10)
    axes[0, 0].grid(axis='y', alpha=0.3)
    for i, v in enumerate(latencies):
        axes[0, 0].text(i, v + max(latencies)*0.02, '{:.1f}ms'.format(v), ha='center', fontsize=9, fontweight='bold')
    
    # 2. Token ì‹œê°„
    axes[0, 1].bar(range(len(model_names)), token_time, color=colors, alpha=0.8)
    axes[0, 1].set_ylabel('Time (ms)', fontsize=11, fontweight='bold')
    axes[0, 1].set_title('2. Time Per Token', fontsize=12, fontweight='bold')
    axes[0, 1].set_xticks(range(len(model_names)))
    axes[0, 1].set_xticklabels(x_labels, fontsize=10)
    axes[0, 1].grid(axis='y', alpha=0.3)
    for i, v in enumerate(token_time):
        axes[0, 1].text(i, v + max(token_time)*0.02, '{:.3f}ms'.format(v), ha='center', fontsize=9, fontweight='bold')
    
    # 3. ë©”ëª¨ë¦¬ ë¶„ì„ (CPU, GPU, Total)
    x = np.arange(len(model_names))
    width = 0.25
    axes[0, 2].bar(x - width, cpu_memory, width, label='CPU', color='#1f77b4', alpha=0.8)
    axes[0, 2].bar(x, gpu_memory, width, label='GPU', color='#ff7f0e', alpha=0.8)
    axes[0, 2].bar(x + width, total_memory, width, label='Total', color='#2ca02c', alpha=0.8)
    axes[0, 2].set_ylabel('Memory (MB)', fontsize=11, fontweight='bold')
    axes[0, 2].set_title('3. Memory Usage (CPU/GPU/Total)', fontsize=12, fontweight='bold')
    axes[0, 2].set_xticks(x)
    axes[0, 2].set_xticklabels(x_labels, fontsize=10)
    axes[0, 2].legend(fontsize=9)
    axes[0, 2].grid(axis='y', alpha=0.3)
    
    # 4. ëª¨ë¸ í¬ê¸°
    axes[0, 3].bar(range(len(model_names)), model_sizes, color=colors, alpha=0.8)
    axes[0, 3].set_ylabel('Size (MB)', fontsize=11, fontweight='bold')
    axes[0, 3].set_title('4. Model File Size', fontsize=12, fontweight='bold')
    axes[0, 3].set_xticks(range(len(model_names)))
    axes[0, 3].set_xticklabels(x_labels, fontsize=10)
    axes[0, 3].grid(axis='y', alpha=0.3)
    for i, v in enumerate(model_sizes):
        axes[0, 3].text(i, v + max(model_sizes)*0.02, '{:.2f}MB'.format(v), ha='center', fontsize=9, fontweight='bold')
    
    # 5. íŒŒë¼ë¯¸í„° ê°œìˆ˜
    axes[1, 0].bar(range(len(model_names)), param_counts, color=colors, alpha=0.8)
    axes[1, 0].set_ylabel('Parameters (M)', fontsize=11, fontweight='bold')
    axes[1, 0].set_title('5. Total Parameters', fontsize=12, fontweight='bold')
    axes[1, 0].set_xticks(range(len(model_names)))
    axes[1, 0].set_xticklabels(model_names, fontsize=9)
    axes[1, 0].grid(axis='y', alpha=0.3)
    for i, v in enumerate(param_counts):
        axes[1, 0].text(i, v + 0.1, '{:.1f}M'.format(v), ha='center', fontsize=10, fontweight='bold')
    
    # 6. FLOPs
    axes[1, 1].bar(range(len(model_names)), flops_values, color=colors, alpha=0.8)
    axes[1, 1].set_ylabel('FLOPs (M)', fontsize=11, fontweight='bold')
    axes[1, 1].set_title('6. Floating Point Ops', fontsize=12, fontweight='bold')
    axes[1, 1].set_xticks(range(len(model_names)))
    axes[1, 1].set_xticklabels(x_labels, fontsize=10)
    axes[1, 1].grid(axis='y', alpha=0.3)
    for i, v in enumerate(flops_values):
        axes[1, 1].text(i, v + max(flops_values)*0.02, '{:.0f}M'.format(v), ha='center', fontsize=9, fontweight='bold')
    
    # 7. METEOR ì ìˆ˜
    axes[1, 2].bar(range(len(model_names)), meteor_scores, color=colors, alpha=0.8)
    axes[1, 2].set_ylabel('METEOR Score', fontsize=11, fontweight='bold')
    axes[1, 2].set_title('7. METEOR Quality Score', fontsize=12, fontweight='bold')
    axes[1, 2].set_xticks(range(len(model_names)))
    axes[1, 2].set_xticklabels(x_labels, fontsize=10)
    max_meteor = max(meteor_scores) if meteor_scores else 1
    axes[1, 2].set_ylim([0, max(max_meteor*1.15, 0.1)])
    axes[1, 2].grid(axis='y', alpha=0.3)
    for i, v in enumerate(meteor_scores):
        axes[1, 2].text(i, v + max_meteor*0.02, '{:.3f}'.format(v), ha='center', fontsize=9, fontweight='bold')
    
    # 8. ì„±ëŠ¥ ì¢…í•© ì ìˆ˜
    latency_score = [1.0 / (lat / 100 + 0.1) for lat in latencies]
    memory_score = [1.0 / (mem / 100 + 0.1) for mem in total_memory]
    overall_score = [
        (latency_score[i] + memory_score[i] + meteor_scores[i] * 10) / 12
        for i in range(len(model_names))
    ]
    
    axes[1, 3].bar(range(len(model_names)), overall_score, color=colors, alpha=0.8)
    axes[1, 3].set_ylabel('Score', fontsize=11, fontweight='bold')
    axes[1, 3].set_title('8. Overall Score', fontsize=12, fontweight='bold')
    axes[1, 3].set_xticks(range(len(model_names)))
    axes[1, 3].set_xticklabels(x_labels, fontsize=10)
    axes[1, 3].grid(axis='y', alpha=0.3)
    for i, v in enumerate(overall_score):
        axes[1, 3].text(i, v + max(overall_score)*0.02, '{:.2f}'.format(v), ha='center', fontsize=9, fontweight='bold')
    
    plt.tight_layout(rect=[0, 0.03, 1, 0.95])
    
    output_path = 'benchmark_comparison.png'
    plt.savefig(output_path, dpi=150, bbox_inches='tight')
    print("Graph saved: {}".format(output_path))
    
    plot_comparison_table(results)

def plot_comparison_table(results):
    """ìƒì„¸ ë¹„êµ í…Œì´ë¸”"""
    fig, ax = plt.subplots(figsize=(16, 6))
    ax.axis('off')
    
    # í…Œì´ë¸” ë°ì´í„°
    model_names = list(results.keys())
    table_data = []
    
    for idx, model_name in enumerate(model_names):
        stats = results[model_name]
        table_data.append([
            'M{}'.format(idx + 1),
            '{:.2f}ms'.format(stats['mean_latency_ms']),
            '{:.3f}ms'.format(stats['mean_latency_ms'] / 50.0),
            '{:.0f}/{:.0f}/{:.0f}'.format(stats['cpu_memory_mb'], stats['gpu_memory_mb'], stats['total_memory_mb']),
            '{:.2f}MB'.format(stats['model_size_mb']),
            '{:.1f}M'.format(stats['total_params'] / 1e6),
            '{:.0f}M'.format(stats['flops_millions']),
            '{:.4f}'.format(stats['meteor_score']),
        ])
    
    # í…Œì´ë¸” ìƒì„±
    col_labels = ['Model', 'Latency', 'Token', 'Memory(C/G/T)', 'Size', 'Params', 'FLOPs', 'METEOR']
    
    table = ax.table(
        cellText=table_data,
        colLabels=col_labels,
        cellLoc='center',
        loc='center',
        colWidths=[0.08, 0.10, 0.10, 0.15, 0.10, 0.10, 0.10, 0.12]
    )
    
    table.auto_set_font_size(False)
    table.set_fontsize(10)
    table.scale(1, 2.5)
    
    # í—¤ë” ìŠ¤íƒ€ì¼
    for i in range(len(col_labels)):
        table[(0, i)].set_facecolor('#4CAF50')
        table[(0, i)].set_text_props(weight='bold', color='white')
    
    # í–‰ ìƒ‰ìƒ
    colors = ['#E8F5E9', '#FFF3E0', '#E3F2FD', '#FCE4EC', '#F3E5F5']
    for i in range(len(table_data)):
        for j in range(len(col_labels)):
            table[(i + 1, j)].set_facecolor(colors[i % len(colors)])
    
    plt.title('Jetson Nano Model Performance Details', fontsize=14, fontweight='bold', pad=20)
    plt.savefig('benchmark_comparison_table.png', dpi=150, bbox_inches='tight')
    print("Table saved: benchmark_comparison_table.png")
    plt.close('all')

if __name__ == "__main__":
    main()
