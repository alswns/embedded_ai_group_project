"""
Jetson Nano ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ ë²¤ì¹˜ë§ˆí¬
- Base Model (ì›ë³¸)
- Base Model + FP16 ì–‘ìí™”
- Base Model + Pruning
- Base Model + Pruning + Fine-tuning
- Base Model + Pruning + Fine-tuning + FP16
"""

import os
import gc
import json
import time
import torch
import numpy as np
import matplotlib.pyplot as plt
import psutil
import sys
from pathlib import Path

# í”„ë¡œì íŠ¸ ëª¨ë“ˆ
from src.utils.memory_safe_import import load_model_class
from src.utils.model_utils import get_model_size_mb

# FLOPs ê³„ì‚°
try:
    from thop import profile
    THOP_AVAILABLE = True
except ImportError:
    THOP_AVAILABLE = False
    print("âš ï¸  thop ë¼ì´ë¸ŒëŸ¬ë¦¬ ì—†ìŒ (pip install thop)", file=sys.stderr)

# ============================================================================
# ì„¤ì •
# ============================================================================
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
NUM_RUNS = 50  # ê° ëª¨ë¸ë‹¹ ì¶”ë¡  íšŸìˆ˜
WARMUP_RUNS = 5  # ì›Œë°ì—… íšŸìˆ˜

print("ğŸ“Š Jetson Nano ëª¨ë¸ ë¹„êµ ë²¤ì¹˜ë§ˆí¬")
print("=" * 70)
print("ë””ë°”ì´ìŠ¤: {}".format(device))
print("=" * 70 + "\n")

# ============================================================================
# ëª¨ë¸ ê²½ë¡œ ì„¤ì •
# ============================================================================
MODELS_CONFIG = {
    'Base Model': {
        'path': 'models/lightweight_captioning_model.pth',
        'quantize': False,
        'pruned': False,
        'finetuned': False,
    },
    'Base Model + FP16': {
        'path': 'models/lightweight_captioning_model.pth',
        'quantize': True,
        'pruned': False,
        'finetuned': False,
    },
    'Base Model + Pruning': {
        'path': 'pruning_results/Pruning_epoch_1_checkpoint.pt',
        'quantize': False,
        'pruned': True,
        'finetuned': False,
    },
    'Base Model + Pruning + FT': {
        'path': 'pruning_results/Pruning_epoch_1_checkpoint.pt',
        'quantize': False,
        'pruned': True,
        'finetuned': True,
    },
    'Base Model + Pruning + FT + FP16': {
        'path': 'pruning_results/Pruning_epoch_1_checkpoint.pt',
        'quantize': True,
        'pruned': True,
        'finetuned': True,
    },
}

# ============================================================================
# ëª¨ë¸ ë¡œë“œ í•¨ìˆ˜
# ============================================================================
def load_model_with_config(config):
    """ì„¤ì •ì— ë”°ë¼ ëª¨ë¸ ë¡œë“œ"""
    model_path = config['path']
    
    if not os.path.exists(model_path):
        print("âŒ ëª¨ë¸ íŒŒì¼ ì—†ìŒ: {}".format(model_path))
        return None, None, None
    
    try:
        # ëª¨ë¸ í´ë˜ìŠ¤ ë¡œë“œ
        Model = load_model_class()
        
        # ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ
        checkpoint = torch.load(model_path, map_location=device)
        
        if isinstance(checkpoint, dict) and 'model_state_dict' in checkpoint:
            word_map = checkpoint.get('word_map')
            rev_word_map = checkpoint.get('rev_word_map')
            vocab_size = checkpoint.get('vocab_size')
            state_dict = checkpoint['model_state_dict']
            
            # state_dictì—ì„œ ëª¨ë¸ í¬ê¸° ì¶”ì¶œ
            decoder_dim = checkpoint.get('decoder_dim', 512)
            attention_dim = checkpoint.get('attention_dim', 256)
            
            # state_dictì—ì„œ ì‹¤ì œ í¬ê¸° ì¶”ì¶œ
            if 'decoder.decode_step.weight_ih' in state_dict:
                actual_size = state_dict['decoder.decode_step.weight_ih'].shape[0]
                decoder_dim = actual_size // 3
            
            if 'decoder.encoder_att.weight' in state_dict:
                attention_dim = state_dict['decoder.encoder_att.weight'].shape[0]
            
            # ëª¨ë¸ ìƒì„±
            gc.collect()
            model = Model(
                vocab_size=vocab_size,
                embed_dim=300,
                decoder_dim=decoder_dim,
                attention_dim=attention_dim
            )
            del Model
            
            # ê°€ì¤‘ì¹˜ ë¡œë“œ
            try:
                model.load_state_dict(state_dict, strict=True)
            except:
                model.load_state_dict(state_dict, strict=False)
            
            model = model.to(device)
            model.eval()
            
            # â˜… FP16 ì–‘ìí™” ì ìš©
            if config['quantize']:
                model = model.half()
            
            return model, word_map, rev_word_map
        else:
            print("âŒ ì˜ëª»ëœ ëª¨ë¸ íŒŒì¼ í˜•ì‹")
            return None, None, None
            
    except Exception as e:
        print("âŒ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {}".format(e))
        import traceback
        traceback.print_exc()
        return None, None, None

# ============================================================================
# FLOPs ê³„ì‚° í•¨ìˆ˜
# ============================================================================
def calculate_flops(model):
    """
    ëª¨ë¸ì˜ FLOPs ê³„ì‚° (ê°„ë‹¨í•œ ì¶”ì • ë°©ì‹)
    thop ë¼ì´ë¸ŒëŸ¬ë¦¬ ì‚¬ìš© ë¶ˆê°€ì‹œ íŒŒë¼ë¯¸í„° ê¸°ë°˜ ì¶”ì •
    """
    try:
        # thop ë¼ì´ë¸ŒëŸ¬ë¦¬ë¡œ ì •í™•í•œ FLOPs ê³„ì‚° ì‹œë„
        if THOP_AVAILABLE:
            try:
                dummy_input = torch.randn(1, 3, 224, 224).to(device)
                if hasattr(model, 'float'):
                    model_float = model.float()
                    dummy_input_float = dummy_input.float()
                    from thop import profile
                    flops, params = profile(model_float, inputs=(dummy_input_float,), verbose=False)
                else:
                    from thop import profile
                    flops, params = profile(model, inputs=(dummy_input,), verbose=False)
                
                return float(flops / 1e6)  # Millions
            except Exception as e:
                print("âš ï¸  thop ê³„ì‚° ì‹¤íŒ¨: {}".format(e))
    except:
        pass
    
    # í´ë°±: íŒŒë¼ë¯¸í„° ê°œìˆ˜ ê¸°ë°˜ ì¶”ì •
    # ì¼ë°˜ì ìœ¼ë¡œ FLOPs â‰ˆ 2 Ã— Parameters (ìˆœì „íŒŒ)
    param_count = sum(p.numel() for p in model.parameters())
    estimated_flops = param_count * 2.5 / 1e6  # Millions
    
    return estimated_flops

# ============================================================================
# ì„±ëŠ¥ ì¸¡ì • í•¨ìˆ˜
# ============================================================================
class BenchmarkMetrics:
    """ëª¨ë¸ ì„±ëŠ¥ ë©”íŠ¸ë¦­"""
    def __init__(self):
        self.inference_times = []
        self.memory_usage = []
        self.process = psutil.Process(os.getpid())
    
    def record_inference(self, inf_time):
        """ì¶”ë¡  ì‹œê°„ ê¸°ë¡"""
        self.inference_times.append(inf_time)
    
    def record_memory(self):
        """ë©”ëª¨ë¦¬ ê¸°ë¡"""
        mem = self.process.memory_info().rss / 1024 / 1024
        self.memory_usage.append(mem)
    
    def get_stats(self):
        """í†µê³„ ê³„ì‚°"""
        if not self.inference_times:
            return None
        
        times = np.array(self.inference_times)
        
        return {
            'mean_latency_ms': float(np.mean(times)),
            'median_latency_ms': float(np.median(times)),
            'min_latency_ms': float(np.min(times)),
            'max_latency_ms': float(np.max(times)),
            'std_latency_ms': float(np.std(times)),
            'fps': float(1000.0 / np.mean(times)),
            'cpu_memory_mb': float(np.mean(self.memory_usage) if self.memory_usage else 0),
            'total_params': 0,
            'model_size_mb': 0,
            'flops_millions': 0,  # ì¶”í›„ ê³„ì‚°
        }

def benchmark_model(model, word_map, rev_word_map, model_name, config):
    """ë‹¨ì¼ ëª¨ë¸ ë²¤ì¹˜ë§ˆí¬"""
    print("\nğŸ” ë²¤ì¹˜ë§ˆí‚¹: {}".format(model_name))
    print("-" * 70)
    
    metrics = BenchmarkMetrics()
    
    # ë”ë¯¸ ì…ë ¥ ìƒì„±
    dummy_input = torch.randn(1, 3, 224, 224).to(device)
    if config['quantize']:
        dummy_input = dummy_input.half()
    
    # ì›Œë°ì—…
    print("  âš™ï¸  ì›Œë°ì—… ì¤‘...", end='')
    with torch.no_grad():
        for _ in range(WARMUP_RUNS):
            _ = model.encoder(dummy_input)
    if device.type == 'cuda':
        torch.cuda.synchronize()
    print(" âœ…")
    
    # ë³¸ ë²¤ì¹˜ë§ˆí¬
    print("  ğŸƒ ì¶”ë¡  ì‹¤í–‰ ì¤‘... ({} runs)".format(NUM_RUNS))
    
    # ë”ë¯¸ ì´ë¯¸ì§€ ìƒì„±
    dummy_frame = np.random.randint(0, 256, (480, 640, 3), dtype=np.uint8)
    
    with torch.no_grad():
        for i in range(NUM_RUNS):
            # ì „ì²˜ë¦¬
            img = dummy_frame.copy()
            img = np.transpose(img, (2, 0, 1)).astype(np.float32) / 255.0
            img_tensor = torch.from_numpy(img).unsqueeze(0).to(device)
            
            if config['quantize']:
                img_tensor = img_tensor.half()
            
            # ì¶”ë¡ 
            metrics.record_memory()
            start = time.time()
            
            try:
                _ = model.generate(img_tensor, word_map, rev_word_map, max_len=50, device=device)
            except:
                # generateê°€ ì—†ìœ¼ë©´ encoderë§Œ ì‹¤í–‰
                features = model.encoder(img_tensor)
                if hasattr(model, 'decoder'):
                    batch_size = features.size(0)
                    channel = features.size(1)
                    features_flat = features.view(batch_size, channel, -1).permute(0, 2, 1)
            
            if device.type == 'cuda':
                torch.cuda.synchronize()
            
            inference_time = (time.time() - start) * 1000
            metrics.record_inference(inference_time)
            
            if (i + 1) % 10 == 0:
                print("    [{}/{}] ì™„ë£Œ".format(i + 1, NUM_RUNS))
    
    # í†µê³„ ê³„ì‚°
    stats = metrics.get_stats()
    
    # ëª¨ë¸ í¬ê¸° ì •ë³´
    param_count = sum(p.numel() for p in model.parameters())
    stats['total_params'] = param_count
    stats['model_size_mb'] = get_model_size_mb(model)
    
    # â˜… FLOPs ê³„ì‚°
    flops_millions = calculate_flops(model)
    stats['flops_millions'] = flops_millions
    
    print("\n  ğŸ“Š ê²°ê³¼:")
    print("    â€¢ í‰ê·  ì§€ì—°ì‹œê°„: {:.2f} ms".format(stats['mean_latency_ms']))
    print("    â€¢ FPS: {:.1f}".format(stats['fps']))
    print("    â€¢ ë©”ëª¨ë¦¬: {:.1f} MB".format(stats['cpu_memory_mb']))
    print("    â€¢ ëª¨ë¸ í¬ê¸°: {:.2f} MB".format(stats['model_size_mb']))
    print("    â€¢ íŒŒë¼ë¯¸í„°: {:,}ê°œ".format(param_count))
    print("    â€¢ FLOPs: {:.1f}M".format(flops_millions))
    
    return stats

# ============================================================================
# ë©”ì¸ ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰
# ============================================================================
def main():
    results = {}
    
    print("\n" + "=" * 70)
    print("ğŸš€ ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ ì‹œì‘")
    print("=" * 70)
    
    for model_name, config in MODELS_CONFIG.items():
        print("\n\n[{}/{}] {}".format(
            list(MODELS_CONFIG.keys()).index(model_name) + 1,
            len(MODELS_CONFIG),
            model_name
        ))
        
        # ëª¨ë¸ ë¡œë“œ
        model, word_map, rev_word_map = load_model_with_config(config)
        
        if model is None:
            print("âš ï¸  ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨, ìŠ¤í‚µ")
            continue
        
        try:
            # ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰
            stats = benchmark_model(model, word_map, rev_word_map, model_name, config)
            results[model_name] = stats
            
        except Exception as e:
            print("âŒ ë²¤ì¹˜ë§ˆí¬ ì‹¤íŒ¨: {}".format(e))
            import traceback
            traceback.print_exc()
        
        finally:
            # ë©”ëª¨ë¦¬ ì •ë¦¬
            del model
            gc.collect()
    
    # ê²°ê³¼ ìš”ì•½
    print("\n\n" + "=" * 70)
    print("ğŸ“Š ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼ ìš”ì•½")
    print("=" * 70)
    
    for model_name, stats in results.items():
        print("\n{}:".format(model_name))
        print("  â€¢ Latency: {:.2f} ms".format(stats['mean_latency_ms']))
        print("  â€¢ FPS: {:.1f}".format(stats['fps']))
        print("  â€¢ Memory: {:.1f} MB".format(stats['cpu_memory_mb']))
        print("  â€¢ Model Size: {:.2f} MB".format(stats['model_size_mb']))
        print("  â€¢ FLOPs: {:.1f}M".format(stats['flops_millions']))
    
    # ê·¸ë˜í”„ ìƒì„±
    print("\n\nğŸ“ˆ ê·¸ë˜í”„ ìƒì„± ì¤‘...")
    plot_comparison(results)
    
    # ê²°ê³¼ ì €ì¥
    with open('benchmark_results.json', 'w') as f:
        json.dump(results, f, indent=2)
    print("âœ… ê²°ê³¼ ì €ì¥: benchmark_results.json")

# ============================================================================
# ê·¸ë˜í”„ ìƒì„± í•¨ìˆ˜
# ============================================================================
def plot_comparison(results):
    """ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ ê·¸ë˜í”„"""
    if not results:
        print("âŒ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤")
        return
    
    model_names = list(results.keys())
    
    # ë°ì´í„° ì¶”ì¶œ
    latencies = [results[m]['mean_latency_ms'] for m in model_names]
    fps_values = [results[m]['fps'] for m in model_names]
    memory_usage = [results[m]['cpu_memory_mb'] for m in model_names]
    model_sizes = [results[m]['model_size_mb'] for m in model_names]
    param_counts = [results[m]['total_params'] / 1e6 for m in model_names]  # Million
    flops_values = [results[m]['flops_millions'] for m in model_names]  # â˜… FLOPs ì¶”ê°€
    
    # ê·¸ë˜í”„ ìƒì„± (3x3ìœ¼ë¡œ ë³€ê²½í•˜ì—¬ 7ê°œ ê·¸ë˜í”„ í‘œì‹œ)
    fig, axes = plt.subplots(3, 3, figsize=(18, 14))
    fig.suptitle('Jetson Nano ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ ë¶„ì„ (FLOPs í¬í•¨)', fontsize=16, fontweight='bold')
    
    # ìƒ‰ìƒ ì„¤ì •
    colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd']
    
    # 1. ì¶”ë¡  ì§€ì—°ì‹œê°„
    axes[0, 0].bar(range(len(model_names)), latencies, color=colors, alpha=0.8)
    axes[0, 0].set_ylabel('ì§€ì—°ì‹œê°„ (ms)', fontsize=11, fontweight='bold')
    axes[0, 0].set_title('â‘  í‰ê·  ì¶”ë¡  ì§€ì—°ì‹œê°„', fontsize=12, fontweight='bold')
    axes[0, 0].set_xticks(range(len(model_names)))
    axes[0, 0].set_xticklabels([m.replace(' + ', '\n+ ') for m in model_names], fontsize=9)
    axes[0, 0].grid(axis='y', alpha=0.3)
    for i, v in enumerate(latencies):
        axes[0, 0].text(i, v + 1, '{:.1f}ms'.format(v), ha='center', fontsize=10, fontweight='bold')
    
    # 2. FPS (ì²˜ë¦¬ëŸ‰)
    axes[0, 1].bar(range(len(model_names)), fps_values, color=colors, alpha=0.8)
    axes[0, 1].set_ylabel('FPS', fontsize=11, fontweight='bold')
    axes[0, 1].set_title('â‘¡ ì´ˆë‹¹ í”„ë ˆì„ ìˆ˜ (FPS)', fontsize=12, fontweight='bold')
    axes[0, 1].set_xticks(range(len(model_names)))
    axes[0, 1].set_xticklabels([m.replace(' + ', '\n+ ') for m in model_names], fontsize=9)
    axes[0, 1].grid(axis='y', alpha=0.3)
    for i, v in enumerate(fps_values):
        axes[0, 1].text(i, v + 0.5, '{:.1f}'.format(v), ha='center', fontsize=10, fontweight='bold')
    
    # 3. CPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰
    axes[0, 2].bar(range(len(model_names)), memory_usage, color=colors, alpha=0.8)
    axes[0, 2].set_ylabel('ë©”ëª¨ë¦¬ (MB)', fontsize=11, fontweight='bold')
    axes[0, 2].set_title('â‘¢ CPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰', fontsize=12, fontweight='bold')
    axes[0, 2].set_xticks(range(len(model_names)))
    axes[0, 2].set_xticklabels([m.replace(' + ', '\n+ ') for m in model_names], fontsize=9)
    axes[0, 2].grid(axis='y', alpha=0.3)
    for i, v in enumerate(memory_usage):
        axes[0, 2].text(i, v + 10, '{:.0f}MB'.format(v), ha='center', fontsize=10, fontweight='bold')
    
    # 4. ëª¨ë¸ í¬ê¸°
    axes[1, 0].bar(range(len(model_names)), model_sizes, color=colors, alpha=0.8)
    axes[1, 0].set_ylabel('í¬ê¸° (MB)', fontsize=11, fontweight='bold')
    axes[1, 0].set_title('â‘£ ëª¨ë¸ íŒŒì¼ í¬ê¸°', fontsize=12, fontweight='bold')
    axes[1, 0].set_xticks(range(len(model_names)))
    axes[1, 0].set_xticklabels([m.replace(' + ', '\n+ ') for m in model_names], fontsize=9)
    axes[1, 0].grid(axis='y', alpha=0.3)
    for i, v in enumerate(model_sizes):
        axes[1, 0].text(i, v + 0.5, '{:.2f}MB'.format(v), ha='center', fontsize=10, fontweight='bold')
    
    # 5. íŒŒë¼ë¯¸í„° ê°œìˆ˜
    axes[1, 1].bar(range(len(model_names)), param_counts, color=colors, alpha=0.8)
    axes[1, 1].set_ylabel('íŒŒë¼ë¯¸í„° (Million)', fontsize=11, fontweight='bold')
    axes[1, 1].set_title('â‘¤ ì´ íŒŒë¼ë¯¸í„° ê°œìˆ˜', fontsize=12, fontweight='bold')
    axes[1, 1].set_xticks(range(len(model_names)))
    axes[1, 1].set_xticklabels([m.replace(' + ', '\n+ ') for m in model_names], fontsize=9)
    axes[1, 1].grid(axis='y', alpha=0.3)
    for i, v in enumerate(param_counts):
        axes[1, 1].text(i, v + 0.1, '{:.1f}M'.format(v), ha='center', fontsize=10, fontweight='bold')
    
    # â˜… 6. FLOPs (ìƒˆë¡œ ì¶”ê°€)
    axes[1, 2].bar(range(len(model_names)), flops_values, color=colors, alpha=0.8)
    axes[1, 2].set_ylabel('FLOPs (Million)', fontsize=11, fontweight='bold')
    axes[1, 2].set_title('â‘¥ ë¶€ë™ì†Œìˆ˜ì  ì—°ì‚°ìˆ˜ (FLOPs)', fontsize=12, fontweight='bold')
    axes[1, 2].set_xticks(range(len(model_names)))
    axes[1, 2].set_xticklabels([m.replace(' + ', '\n+ ') for m in model_names], fontsize=9)
    axes[1, 2].grid(axis='y', alpha=0.3)
    for i, v in enumerate(flops_values):
        axes[1, 2].text(i, v + 50, '{:.0f}M'.format(v), ha='center', fontsize=10, fontweight='bold')
    
    # 7. ì„±ëŠ¥-ë©”ëª¨ë¦¬ íŠ¸ë ˆì´ë“œì˜¤í”„ (Latency Ã— Memory)
    tradeoff = [lat * mem for lat, mem in zip(latencies, memory_usage)]
    axes[2, 0].bar(range(len(model_names)), tradeoff, color=colors, alpha=0.8)
    axes[2, 0].set_ylabel('íŠ¸ë ˆì´ë“œì˜¤í”„ (msÃ—MB)', fontsize=11, fontweight='bold')
    axes[2, 0].set_title('â‘¦ ì„±ëŠ¥-ë©”ëª¨ë¦¬ íŠ¸ë ˆì´ë“œì˜¤í”„\n(ë‚®ì„ìˆ˜ë¡ ìš°ìˆ˜)', fontsize=12, fontweight='bold')
    axes[2, 0].set_xticks(range(len(model_names)))
    axes[2, 0].set_xticklabels([m.replace(' + ', '\n+ ') for m in model_names], fontsize=9)
    axes[2, 0].grid(axis='y', alpha=0.3)
    for i, v in enumerate(tradeoff):
        axes[2, 0].text(i, v + 10, '{:.0f}'.format(v), ha='center', fontsize=10, fontweight='bold')
    
    # ë‚˜ë¨¸ì§€ ë¹ˆ ì„œë¸Œí”Œë¡¯ ìˆ¨ê¸°ê¸°
    axes[2, 1].axis('off')
    axes[2, 2].axis('off')
    
    plt.tight_layout(rect=[0, 0.03, 1, 0.95])
    
    # ì €ì¥
    output_path = 'benchmark_comparison.png'
    plt.savefig(output_path, dpi=150, bbox_inches='tight')
    print("âœ… ê·¸ë˜í”„ ì €ì¥: {}".format(output_path))
    
    # ìƒì„¸ í…Œì´ë¸”
    plot_comparison_table(results)
    
    plt.show()

def plot_comparison_table(results):
    """ìƒì„¸ ë¹„êµ í…Œì´ë¸”"""
    fig, ax = plt.subplots(figsize=(16, 6))
    ax.axis('off')
    
    # í…Œì´ë¸” ë°ì´í„°
    model_names = list(results.keys())
    table_data = []
    
    for model_name in model_names:
        stats = results[model_name]
        table_data.append([
            model_name,
            '{:.2f}ms'.format(stats['mean_latency_ms']),
            '{:.1f}'.format(stats['fps']),
            '{:.1f}MB'.format(stats['cpu_memory_mb']),
            '{:.2f}MB'.format(stats['model_size_mb']),
            '{:.1f}M'.format(stats['total_params'] / 1e6),
            '{:.0f}M'.format(stats['flops_millions']),  # â˜… FLOPs ì¶”ê°€
        ])
    
    # í…Œì´ë¸” ìƒì„±
    table = ax.table(
        cellText=table_data,
        colLabels=['ëª¨ë¸', 'ì§€ì—°ì‹œê°„', 'FPS', 'CPU ë©”ëª¨ë¦¬', 'ëª¨ë¸ í¬ê¸°', 'íŒŒë¼ë¯¸í„°', 'FLOPs'],
        cellLoc='center',
        loc='center',
        colWidths=[0.22, 0.12, 0.10, 0.13, 0.13, 0.12, 0.12]  # â˜… FLOPs ì¶”ê°€
    )
    
    table.auto_set_font_size(False)
    table.set_fontsize(10)
    table.scale(1, 2.5)
    
    # í—¤ë” ìŠ¤íƒ€ì¼
    for i in range(len(table_data[0]) + 1):
        table[(0, i)].set_facecolor('#4CAF50')
        table[(0, i)].set_text_props(weight='bold', color='white')
    
    # í–‰ ìƒ‰ìƒ
    colors = ['#E8F5E9', '#FFF3E0', '#E3F2FD', '#FCE4EC', '#F3E5F5']
    for i in range(len(table_data)):
        for j in range(len(table_data[0]) + 1):
            table[(i + 1, j)].set_facecolor(colors[i % len(colors)])
    
    plt.title('Jetson Nano ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ ìƒì„¸í‘œ', fontsize=14, fontweight='bold', pad=20)
    plt.savefig('benchmark_comparison_table.png', dpi=150, bbox_inches='tight')
    print("âœ… ìƒì„¸í‘œ ì €ì¥: benchmark_comparison_table.png")

if __name__ == "__main__":
    main()
