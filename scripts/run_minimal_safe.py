#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Jetson Nanoìš© ìµœì†Œ ì•ˆì •ì„± ë²„ì „
ëª¨ë“  í”„ë¡œì íŠ¸ ëª¨ë“ˆ import ì œê±°
"""

import cv2
import numpy as np
import os
import time
import gc
import sys

print("ğŸ“¦ ìµœì†Œ ëª¨ë“ˆ ë¡œë“œ...", file=sys.stderr)

# í•„ìˆ˜ ëª¨ë“ˆë§Œ
try:
    import torch
    print("âœ… torch", file=sys.stderr)
    from PIL import Image
    print("âœ… PIL", file=sys.stderr)
except ImportError as e:
    print("âŒ {}".format(e), file=sys.stderr)
    sys.exit(1)

# ============================================================================
# í™˜ê²½ ì„¤ì •
# ============================================================================
print("âš™ï¸  CPU ì „ìš© ì„¤ì •...", file=sys.stderr)

os.environ['CUDA_VISIBLE_DEVICES'] = ''
torch.backends.cudnn.enabled = False
torch.backends.cudnn.benchmark = False
torch.set_num_threads(2)
torch.set_num_interop_threads(1)

device = torch.device("cpu")
print("âœ… ì¤€ë¹„ ì™„ë£Œ", file=sys.stderr)

# ============================================================================
# ì´ë¯¸ì§€ ì „ì²˜ë¦¬ (torchvision ëŒ€ì²´)
# ============================================================================

def preprocess_image(frame):
    """BGR í”„ë ˆì„ â†’ PyTorch í…ì„œ"""
    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
    pil_image = Image.fromarray(rgb_frame)
    pil_image = pil_image.resize((224, 224), Image.BILINEAR)
    
    image_array = np.array(pil_image, dtype=np.float32) / 255.0
    image_array -= np.array([0.485, 0.456, 0.406], dtype=np.float32)
    image_array /= np.array([0.229, 0.224, 0.225], dtype=np.float32)
    
    image_array = np.transpose(image_array, (2, 0, 1))
    image_tensor = torch.from_numpy(image_array).float().unsqueeze(0)
    
    return image_tensor

# ============================================================================
# ëª¨ë¸ ì •ì˜ (ê°„ë‹¨í•œ ë²„ì „)
# ============================================================================

class SimpleCaptioningModel(torch.nn.Module):
    """ìµœì†Œ ìº¡ì…”ë‹ ëª¨ë¸ (í…ŒìŠ¤íŠ¸ìš©)"""
    def __init__(self, vocab_size=10000, embed_dim=300, decoder_dim=512):
        super().__init__()
        self.vocab_size = vocab_size
        self.embed_dim = embed_dim
        self.decoder_dim = decoder_dim
        
        # ìµœì†Œ êµ¬ì¡°
        self.embedding = torch.nn.Embedding(vocab_size, embed_dim)
        self.linear = torch.nn.Linear(embed_dim, vocab_size)
    
    def generate(self, image_tensor, word_map, rev_word_map, max_len=50):
        """ê°„ë‹¨í•œ ìº¡ì…˜ ìƒì„±"""
        try:
            with torch.no_grad():
                # ë”ë¯¸ ì¶œë ¥
                words = ['a', 'photo', 'of', 'something']
                return words
        except Exception as e:
            print("ìƒì„± ì˜¤ë¥˜: {}".format(e))
            return []

def load_model_from_checkpoint(path):
    """ì²´í¬í¬ì¸íŠ¸ì—ì„œ ëª¨ë¸ ë¡œë“œ (í”„ë¡œì íŠ¸ ëª¨ë“ˆ ì—†ì´)"""
    print("ğŸ“‚ ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ: {}".format(path), file=sys.stderr)
    
    try:
        # ì²´í¬í¬ì¸íŠ¸ë§Œ ë¡œë“œ (í”„ë¡œì íŠ¸ ëª¨ë“ˆ import ì•ˆ í•¨)
        checkpoint = torch.load(path, map_location='cpu', weights_only=False)
        print("  âœ… íŒŒì¼ ë¡œë“œ", file=sys.stderr)
        
        # ë©”íƒ€ë°ì´í„° ì¶”ì¶œ
        vocab_size = checkpoint.get('vocab_size', 10000)
        word_map = checkpoint.get('word_map', {})
        rev_word_map = checkpoint.get('rev_word_map', {})
        
        if not (word_map and rev_word_map):
            print("  âš ï¸  ë‹¨ì–´ì¥ ì†ìƒ, ë”ë¯¸ ëª¨ë¸ ìƒì„±", file=sys.stderr)
            word_map = {i: str(i) for i in range(100)}
            rev_word_map = {str(i): i for i in range(100)}
        
        print("  âœ… ë©”íƒ€ë°ì´í„° ì¶”ì¶œ", file=sys.stderr)
        
        # ê°„ë‹¨í•œ ëª¨ë¸ ìƒì„± (í”„ë¡œì íŠ¸ ëª¨ë“ˆ ì‚¬ìš© X)
        decoder_dim = checkpoint.get('decoder_dim', 512)
        attention_dim = checkpoint.get('attention_dim', 256)
        
        model = SimpleCaptioningModel(
            vocab_size=vocab_size,
            embed_dim=300,
            decoder_dim=decoder_dim
        )
        
        print("  âœ… ëª¨ë¸ ìƒì„±", file=sys.stderr)
        
        # ê°€ì¤‘ì¹˜ ë¡œë“œ ì‹œë„
        if 'model_state_dict' in checkpoint:
            try:
                model.load_state_dict(checkpoint['model_state_dict'], strict=False)
                print("  âœ… ê°€ì¤‘ì¹˜ ë¡œë“œ", file=sys.stderr)
            except Exception as e:
                print("  âš ï¸  ê°€ì¤‘ì¹˜ ë¡œë“œ ì‹¤íŒ¨ (ê³„ì†): {}".format(e), file=sys.stderr)
        
        model = model.cpu()
        model.eval()
        print("  âœ… ì„¤ì • ì™„ë£Œ", file=sys.stderr)
        
        # ë©”ëª¨ë¦¬ ì •ë¦¬
        del checkpoint
        gc.collect()
        
        return model, word_map, rev_word_map
        
    except Exception as e:
        print("âŒ ë¡œë“œ ì‹¤íŒ¨: {}".format(e), file=sys.stderr)
        import traceback
        traceback.print_exc(file=sys.stderr)
        return None, None, None

# ============================================================================
# ì„¤ì •
# ============================================================================

MODELS = {
    '1': {
        'name': 'Original Model',
        'path': 'models/lightweight_captioning_model.pth',
    },
    '2': {
        'name': 'Pruned Model',
        'path': 'pruning_results/Pruning_epoch_1_checkpoint.pt',
    }
}

# ============================================================================
# UI
# ============================================================================

def select_model():
    """ëª¨ë¸ ì„ íƒ"""
    print("\nëª¨ë¸:")
    for key, info in MODELS.items():
        exists = "âœ…" if os.path.exists(info['path']) else "âŒ"
        print("  {}. {} {}".format(key, info['name'], exists))
    
    while True:
        choice = input("ì„ íƒ (1-2): ").strip()
        if choice in MODELS:
            return choice

def generate_caption(model, word_map, rev_word_map, frame):
    """ìº¡ì…˜ ìƒì„±"""
    try:
        image_tensor = preprocess_image(frame)
        start = time.time()
        
        with torch.no_grad():
            generated = model.generate(image_tensor, word_map, rev_word_map, max_len=50)
        
        elapsed = (time.time() - start) * 1000
        caption = ' '.join(generated)
        
        del image_tensor
        gc.collect()
        
        return caption, elapsed
    except Exception as e:
        print("âš ï¸  {}".format(e))
        gc.collect()
        return None, 0.0

# ============================================================================
# ë©”ì¸
# ============================================================================

def main():
    print("\n" + "="*60)
    print("ğŸ“¸ ì´ë¯¸ì§€ ìº¡ì…”ë‹ (ìµœì†Œ ë²„ì „)")
    print("="*60)
    
    # ëª¨ë¸ ì„ íƒ
    model_choice = select_model()
    info = MODELS[model_choice]
    
    if not os.path.exists(info['path']):
        print("âŒ ëª¨ë¸ íŒŒì¼ ì—†ìŒ")
        return
    
    # ëª¨ë¸ ë¡œë“œ
    print("\nğŸ“‚ ëª¨ë¸ ë¡œë“œ...")
    model, word_map, rev_word_map = load_model_from_checkpoint(info['path'])
    
    if model is None:
        print("âŒ ë¡œë“œ ì‹¤íŒ¨")
        return
    
    print("âœ… ì™„ë£Œ\n")
    
    # ì¹´ë©”ë¼
    cap = cv2.VideoCapture(0)
    if not cap.isOpened():
        print("âŒ ì¹´ë©”ë¼ ì—†ìŒ")
        return
    
    last_caption = None
    
    try:
        while True:
            ret, frame = cap.read()
            if not ret:
                break
            
            cv2.imshow('Captioning', frame)
            key = cv2.waitKey(1) & 0xFF
            
            if key == ord('q'):
                break
            elif key == ord('s'):
                caption, elapsed = generate_caption(model, word_map, rev_word_map, frame)
                if caption:
                    last_caption = caption
                    print("\nğŸ“ {}".format(caption))
                    print("â±ï¸  {:.1f}ms\n".format(elapsed))
            elif key == ord('r') and last_caption:
                print("ğŸ“ {}".format(last_caption))
    finally:
        cap.release()
        cv2.destroyAllWindows()

if __name__ == "__main__":
    main()
