"""
Quantization ë²¤ì¹˜ë§ˆí¬ ìŠ¤í¬ë¦½íŠ¸ (ìµœì¢… ìˆ˜ì •ë¨)
- ìˆ˜ì •ì‚¬í•­ 1: IndexError ë°©ì§€ (files[567] -> files[0])
- ìˆ˜ì •ì‚¬í•­ 2: QAT í•™ìŠµ ì‹œ Mixed Precision ë¹„í™œì„±í™” (ì •í™•ë„ í–¥ìƒ)
- ìˆ˜ì •ì‚¬í•­ 3: ë¶ˆí•„ìš”í•œ Wrapper í´ë˜ìŠ¤ ì‚­ì œ
"""
import torch
import torch.nn as nn
from torch.quantization import quantize_fx
import numpy as np
import os
import time
import psutil
import matplotlib.pyplot as plt
import matplotlib
from copy import deepcopy
import gc
from collections import defaultdict
from PIL import Image
from torchvision import transforms
import platform
import warnings
import sys # ì¶”ê°€ë¨

warnings.filterwarnings('ignore')

# -------------------------------------------------------------------------
# [ì¤‘ìš”] ëª¨ë¸ import ê²½ë¡œ í™•ì¸
# -------------------------------------------------------------------------
try:
    from src.muti_modal_model.model import MobileNetCaptioningModel
except ImportError:
    print("âš ï¸ ëª¨ë¸ í´ë˜ìŠ¤ë¥¼ importí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê²½ë¡œë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.")
    # ë”ë¯¸ í´ë˜ìŠ¤
    class MobileNetCaptioningModel(nn.Module):
        def __init__(self, vocab_size, embed_dim):
            super().__init__()
            self.emb = nn.Embedding(vocab_size, embed_dim)
            self.gru = nn.GRU(embed_dim, 512)
            self.fc = nn.Linear(512, vocab_size)
        def generate(self, img, wm, rwm, max_len):
            return ["<start>", "a", "test", "caption", "<end>"]

# NLTK ì„¤ì •
try:
    from nltk.translate.meteor_score import meteor_score
    from nltk.tokenize import word_tokenize
    import nltk
    nltk.download('punkt', quiet=True)
    nltk.download('wordnet', quiet=True)
    METEOR_AVAILABLE = True
except ImportError:
    print("âš ï¸ nltkê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. METEOR ì ìˆ˜ ê³„ì‚° ë¶ˆê°€.")
    METEOR_AVAILABLE = False

# [ì‚­ì œë¨] QuantizedEncoderWrapperëŠ” FX ëª¨ë“œì—ì„œ í•„ìš” ì—†ìŠµë‹ˆë‹¤.

# ============================================================================
# ì„¤ì •
# ============================================================================
matplotlib.use('Agg')

# í•œê¸€ í°íŠ¸ ì„¤ì •
os_name = platform.system()
if os_name == 'Windows':
    plt.rcParams['font.family'] = 'Malgun Gothic'
    plt.rcParams['axes.unicode_minus'] = False 
elif os_name == 'Darwin': 
    plt.rcParams['font.family'] = 'AppleGothic'
    plt.rcParams['axes.unicode_minus'] = False
elif os_name == 'Linux':
    plt.rcParams['font.family'] = 'NanumGothic'
    plt.rcParams['axes.unicode_minus'] = False
else:
    plt.rcParams['axes.unicode_minus'] = False

MODEL_PATH = "models/lightweight_captioning_model.pth"
TEST_IMAGE_DIR = "assets/images"
CAPTIONS_FILE = "assets/captions.txt"
OUTPUT_DIR = "benchmark_results"
NUM_RUNS = 50

# QAT ì„¤ì •
USE_QAT = True 
QAT_EPOCHS = 20

# ë””ë°”ì´ìŠ¤ ì„ íƒ
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
if hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():
    device = torch.device("mps")
print(f"ğŸš€ ì‹¤í–‰ ë””ë°”ì´ìŠ¤: {device}")

# ì´ë¯¸ì§€ ì „ì²˜ë¦¬
transform = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
])

# ============================================================================
# ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜
# ============================================================================
def count_parameters(model):
    total_params = sum(p.numel() for p in model.parameters())
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    return total_params, trainable_params

def get_model_size_mb(model):
    param_size = 0
    buffer_size = 0
    for param in model.parameters():
        param_size += param.nelement() * param.element_size()
    for buffer in model.buffers():
        buffer_size += buffer.nelement() * buffer.element_size()
    return (param_size + buffer_size) / 1024 / 1024

def get_peak_memory_mb():
    process = psutil.Process(os.getpid())
    return process.memory_info().rss / 1024 / 1024

def load_data():
    """í…ŒìŠ¤íŠ¸ ì´ë¯¸ì§€ì™€ ì°¸ì¡° ìº¡ì…˜ ë¡œë“œ"""
    img_tensor = None
    filename = None
    
    if os.path.exists(TEST_IMAGE_DIR):
        files = [f for f in os.listdir(TEST_IMAGE_DIR) if f.lower().endswith(('.jpg', '.png', '.jpeg'))]
        if files:
            import random
            # [ìˆ˜ì •ë¨] í•˜ë“œì½”ë”©ëœ ì¸ë±ìŠ¤ ì œê±° (IndexError ë°©ì§€)
            # íŒŒì¼ì´ ìˆìœ¼ë©´ ì²« ë²ˆì§¸ íŒŒì¼ ì‚¬ìš©, ì—†ìœ¼ë©´ ë”ë¯¸ ì‚¬ìš©
            filename = files[2] 
            img_path = os.path.join(TEST_IMAGE_DIR, filename)
            try:
                img = Image.open(img_path).convert('RGB')
                img_tensor = transform(img).unsqueeze(0).to(device)
                print(f"ğŸ“¸ í…ŒìŠ¤íŠ¸ ì´ë¯¸ì§€: {filename}")
            except Exception as e:
                print(f"âš ï¸ ì´ë¯¸ì§€ ë¡œë“œ ì¤‘ ì—ëŸ¬: {e}")

    if img_tensor is None:
        print("âš ï¸ ì´ë¯¸ì§€ë¥¼ ì°¾ì„ ìˆ˜ ì—†ì–´ ë”ë¯¸ ë°ì´í„°ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")
        img_tensor = torch.randn(1, 3, 224, 224).to(device)
        filename = "dummy"

    ref_caption = None
    if os.path.exists(CAPTIONS_FILE) and filename != "dummy":
        with open(CAPTIONS_FILE, 'r', encoding='utf-8') as f:
            for line in f:
                if filename in line:
                    parts = line.split(',', 1) if ',' in line else line.split('\t', 1)
                    if len(parts) > 1:
                        if parts[0].strip() == filename:
                            ref_caption = parts[1].strip()
                            print(f"ğŸ“ ì°¸ì¡° ìº¡ì…˜: {ref_caption}")
                            break
                        else:
                            continue
    
    return img_tensor, ref_caption

def calculate_meteor(gen_list, ref_str):
    if not METEOR_AVAILABLE or not ref_str:
        return None
    try:
        gen_str = ' '.join([w for w in gen_list if w not in ['<start>', '<end>', '<pad>', '<unk>']])
        return meteor_score([word_tokenize(ref_str.lower())], word_tokenize(gen_str.lower()))
    except:
        return None

# ============================================================================
# ëª¨ë¸ ë¡œë“œ ë° ë³€í™˜ í•¨ìˆ˜
# ============================================================================
def load_base_model():
    if not os.path.exists(MODEL_PATH):
        print(f"âŒ ëª¨ë¸ íŒŒì¼ ì—†ìŒ: {MODEL_PATH}")
        return MobileNetCaptioningModel(vocab_size=5000, embed_dim=300).to(device), {}, {}

    checkpoint = torch.load(MODEL_PATH, map_location=device)
    vocab_size = checkpoint.get('vocab_size', 5000)
    
    model = MobileNetCaptioningModel(vocab_size=vocab_size, embed_dim=300).to(device)
    if 'model_state_dict' in checkpoint:
        model.load_state_dict(checkpoint['model_state_dict'])
    
    model.eval()
    return model, checkpoint.get('word_map', {}), checkpoint.get('rev_word_map', {})

def prepare_calibration_dataset(word_map, num_samples=100, max_len=20):
    """ì •ì  ì–‘ìí™”ë¥¼ ìœ„í•œ Calibration ë°ì´í„°ì…‹ ì¤€ë¹„"""
    calibration_images = []
    
    if not os.path.exists(TEST_IMAGE_DIR):
        for _ in range(num_samples):
            calibration_images.append(torch.randn(1, 3, 224, 224))
        return calibration_images, None
    
    image_files = [f for f in os.listdir(TEST_IMAGE_DIR) if f.lower().endswith(('.jpg', '.jpeg', '.png'))]
    if not image_files:
        for _ in range(num_samples):
            calibration_images.append(torch.randn(1, 3, 224, 224))
        return calibration_images, None
    
    import random
    selected_files = random.sample(image_files, min(num_samples, len(image_files)))
    
    print(f"   ğŸ“Š Calibration ë°ì´í„°ì…‹ ì¤€ë¹„ ì¤‘: {len(selected_files)}ê°œ ì´ë¯¸ì§€")
    
    for filename in selected_files:
        try:
            img_path = os.path.join(TEST_IMAGE_DIR, filename)
            img = Image.open(img_path).convert('RGB')
            img_tensor = transform(img).unsqueeze(0) 
            calibration_images.append(img_tensor)
        except Exception:
            continue
            
    while len(calibration_images) < num_samples:
        calibration_images.append(torch.randn(1, 3, 224, 224))
    
    return calibration_images[:num_samples], None

def convert_to_int8(model, word_map=None, use_qat=False, qat_epochs=2):
    if use_qat:
        return convert_to_int8_qat(model, word_map, qat_epochs)
    else:
        return convert_to_int8_static(model, word_map)

def convert_to_int8_static(model, word_map=None):
    """Int8 Static Quantization (FX Graph Mode)"""
    print("   ğŸ‘‰ Int8 ë³€í™˜ ì¤‘ (Static Quantization: FX Graph Mode)...")
    
    import platform
    machine = platform.machine().lower()
    if 'arm' in machine or 'aarch64' in machine:
        backend = 'qnnpack'
    elif 'x86' in machine or 'amd64' in machine:
        backend = 'fbgemm'
    else:
        backend = 'qnnpack'
    
    torch.backends.quantized.engine = backend
    print(f"   âš™ï¸ Quantization Engine: {backend}")

    model_cpu = deepcopy(model).cpu()
    model_cpu.eval()

    if word_map is None:
        print("   âš ï¸ word_mapì´ ì—†ì–´ Dynamic Quantizationìœ¼ë¡œ fallback")
        return torch.quantization.quantize_dynamic(model_cpu, {nn.Linear}, dtype=torch.qint8)

    print("   ğŸ“Š Calibration ë°ì´í„° ì¤€ë¹„ ì¤‘...")
    cal_images, _ = prepare_calibration_dataset(word_map, num_samples=20)
    example_input = cal_images[0]

    try:
        qconfig_dict = {"": torch.quantization.get_default_qconfig(backend)}
        
        print("   ğŸ”§ ì¸ì½”ë” ìë™ ìœµí•© ë° ì¤€ë¹„ (Prepare FX)...")
        model_cpu.encoder = quantize_fx.prepare_fx(model_cpu.encoder, qconfig_dict, example_input)

        print("   ğŸ”„ Calibration ì§„í–‰ ì¤‘ (ì¸ì½”ë”)...")
        with torch.no_grad():
            for img in cal_images:
                model_cpu.encoder(img)

        print("   âš¡ ì¸ì½”ë” ë³€í™˜ (Convert FX)...")
        model_cpu.encoder = quantize_fx.convert_fx(model_cpu.encoder)

        print("   ğŸ”„ ë””ì½”ë” ë™ì  ì–‘ìí™” ì ìš©...")
        quantized_model = torch.quantization.quantize_dynamic(
            model_cpu,
            {nn.Linear, nn.GRU, nn.LSTM},
            dtype=torch.qint8
        )
        
        print("   âœ… ì •ì (Encoder/FX) + ë™ì (Decoder) ì–‘ìí™” ì™„ë£Œ!")
        return quantized_model

    except Exception as e:
        print(f"   âš ï¸ FX ì •ì  ì–‘ìí™” ì‹¤íŒ¨: {e}")
        return torch.quantization.quantize_dynamic(
            deepcopy(model).cpu(),
            {nn.Linear, nn.GRU},
            dtype=torch.qint8
        )

def convert_to_int8_qat(model, word_map=None, qat_epochs=2):
    """Int8 QAT (Quantization-Aware Training) - [ìˆ˜ì •ë¨: FP32 í•™ìŠµ ê°•ì œ]"""
    print(f"   ğŸ‘‰ Int8 ë³€í™˜ ì¤‘ (ì •ì  ì–‘ìí™” â†’ QAT Fine-tuning: {qat_epochs} epochs)...")
    
    import platform
    machine = platform.machine().lower()
    backend = 'qnnpack' if ('arm' in machine or 'aarch64' in machine) else 'fbgemm'
    torch.backends.quantized.engine = backend
    print(f"   âš™ï¸ Quantization Engine: {backend}")

    model_cpu = deepcopy(model).cpu()
    model_cpu.train() 
    
    if word_map is None:
        return torch.quantization.quantize_dynamic(model_cpu, {nn.Linear}, dtype=torch.qint8)
    
    cal_images, _ = prepare_calibration_dataset(word_map, num_samples=20)
    example_input = cal_images[0]
    
    # [ì„¤ì •] QAT Config
    qconfig_dict = {"": torch.quantization.get_default_qat_qconfig(backend)}
    
    print("   ğŸ”§ ì¸ì½”ë” QAT ì¤€ë¹„ (Prepare QAT FX)...")
    model_cpu.encoder = quantize_fx.prepare_qat_fx(model_cpu.encoder, qconfig_dict, example_input)
    
    # [ì´ˆê¸°í™”] Calibration
    print("   ğŸ”„ ì´ˆê¸° Calibration (Start)...")
    model_cpu.encoder.eval() # í†µê³„ ìˆ˜ì§‘ë§Œ
    with torch.no_grad():
        for img in cal_images:
            model_cpu.encoder(img)
    model_cpu.train() # ë‹¤ì‹œ í•™ìŠµ ëª¨ë“œ
    
    # [í•™ìŠµ] QAT Fine-tuning
    print(f"\n   [3ë‹¨ê³„] QAT Fine-tuning ì‹œì‘ ({qat_epochs} epochs)...")
    
    # Dataset/DataLoader ì„¤ì •
    from torch.utils.data import DataLoader, Dataset
    
    # CaptionDataset ì •ì˜ (ê°„ì†Œí™”)
    class CaptionDataset(Dataset):
        def __init__(self, images_dir, captions_file, transform, word_map, max_len=50):
            self.images_dir = images_dir
            self.transform = transform
            self.word_map = word_map
            self.max_len = max_len
            self.pairs = []
            
            # íŒŒì¼ ì½ê¸° ë° ë§¤í•‘ ë¡œì§ (ìƒëµ ê°€ëŠ¥í•˜ë‚˜ ì•ˆì „ì„ ìœ„í•´ ìœ ì§€)
            if os.path.exists(captions_file):
                 with open(captions_file, 'r', encoding='utf-8') as f:
                    for line in f:
                        if ',' in line:
                            parts = line.split(',', 1)
                            if len(parts) == 2:
                                self.pairs.append((parts[0].strip(), parts[1].strip()))

        def __len__(self):
            return len(self.pairs)
        
        def __getitem__(self, idx):
            img_name, cap_text = self.pairs[idx]
            # ì´ë¯¸ì§€ ë¡œë“œ
            try:
                img = Image.open(os.path.join(self.images_dir, img_name)).convert('RGB')
                if self.transform: img = self.transform(img)
            except:
                img = torch.zeros(3, 224, 224)
            
            # ìº¡ì…˜ ì¸ì½”ë”©
            tokens = cap_text.lower().split()
            enc = [self.word_map.get('<start>', 1)] + \
                  [self.word_map.get(t, self.word_map.get('<unk>', 3)) for t in tokens[:self.max_len-2]] + \
                  [self.word_map.get('<end>', 2)]
            while len(enc) < self.max_len: enc.append(0)
            return img, torch.LongTensor(enc[:self.max_len])

    dataset = CaptionDataset(TEST_IMAGE_DIR, CAPTIONS_FILE, transform, word_map)
    
    if len(dataset) == 0:
        print("   âš ï¸ í•™ìŠµ ë°ì´í„°ê°€ ë¶€ì¡±í•´ Static Quantizationìœ¼ë¡œ ëŒ€ì²´í•©ë‹ˆë‹¤.")
        return convert_to_int8_static(model, word_map)

    dataloader = DataLoader(dataset, batch_size=4, shuffle=True)
    
    qat_device = torch.device("cpu") 
    
    # CUDA(NVIDIA)ê°€ ìˆëŠ” ê²½ìš°ì—ë§Œ GPU ì‚¬ìš© (CUDAëŠ” ì§€ì›í•¨)
    if torch.cuda.is_available():
        qat_device = torch.device("cuda")
        print(f"   ğŸš€ QAT í•™ìŠµ ë””ë°”ì´ìŠ¤: CUDA (Precision: FP32)")
    else:
        print(f"   ğŸ’» QAT í•™ìŠµ ë””ë°”ì´ìŠ¤: CPU (MPS ë¯¸ì§€ì›ìœ¼ë¡œ ì¸í•œ ê°•ì œ ì„¤ì •)")
    model_cpu = model_cpu.to(qat_device)
    
    optimizer = torch.optim.Adam(model_cpu.parameters(), lr=4e-4) # í•™ìŠµë¥  ë‚®ì¶¤
    criterion = nn.CrossEntropyLoss(ignore_index=0)
    
    for epoch in range(qat_epochs):
        epoch_loss = 0
        steps = 0
        for i, (imgs, caps) in enumerate(dataloader):
            if i > 50: break # ì‹œê°„ ì ˆì•½ì„ ìœ„í•´ epochë‹¹ 50ë°°ì¹˜ë§Œ
            
            imgs = imgs.to(qat_device)
            caps = caps.to(qat_device)
            
            optimizer.zero_grad()
            
            # [ìˆ˜ì •] Autocast ì œê±° -> FP32 ê°•ì œ
            outputs, _ = model_cpu(imgs, caps)
            targets = caps[:, 1:]
            outputs = outputs[:, :targets.shape[1], :]
            
            loss = criterion(outputs.reshape(-1, len(word_map)), targets.reshape(-1))
            loss.backward()
            optimizer.step()
            
            epoch_loss += loss.item()
            steps += 1
            
        print(f"      Epoch {epoch+1}/{qat_epochs} Loss: {epoch_loss/steps:.4f}")

    print("   ğŸ”„ CPUë¡œ ì´ë™ ë° ë³€í™˜ ì¤€ë¹„...")
    model_cpu = model_cpu.cpu()
    model_cpu.eval() # Convert ì „ì—ëŠ” ë°˜ë“œì‹œ Eval ëª¨ë“œ
    
    print("   âš¡ QAT ëª¨ë¸ ë³€í™˜ (Convert FX)...")
    model_cpu.encoder = quantize_fx.convert_fx(model_cpu.encoder)
    
    print("   ğŸ”„ ë””ì½”ë” ë™ì  ì–‘ìí™” ì ìš©...")
    quantized_model = torch.quantization.quantize_dynamic(
        model_cpu,
        {nn.Linear, nn.GRU, nn.LSTM},
        dtype=torch.qint8
    )
    
    return quantized_model

# ============================================================================
# ë²¤ì¹˜ë§ˆí¬ ì—”ì§„ (ê¸°ì¡´ê³¼ ë™ì¼)
# ============================================================================
def run_benchmark(model, img_tensor, wm, rwm, precision_name, ref_caption=None):
    print(f"\n[{precision_name}] ë²¤ì¹˜ë§ˆí¬ ì‹œì‘...")
    model_device = next(model.parameters()).device
    inp = img_tensor.clone().detach().to(model_device)
    
    if precision_name == "FP16": inp = inp.half()
    elif "Int8" in precision_name: inp = inp.float().cpu()

    # Warm-up
    with torch.no_grad():
        try: _ = model.generate(inp, wm, rwm, 20)
        except: pass

    # Time
    latencies = []
    start_mem = get_peak_memory_mb()
    peak_mem = start_mem
    
    for i in range(NUM_RUNS):
        gc.collect()
        if device.type == 'cuda': 
            torch.cuda.empty_cache()
            torch.cuda.synchronize()
        
        start = time.time()
        with torch.no_grad():
            gen_seq = model.generate(inp, wm, rwm, 20)
        if device.type == 'cuda': torch.cuda.synchronize()
        
        latencies.append((time.time() - start) * 1000)
        
        current_mem = get_peak_memory_mb()
        peak_mem = max(peak_mem, current_mem)
        
        if (i + 1) % 10 == 0:
            print(f"   ì§„í–‰: {i+1}/{NUM_RUNS}")
    
    # METEOR
    meteor_scores = []
    example_caption = "N/A"
    if ref_caption:
        for _ in range(5):
            with torch.no_grad(): 
                gen = model.generate(inp, wm, rwm, 20)
            score = calculate_meteor(gen, ref_caption)
            if score: meteor_scores.append(score)
            if _ == 0:
                example_caption = ' '.join([w for w in gen if w not in ['<start>', '<end>', '<pad>', '<unk>']])
            
    avg_meteor = np.mean(meteor_scores) if meteor_scores else None
    
    # íŒŒë¼ë¯¸í„° ê°œìˆ˜ ê³„ì‚°
    total_params, trainable_params = count_parameters(model)
    
    # ê²°ê³¼ ì •ë¦¬
    avg_time = np.mean(latencies)
    std_time = np.std(latencies)
    size_mb = get_model_size_mb(model)
    memory_usage = peak_mem - start_mem
    
    print(f"   â±ï¸ í‰ê·  ì‹œê°„: {avg_time:.2f} Â± {std_time:.2f} ms")
    print(f"   ğŸ’¾ ëª¨ë¸ í¬ê¸°: {size_mb:.2f} MB")
    print(f"   ğŸ“Š íŒŒë¼ë¯¸í„° ê°œìˆ˜: {total_params:,} (í•™ìŠµ ê°€ëŠ¥: {trainable_params:,})")
    print(f"   ğŸ§  ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: {memory_usage:.2f} MB")
    if avg_meteor: 
        print(f"   â­ METEOR: {avg_meteor:.4f}")
    print(f"   ğŸ“ ì˜ˆì‹œ ìº¡ì…˜: {example_caption}")
    
    return {
        'precision': precision_name,
        'mean_time_ms': avg_time,
        'std_time_ms': std_time,
        'min_time_ms': np.min(latencies),
        'max_time_ms': np.max(latencies),
        'model_size_mb': size_mb,
        'memory_usage_mb': memory_usage,
        'meteor_score': avg_meteor,
        'inference_times': latencies,  # ì¶”ê°€: Box plotìš©
        'total_params': total_params,  # ì¶”ê°€: íŒŒë¼ë¯¸í„° ê°œìˆ˜
        'trainable_params': trainable_params,
        'example_caption': example_caption
    }

def plot_benchmark(results):
    """
    ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼ë¥¼ ì¢…í•©ì ìœ¼ë¡œ ì‹œê°í™”í•˜ì—¬ ì €ì¥í•˜ëŠ” í•¨ìˆ˜
    (ì¶”ë¡  ì‹œê°„, ëª¨ë¸ í¬ê¸°, ë©”ëª¨ë¦¬, METEOR, íŒŒë¼ë¯¸í„°, ì‹œê°„ ë¶„í¬)
    """
    if not results:
        print("âŒ ê²°ê³¼ê°€ ì—†ì–´ plotì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    # ì €ì¥ ë””ë ‰í† ë¦¬ ìƒì„±
    os.makedirs(OUTPUT_DIR, exist_ok=True)
    
    # 1. ë°ì´í„° ì¶”ì¶œ
    precisions = [r['precision'] for r in results]
    mean_times = [r['mean_time_ms'] for r in results]
    std_times = [r['std_time_ms'] for r in results]
    model_sizes = [r['model_size_mb'] for r in results]
    memory_usages = [r['memory_usage_mb'] for r in results]
    
    # METEOR ì ìˆ˜ ì²˜ë¦¬ (Noneì´ ì•„ë‹Œ ê²½ìš°ë§Œ)
    meteor_scores = []
    for r in results:
        if r.get('meteor_score') is not None:
            meteor_scores.append(r['meteor_score'])
        else:
            meteor_scores.append(0)
    
    # inference_times ì²˜ë¦¬ (ë¹ˆ ë¦¬ìŠ¤íŠ¸ ë°©ì§€)
    inference_times_list = []
    for r in results:
        times = r.get('inference_times', [])
        if times and len(times) > 0:
            inference_times_list.append(times)
        else:
            # fallback: mean_time_msë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë”ë¯¸ ë°ì´í„° ìƒì„±
            mean = r.get('mean_time_ms', 0)
            std = r.get('std_time_ms', 0)
            dummy_times = np.random.normal(mean, std, NUM_RUNS).tolist()
            inference_times_list.append(dummy_times)
    
    # total_params ì²˜ë¦¬
    total_params_list = []
    for r in results:
        params = r.get('total_params', 0)
        if params == 0:
            # fallback: ëª¨ë¸ í¬ê¸°ë¡œë¶€í„° ì¶”ì •
            size_mb = r.get('model_size_mb', 0)
            # ëŒ€ëµì ì¸ ì¶”ì • (4 bytes per param)
            estimated_params = int(size_mb * 1024 * 1024 / 4)
            total_params_list.append(estimated_params)
        else:
            total_params_list.append(params)
    
    # METEOR ì ìˆ˜ê°€ ìœ íš¨í•œì§€ í™•ì¸ (í•˜ë‚˜ë¼ë„ ìˆìœ¼ë©´ í‘œì‹œ)
    has_meteor = any(r.get('meteor_score') is not None and r.get('meteor_score') > 0 for r in results)
    
    # ìƒ‰ìƒ ì„¤ì • (ìˆœì„œëŒ€ë¡œ: íŒŒë‘, ë¹¨ê°•, ì´ˆë¡, ì£¼í™©)
    colors = ['#3498db', '#e74c3c', '#2ecc71', '#f39c12'] 
    bar_colors = colors[:len(precisions)]
    
    # 2. ìº”ë²„ìŠ¤ ì„¤ì • (3í–‰ 2ì—´)
    fig, axes = plt.subplots(3, 2, figsize=(14, 15))
    fig.suptitle('Quantization ì„±ëŠ¥ ë¹„êµ ì¢…í•©', fontsize=16, fontweight='bold')
    
    # -------------------------------------------------------
    # [1] ì¶”ë¡  ì‹œê°„ (Bar Chart)
    # -------------------------------------------------------
    ax = axes[0, 0]
    bars = ax.bar(precisions, mean_times, yerr=std_times, capsize=5, alpha=0.8, color=bar_colors)
    ax.set_ylabel('ì¶”ë¡  ì‹œê°„ (ms)', fontweight='bold')
    ax.set_title('ì¶”ë¡  ì‹œê°„ (ë‚®ì„ìˆ˜ë¡ ì¢‹ìŒ)', fontweight='bold')
    ax.grid(axis='y', alpha=0.3)
    # ê°’ í‘œì‹œ
    for bar, mean in zip(bars, mean_times):
        height = bar.get_height()
        ax.text(bar.get_x() + bar.get_width()/2., height + 1,
                f'{mean:.1f}', ha='center', va='bottom', fontsize=9)

    # -------------------------------------------------------
    # [2] ëª¨ë¸ í¬ê¸° (Bar Chart)
    # -------------------------------------------------------
    ax = axes[0, 1]
    bars = ax.bar(precisions, model_sizes, alpha=0.8, color=bar_colors)
    ax.set_ylabel('ëª¨ë¸ í¬ê¸° (MB)', fontweight='bold')
    ax.set_title('ëª¨ë¸ í¬ê¸° (ë‚®ì„ìˆ˜ë¡ ì¢‹ìŒ)', fontweight='bold')
    ax.grid(axis='y', alpha=0.3)
    for bar, size in zip(bars, model_sizes):
        height = bar.get_height()
        ax.text(bar.get_x() + bar.get_width()/2., height + 0.1,
                f'{size:.1f}', ha='center', va='bottom', fontsize=9)

    # -------------------------------------------------------
    # [3] ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ (Bar Chart)
    # -------------------------------------------------------
    ax = axes[1, 0]
    bars = ax.bar(precisions, memory_usages, alpha=0.8, color=bar_colors)
    ax.set_ylabel('ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ (MB)', fontweight='bold')
    ax.set_title('ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰', fontweight='bold')
    ax.grid(axis='y', alpha=0.3)
    for bar, mem in zip(bars, memory_usages):
        height = bar.get_height()
        ax.text(bar.get_x() + bar.get_width()/2., height + 0.05,
                f'{mem:.1f}', ha='center', va='bottom', fontsize=9)

    # -------------------------------------------------------
    # [4] METEOR ì ìˆ˜ (Bar Chart)
    # -------------------------------------------------------
    ax = axes[1, 1]
    if has_meteor:
        bars = ax.bar(precisions, meteor_scores, alpha=0.8, color=bar_colors)
        ax.set_ylabel('METEOR ì ìˆ˜', fontweight='bold')
        ax.set_title('ì •í™•ë„ (ë†’ì„ìˆ˜ë¡ ì¢‹ìŒ)', fontweight='bold')
        ax.set_ylim(0, 1.0) # ì ìˆ˜ëŠ” 0~1 ì‚¬ì´
        ax.grid(axis='y', alpha=0.3)
        for bar, score in zip(bars, meteor_scores):
            height = bar.get_height()
            ax.text(bar.get_x() + bar.get_width()/2., height + 0.01,
                    f'{score:.3f}', ha='center', va='bottom', fontsize=9)
    else:
        ax.text(0.5, 0.5, 'METEOR ì ìˆ˜ ì—†ìŒ', ha='center', va='center')

    # -------------------------------------------------------
    # [5] íŒŒë¼ë¯¸í„° ê°œìˆ˜ (Bar Chart)
    # -------------------------------------------------------
    ax = axes[2, 0]
    # ë°±ë§Œ(M) ë‹¨ìœ„ë¡œ ë³€í™˜
    params_m = [p / 1e6 if p > 0 else 0 for p in total_params_list]
    if any(p > 0 for p in params_m):
        bars = ax.bar(precisions, params_m, alpha=0.8, color=bar_colors)
        ax.set_ylabel('íŒŒë¼ë¯¸í„° ìˆ˜ (Million)', fontweight='bold')
        ax.set_title('íŒŒë¼ë¯¸í„° ê°œìˆ˜ (M)', fontweight='bold')
        ax.grid(axis='y', alpha=0.3)
        for bar, param in zip(bars, params_m):
            if param > 0:
                height = bar.get_height()
                ax.text(bar.get_x() + bar.get_width()/2., height + max(params_m) * 0.02,
                        f'{param:.2f}M', ha='center', va='bottom', fontsize=9)
    else:
        ax.text(0.5, 0.5, 'íŒŒë¼ë¯¸í„° ë°ì´í„° ì—†ìŒ', ha='center', va='center', transform=ax.transAxes)
        ax.set_title('íŒŒë¼ë¯¸í„° ê°œìˆ˜ (ë°ì´í„° ì—†ìŒ)', fontweight='bold')

    # -------------------------------------------------------
    # [6] ì¶”ë¡  ì‹œê°„ ë¶„í¬ (Box Plot)
    # -------------------------------------------------------
    ax = axes[2, 1]
    # ë¹ˆ ë¦¬ìŠ¤íŠ¸ í•„í„°ë§
    valid_times = []
    valid_labels = []
    valid_colors = []
    for i, (times, label, color) in enumerate(zip(inference_times_list, precisions, bar_colors)):
        if times and len(times) > 0:
            valid_times.append(times)
            valid_labels.append(label)
            valid_colors.append(color)
    
    if valid_times:
        bp = ax.boxplot(valid_times, labels=valid_labels, patch_artist=True)
        # ë°•ìŠ¤ ìƒ‰ìƒ ì…íˆê¸°
        for patch, color in zip(bp['boxes'], valid_colors):
            patch.set_facecolor(color)
            patch.set_alpha(0.8)
        ax.set_ylabel('ì‹œê°„ (ms)', fontweight='bold')
        ax.set_title('ì¶”ë¡  ì‹œê°„ ë¶„í¬ (ì•ˆì •ì„±)', fontweight='bold')
        ax.grid(axis='y', alpha=0.3)
    else:
        ax.text(0.5, 0.5, 'ì¶”ë¡  ì‹œê°„ ë°ì´í„° ì—†ìŒ', ha='center', va='center', transform=ax.transAxes)
        ax.set_title('ì¶”ë¡  ì‹œê°„ ë¶„í¬ (ë°ì´í„° ì—†ìŒ)', fontweight='bold')

    # ë ˆì´ì•„ì›ƒ ì¡°ì • ë° ì €ì¥
    plt.tight_layout()
    save_path = os.path.join(OUTPUT_DIR, 'quantization_benchmark_result.png')
    plt.savefig(save_path, dpi=300, bbox_inches='tight')
    print(f"âœ… ê·¸ë˜í”„ ì €ì¥ ì™„ë£Œ: {save_path}")
    plt.close()

# ============================================================================
# Main
# ============================================================================
def main():
    os.makedirs(OUTPUT_DIR, exist_ok=True)
    base_model, wm, rwm = load_base_model()
    img_tensor, ref_caption = load_data()
    
    results = []
    
    # FP32
    results.append(run_benchmark(base_model, img_tensor, wm, rwm, "FP32", ref_caption))
    
    # FP16 (GPU only)
    if device.type in ['cuda', 'mps']:
        m_fp16 = deepcopy(base_model).half()
        results.append(run_benchmark(m_fp16, img_tensor, wm, rwm, "FP16", ref_caption))
        del m_fp16
    
    # Int8 Static
    m_static = convert_to_int8(base_model, wm, use_qat=False)
    results.append(run_benchmark(m_static, img_tensor, wm, rwm, "Int8-Static", ref_caption))
    del m_static
    
    # Int8 QAT
    if USE_QAT:
        m_qat = convert_to_int8(base_model, wm, use_qat=True, qat_epochs=QAT_EPOCHS)
        results.append(run_benchmark(m_qat, img_tensor, wm, rwm, "Int8-QAT", ref_caption))
        del m_qat
        
    print("\nâœ… ëª¨ë“  ë²¤ì¹˜ë§ˆí¬ ì™„ë£Œ.")
    plot_benchmark(results)
if __name__ == "__main__":
    main()