import torch
import torch.nn as nn
import torch.optim as optim
from torch.optim.lr_scheduler import ReduceLROnPlateau
from torch.utils.data import DataLoader, Dataset
from torchvision import transforms
from PIL import Image
import os
import re
import numpy as np
from collections import Counter, defaultdict
from src.muti_modal_model.model import MobileNetCaptioningModel
import warnings
from tqdm import tqdm

# ìœ í‹¸ë¦¬í‹° import
from src.utils import (
    setup_device,
    get_image_transform,
    CaptionDataset as CaptionDatasetUtil,  # ìœ í‹¸ ë²„ì „ (í•„ìš”ì‹œ ì‚¬ìš©)
    calculate_meteor,
    METEOR_AVAILABLE,
)
from src.utils.glove_utils import (
    load_glove_embeddings_with_fallback,
    create_embedding_matrix
)
from src.utils.finetune_utils import (
    load_model_checkpoint,
    save_checkpoint as save_checkpoint_util,
)

warnings.filterwarnings("ignore")

# --- [0] ì„¤ì • (Configuration) ---
# ë””ë°”ì´ìŠ¤ ì„ íƒ: CUDA > MPS > CPU
if torch.cuda.is_available():
    device = torch.device("cuda")
    print("CUDA ì‚¬ìš© ê°€ëŠ¥ - GPU ê°€ì† í™œì„±í™”")
elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():
    device = torch.device("mps")
    print("MPS ì‚¬ìš© ê°€ëŠ¥ - Apple Silicon GPU ê°€ì† í™œì„±í™”")
else:
    device = torch.device("cpu")
    print("CPU ëª¨ë“œë¡œ ì‹¤í–‰")

LEARNING_RATE = 4e-4  # í•™ìŠµë¥  (ë„ˆë¬´ í¬ë©´ ë°œì‚°í•¨)
BATCH_SIZE = 64 if device.type != "cpu" else 16  # GPU/MPS ì‚¬ìš© ì‹œ ë” í° ë°°ì¹˜
EPOCHS = 400          # ì „ì²´ ë°˜ë³µ íšŸìˆ˜
MAX_CAPTION_LEN = 50  # ìµœëŒ€ ìº¡ì…˜ ê¸¸ì´
MIN_WORD_FREQ = 2     # ë‹¨ì–´ì¥ì— í¬í•¨ë  ìµœì†Œ ë¹ˆë„
ENCODER_FINE_TUNING = True
USE_MIXED_PRECISION = device.type in ["cuda", "mps"]  # Mixed precision (FP16) ì‚¬ìš©
NUM_WORKERS = 0 if device.type == "mps" else 4  # MPSì—ì„œëŠ” 0ì´ ì•ˆì „, CUDAì—ì„œëŠ” ë©€í‹°í”„ë¡œì„¸ì‹±
PIN_MEMORY = device.type != "cpu"  # GPU ì‚¬ìš© ì‹œ ë©”ëª¨ë¦¬ ê³ ì •

# === ê²½ë¡œ ì„¤ì • (Colab í™˜ê²½ ìë™ ê°ì§€) ===
# Colab í™˜ê²½ ê°ì§€
IS_COLAB = 'COLAB_GPU' in os.environ or 'COLAB_TPU' in os.environ

if IS_COLAB:
    # Colab Google Drive ê²½ë¡œ
    BASE_DIR = "/content/drive/MyDrive"
    IMAGES_DIR = os.path.join(BASE_DIR, "assets/images")
    CAPTIONS_FILE = os.path.join(BASE_DIR, "assets/captions.txt")
    MODEL_SAVE_DIR = os.path.join(BASE_DIR, "models")
    ASSETS_DIR = os.path.join(BASE_DIR, "assets")
    
    print("ğŸ”µ Colab í™˜ê²½ ê°ì§€ë¨")
    print("   ì´ë¯¸ì§€ ê²½ë¡œ: {}".format(IMAGES_DIR))
    print("   ìº¡ì…˜ íŒŒì¼: {}".format(CAPTIONS_FILE))
    print("   ëª¨ë¸ ì €ì¥ ê²½ë¡œ: {}".format(MODEL_SAVE_DIR))
    
    # ëª¨ë¸ ì €ì¥ ë””ë ‰í† ë¦¬ ìƒì„±
    os.makedirs(MODEL_SAVE_DIR, exist_ok=True)
else:
    # ë¡œì»¬ í™˜ê²½
    IMAGES_DIR = "assets/images"
    CAPTIONS_FILE = "assets/captions.txt"
    MODEL_SAVE_DIR = "models"  # models í´ë”ì— ì €ì¥
    ASSETS_DIR = "assets"
    print("ğŸŸ¢ ë¡œì»¬ í™˜ê²½")
    
    # ëª¨ë¸ ì €ì¥ ë””ë ‰í† ë¦¬ ìƒì„±
    os.makedirs(MODEL_SAVE_DIR, exist_ok=True)

# ì‚¬ì „ í•™ìŠµëœ ì„ë² ë”© ì„¤ì •
EMBED_DIM = 300  # GloVe 6B.300d ì‚¬ìš© (ë˜ëŠ” ìµœì í™” í›„ 100ìœ¼ë¡œ ë³€ê²½ ê°€ëŠ¥)
USE_PRETRAINED_EMBEDDING = True  # ì‚¬ì „ í•™ìŠµëœ ì„ë² ë”© ì‚¬ìš© ì—¬ë¶€

# GloVe íŒŒì¼ ê²½ë¡œ (assets í•˜ìœ„ì— ìœ„ì¹˜)
# íŒŒì¼ì„ assets/glove.6B.300d.txt ìœ„ì¹˜ì— ì €ì¥
GLOVE_PATH = os.path.join(ASSETS_DIR, "glove.6B.300d.txt")
GLOVE_OPTIMIZED_PATH = os.path.join(ASSETS_DIR, "glove_optimized.pkl")

# --- [1] ì´ë¯¸ì§€ ì „ì²˜ë¦¬ ---
transform = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406], 
                         std=[0.229, 0.224, 0.225])  # ImageNet ì •ê·œí™”
])

# --- [2] ìº¡ì…˜ ì „ì²˜ë¦¬ í•¨ìˆ˜ ---

def build_vocab(captions, min_freq=MIN_WORD_FREQ):
    """ìº¡ì…˜ ë¦¬ìŠ¤íŠ¸ë¡œë¶€í„° ë‹¨ì–´ì¥ ìƒì„±"""
    # ëª¨ë“  ë‹¨ì–´ ìˆ˜ì§‘
    word_counts = Counter()
    for caption in captions:
        # ì†Œë¬¸ì ë³€í™˜ ë° ë‹¨ì–´ ë¶„ë¦¬
        words = re.findall(r'\w+', caption.lower())
        word_counts.update(words)
    
    # ìµœì†Œ ë¹ˆë„ ì´ìƒì¸ ë‹¨ì–´ë§Œ ì„ íƒ
    vocab = {'<pad>': 0, '<start>': 1, '<end>': 2, '<unk>': 3}
    idx = 4
    for word, count in word_counts.items():
        if count >= min_freq:
            vocab[word] = idx
            idx += 1
    
    return vocab, {v: k for k, v in vocab.items()}

def encode_caption(caption, word_map, max_len=MAX_CAPTION_LEN):
    """ìº¡ì…˜ í…ìŠ¤íŠ¸ë¥¼ ì •ìˆ˜ ì‹œí€€ìŠ¤ë¡œ ë³€í™˜"""
    words = re.findall(r'\w+', caption.lower())
    encoded = [word_map.get('<start>', 1)]
    
    for word in words:
        encoded.append(word_map.get(word, word_map.get('<unk>', 3)))
    
    encoded.append(word_map.get('<end>', 2))
    
    # íŒ¨ë”© ë˜ëŠ” ìë¥´ê¸°
    if len(encoded) > max_len:
        encoded = encoded[:max_len]
        encoded[-1] = word_map.get('<end>', 2)  # ë§ˆì§€ë§‰ì„ <end>ë¡œ
    else:
        encoded.extend([word_map.get('<pad>', 0)] * (max_len - len(encoded)))
    
    return torch.tensor(encoded, dtype=torch.long)

# --- [3] ì‹¤ì œ ë°ì´í„°ì…‹ í´ë˜ìŠ¤ ---
class CaptionDataset(Dataset):
    def __init__(self, images_dir, captions_file, transform=None, word_map=None, max_len=MAX_CAPTION_LEN):
        self.images_dir = images_dir
        self.transform = transform
        self.word_map = word_map
        self.max_len = max_len
        
        # ì´ë¯¸ì§€ íŒŒì¼ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°
        available_images = set([f for f in os.listdir(images_dir) 
                               if f.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp'))])
        
        # ìº¡ì…˜ íŒŒì¼ ì½ê¸° (CSV í˜•ì‹ ì§€ì›)
        # ì´ë¯¸ì§€ íŒŒì¼ëª… -> ìº¡ì…˜ ë¦¬ìŠ¤íŠ¸ ë§¤í•‘
        image_to_captions = defaultdict(list)
        
        if os.path.exists(captions_file):
            with open(captions_file, 'r', encoding='utf-8') as f:
                lines = f.readlines()
                # ì²« ë²ˆì§¸ ì¤„ì´ í—¤ë”ì¸ì§€ í™•ì¸
                first_line = lines[0].strip() if lines else ""
                start_idx = 1 if first_line.lower().startswith('image') or first_line.lower().startswith('filename') else 0
                
                for line in lines[start_idx:]:
                    line = line.strip()
                    if not line:
                        continue
                    
                    # CSV í˜•ì‹ (ì‰¼í‘œë¡œ êµ¬ë¶„)
                    if ',' in line:
                        parts = line.split(',', 1)  # ì²« ë²ˆì§¸ ì‰¼í‘œë§Œìœ¼ë¡œ ë¶„ë¦¬
                        if len(parts) == 2:
                            img_name = parts[0].strip()
                            caption = parts[1].strip()
                            if img_name and caption and img_name in available_images:
                                image_to_captions[img_name].append(caption)
                    # íƒ­ìœ¼ë¡œ êµ¬ë¶„ëœ ê²½ìš°
                    elif '\t' in line:
                        parts = line.split('\t', 1)
                        if len(parts) == 2:
                            img_name = parts[0].strip()
                            caption = parts[1].strip()
                            if img_name and caption and img_name in available_images:
                                image_to_captions[img_name].append(caption)
                    # ë‹¨ìˆœ ìº¡ì…˜ë§Œ ìˆëŠ” ê²½ìš° (ì´ë¯¸ì§€ íŒŒì¼ëª… ìˆœì„œëŒ€ë¡œ ë§¤ì¹­)
                    else:
                        # ì´ ê²½ìš°ëŠ” ë‚˜ì¤‘ì— ì²˜ë¦¬
                        pass
        
        # ì´ë¯¸ì§€-ìº¡ì…˜ ìŒ ìƒì„± (í•˜ë‚˜ì˜ ì´ë¯¸ì§€ì— ì—¬ëŸ¬ ìº¡ì…˜ì´ ìˆìœ¼ë©´ ê°ê° ë³„ë„ ìƒ˜í”Œë¡œ)
        self.image_caption_pairs = []
        for img_name, captions in image_to_captions.items():
            if captions:
                # ê° ìº¡ì…˜ì„ ë³„ë„ ìƒ˜í”Œë¡œ ì¶”ê°€
                for caption in captions:
                    self.image_caption_pairs.append((img_name, caption))
        # ë‹¨ìˆœ ìº¡ì…˜ë§Œ ìˆëŠ” ê²½ìš° ì²˜ë¦¬ (ì´ë¯¸ì§€ íŒŒì¼ ìˆœì„œëŒ€ë¡œ ë§¤ì¹­)
        if not self.image_caption_pairs:
            image_files = sorted(available_images)
            if os.path.exists(captions_file):
                with open(captions_file, 'r', encoding='utf-8') as f:
                    lines = f.readlines()
                    captions_only = [line.strip() for line in lines if line.strip() and not line.strip().startswith('image')]
                    for img_file, caption in zip(image_files, captions_only):
                        if caption:
                            self.image_caption_pairs.append((img_file, caption))
        
        print("ë¡œë“œëœ ë°ì´í„°: {}ê°œì˜ ì´ë¯¸ì§€-ìº¡ì…˜ ìŒ".format(len(self.image_caption_pairs)))
        print("ê³ ìœ  ì´ë¯¸ì§€ ìˆ˜: {}".format(len(set([pair[0] for pair in self.image_caption_pairs]))))
        
    def __getitem__(self, idx):
        # ì´ë¯¸ì§€ ë¡œë“œ
        img_name, caption_text = self.image_caption_pairs[idx]
        img_path = os.path.join(self.images_dir, img_name)
        
        try:
            image = Image.open(img_path).convert('RGB')
            if self.transform:
                image = self.transform(image)
        except Exception as e:
            print("ì´ë¯¸ì§€ ë¡œë“œ ì‹¤íŒ¨: {}, ì˜¤ë¥˜: {}".format(img_path, e))
            # ì˜¤ë¥˜ ì‹œ ê²€ì€ ì´ë¯¸ì§€ ë°˜í™˜
            image = torch.zeros(3, 224, 224)
        # ìº¡ì…˜ ì¸ì½”ë”©
        if self.word_map:
            caption = encode_caption(caption_text, self.word_map, self.max_len)
        else:
            # ë‹¨ì–´ì¥ì´ ì—†ìœ¼ë©´ ë”ë¯¸ ìº¡ì…˜
            caption = torch.zeros(self.max_len, dtype=torch.long)
        
        return image, caption
    
    def __len__(self):
        return len(self.image_caption_pairs)

# --- [2] í•™ìŠµ í•¨ìˆ˜ ì •ì˜ ---
def train_epoch(model, dataloader, criterion, optimizer, epoch, vocab_size, scaler=None, use_mixed_precision=False):
    model.train() # í•™ìŠµ ëª¨ë“œ ì„¤ì •
    total_loss = 0
    for i, (imgs, caps) in enumerate(tqdm(dataloader, desc="Training Epoch {}".format(epoch+1))):
        imgs = imgs.to(device, non_blocking=True)
        caps = caps.to(device, non_blocking=True)
        
        # 1. ê¸°ìš¸ê¸° ì´ˆê¸°í™”
        optimizer.zero_grad()
        
        # 2. Mixed Precision Training (FP16)
        if use_mixed_precision:
            if device.type == "cuda" and scaler is not None:
                with torch.cuda.amp.autocast():
                    # ëª¨ë¸ ì˜ˆì¸¡ (Forward)
                    outputs, alphas = model(imgs, caps)
                    
                    # ì •ë‹µê³¼ ë¹„êµë¥¼ ìœ„í•œ ì°¨ì› ì¡°ì ˆ
                    targets = caps[:, 1:] 
                    outputs = outputs[:, :targets.shape[1], :]
                    
                    # ì†ì‹¤ ê³„ì‚°
                    loss = criterion(outputs.reshape(-1, vocab_size), targets.reshape(-1))
                
                # ì—­ì „íŒŒ (Scaled)
                scaler.scale(loss).backward()
                scaler.step(optimizer)
                scaler.update()
            elif device.type == "mps":
                # MPSëŠ” autocastë§Œ ì§€ì›
                with torch.amp.autocast(device_type="mps", dtype=torch.float16):
                    # ëª¨ë¸ ì˜ˆì¸¡ (Forward)
                    outputs, alphas = model(imgs, caps)
                    
                    # ì •ë‹µê³¼ ë¹„êµë¥¼ ìœ„í•œ ì°¨ì› ì¡°ì ˆ
                    targets = caps[:, 1:] 
                    outputs = outputs[:, :targets.shape[1], :]
                    
                    # ì†ì‹¤ ê³„ì‚°
                    loss = criterion(outputs.reshape(-1, vocab_size), targets.reshape(-1))
                
                # ì—­ì „íŒŒ
                loss.backward()
                optimizer.step()
            else:
                # ì¼ë°˜ í•™ìŠµìœ¼ë¡œ í´ë°±
                outputs, alphas = model(imgs, caps)
                targets = caps[:, 1:] 
                outputs = outputs[:, :targets.shape[1], :]
                loss = criterion(outputs.reshape(-1, vocab_size), targets.reshape(-1))
                loss.backward()
                optimizer.step()
        else:
            # ì¼ë°˜ í•™ìŠµ (FP32)
            # ëª¨ë¸ ì˜ˆì¸¡ (Forward)
            outputs, alphas = model(imgs, caps)
            
            # ì •ë‹µê³¼ ë¹„êµë¥¼ ìœ„í•œ ì°¨ì› ì¡°ì ˆ
            targets = caps[:, 1:] 
            outputs = outputs[:, :targets.shape[1], :]
            
            # ì†ì‹¤ ê³„ì‚°
            loss = criterion(outputs.reshape(-1, vocab_size), targets.reshape(-1))
            
            # ì—­ì „íŒŒ
            loss.backward()
            
            # ê°€ì¤‘ì¹˜ ì—…ë°ì´íŠ¸
            optimizer.step()
        
        total_loss += loss.item()
        
        # if i % 10 == 0:
        #     print("Epoch [{}/{}], Step [{}/{}], Loss: {}".format(epoch+1, EPOCHS, i, len(dataloader), loss.item():.4f))
    return total_loss / len(dataloader)

# --- [2.5] ê²€ì¦ í•¨ìˆ˜ ì •ì˜ ---
def validate_epoch(model, val_dataloader, criterion, epoch, vocab_size, word_map=None, rev_word_map=None):
    """ê²€ì¦ ë°ì´í„°ì…‹ì—ì„œ ëª¨ë¸ í‰ê°€ (Loss + METEOR ì ìˆ˜)"""
    model.eval()
    total_val_loss = 0
    meteor_scores = []
    
    with torch.no_grad():
        for i, (imgs, caps) in enumerate(val_dataloader):
            imgs = imgs.to(device, non_blocking=True)
            caps = caps.to(device, non_blocking=True)
            
            # ëª¨ë¸ ì˜ˆì¸¡ (Forward)
            outputs, alphas = model(imgs, caps)
            
            # ì •ë‹µê³¼ ë¹„êµë¥¼ ìœ„í•œ ì°¨ì› ì¡°ì ˆ
            targets = caps[:, 1:] 
            outputs = outputs[:, :targets.shape[1], :]
            
            # ì†ì‹¤ ê³„ì‚°
            loss = criterion(outputs.reshape(-1, vocab_size), targets.reshape(-1))
            total_val_loss += loss.item()
            
            # METEOR ì ìˆ˜ ê³„ì‚° (word_mapì´ ì œê³µëœ ê²½ìš°)
            if word_map is not None and rev_word_map is not None:
                try:
                    # ë°°ì¹˜ì˜ ê° ìƒ˜í”Œì— ëŒ€í•´ ìº¡ì…˜ ìƒì„±
                    for j in range(imgs.shape[0]):
                        img_single = imgs[j:j+1]
                        cap_single = caps[j:j+1]
                        
                        # ìº¡ì…˜ ìƒì„±
                        generated_words = model.generate(img_single, word_map, rev_word_map, max_len=MAX_CAPTION_LEN)
                        generated_caption = ' '.join([w for w in generated_words if w not in ['<start>', '<end>', '<pad>', '<unk>']])
                        
                        # ì°¸ì¡° ìº¡ì…˜
                        reference_cap = ' '.join([rev_word_map.get(int(idx), '<unk>') for idx in cap_single[0] if int(idx) > 0])
                        reference_cap = reference_cap.replace('<start> ', '').replace(' <end>', '')
                        
                        # METEOR ê³„ì‚° (ìœ í‹¸ í•¨ìˆ˜ ì‚¬ìš©)
                        meteor = calculate_meteor(
                            generated_caption.lower().split(),
                            reference_cap
                        )
                        if meteor is None:
                            meteor = 0.0
                        
                        meteor_scores.append(meteor)
                except Exception as e:
                    # METEOR ê³„ì‚° ì‹¤íŒ¨ ì‹œ 0.0 ì¶”ê°€
                    meteor_scores.append(0.0)
            
            if i % 10 == 0:
                print("  Validation Step [{}/{}], Loss: {:.4f}".format(i, len(val_dataloader), loss.item()))
    
    avg_val_loss = total_val_loss / len(val_dataloader)
    avg_meteor = sum(meteor_scores) / len(meteor_scores) if meteor_scores else 0.0
    
    model.train()  # ë‹¤ì‹œ í•™ìŠµ ëª¨ë“œë¡œ
    
    return avg_val_loss, avg_meteor


def evaluate_multiple_samples(model, dataset, word_map, rev_word_map, num_samples=5, start_idx=0):
    """val ë°ì´í„°ì…‹ ì „ì²´ì˜ í‰ê·  METEOR ì ìˆ˜ ê³„ì‚°"""
    model.eval()
    
    meteor_scores = []
    
    # ì „ì²´ val ë°ì´í„°ì…‹ ì‚¬ìš© (num_samplesëŠ” ë¬´ì‹œ)
    total_samples = len(dataset)
    
    print("\n{'='*70}")
    print("ğŸ” ê²€ì¦ ë°ì´í„°ì…‹ í‰ê°€: {}ê°œ ìƒ˜í”Œì˜ í‰ê·  METEOR ê³„ì‚°".format(total_samples))
    print("{'='*70}")
    
    with torch.no_grad():
        for i in range(total_samples):
            try:
                img_name, original_caption = dataset.image_caption_pairs[i]
                image, _ = dataset[i]
                
                # ë°°ì¹˜ ì°¨ì› ì¶”ê°€ [1, 3, 224, 224]
                image = image.unsqueeze(0).to(device)
                
                # ìº¡ì…˜ ìƒì„±
                generated_words = model.generate(image, word_map, rev_word_map, max_len=MAX_CAPTION_LEN)
                
                # í† í° ì œê±°í•˜ê³  ë¬¸ì¥ìœ¼ë¡œ ë³€í™˜
                generated_caption = ' '.join([w for w in generated_words if w not in ['<start>', '<end>', '<pad>', '<unk>']])
                
                # METEOR ì ìˆ˜ ê³„ì‚° (ìœ í‹¸ í•¨ìˆ˜ ì‚¬ìš©)
                meteor = calculate_meteor(
                    generated_caption.lower().split(),
                    original_caption
                )
                if meteor is None:
                    meteor = 0.0
                
                meteor_scores.append(meteor)
                
                # ì§„í–‰ë„ í‘œì‹œ (100ê°œë§ˆë‹¤)
                if (i + 1) % 100 == 0:
                    current_avg = sum(meteor_scores) / len(meteor_scores)
                    print("  ì§„í–‰: {}/{}, í˜„ì¬ í‰ê·  METEOR: {}".format(i+1, total_samples, current_avg:.4f))
                    
            except Exception as e:
                print("  âš ï¸ ìƒ˜í”Œ {} ìƒì„± ì‹¤íŒ¨: {}".format(i+1, e))
                meteor_scores.append(0.0)
    
    # ì „ì²´ í‰ê·  METEOR ì ìˆ˜
    avg_meteor = sum(meteor_scores) / len(meteor_scores) if meteor_scores else 0.0
    
    print("\n{'='*70}")
    print("ğŸ“ˆ ê²€ì¦ ë°ì´í„°ì…‹ METEOR í†µê³„:")
    print("  â€¢ í‰ê°€ ìƒ˜í”Œ: {}ê°œ".format(total_samples))
    print("  â€¢ í‰ê·  METEOR ì ìˆ˜: {}".format(avg_meteor:.4f))
    if meteor_scores:
        print("  â€¢ ìµœê³  METEOR ì ìˆ˜: {}".format(max(meteor_scores):.4f))
        print("  â€¢ ìµœì € METEOR ì ìˆ˜: {}".format(min(meteor_scores):.4f))
        print("  â€¢ METEOR ì ìˆ˜ ë¶„í¬:")
        print("    - 0.5 ì´ìƒ (ìš°ìˆ˜): {}ê°œ".format(sum([1 for s in meteor_scores if s >= 0.5])))
        print("    - 0.3-0.5 (ì–‘í˜¸): {}ê°œ".format(sum([1 for s in meteor_scores if 0.3 <= s < 0.5])))
        print("    - 0.3 ë¯¸ë§Œ (ê°œì„  í•„ìš”): {}ê°œ".format(sum([1 for s in meteor_scores if s < 0.3])))
    print("{'='*70}\n")
    
    model.train()  # ë‹¤ì‹œ í•™ìŠµ ëª¨ë“œë¡œ
    
    return {
        'avg_meteor': avg_meteor,
        'max_meteor': max(meteor_scores) if meteor_scores else 0.0,
        'min_meteor': min(meteor_scores) if meteor_scores else 0.0,
        'meteor_scores': meteor_scores
    }

# --- [4] ë©”ì¸ ì‹¤í–‰ ì½”ë“œ ---
def main():
    # 1. ìº¡ì…˜ íŒŒì¼ ì½ì–´ì„œ ë‹¨ì–´ì¥ ìƒì„±
    print("ë‹¨ì–´ì¥ ìƒì„± ì¤‘...")
    captions_list = []
    if os.path.exists(CAPTIONS_FILE):
        with open(CAPTIONS_FILE, 'r', encoding='utf-8') as f:
            lines = f.readlines()
            # ì²« ë²ˆì§¸ ì¤„ì´ í—¤ë”ì¸ì§€ í™•ì¸
            first_line = lines[0].strip() if lines else ""
            start_idx = 1 if first_line.lower().startswith('image') or first_line.lower().startswith('filename') else 0
            
            for line in lines[start_idx:]:
                line = line.strip()
                if line:
                    # CSV í˜•ì‹ (ì‰¼í‘œë¡œ êµ¬ë¶„)
                    if ',' in line:
                        parts = line.split(',', 1)
                        if len(parts) == 2:
                            captions_list.append(parts[1].strip())
                    # íƒ­ìœ¼ë¡œ êµ¬ë¶„ëœ ê²½ìš° ìº¡ì…˜ ë¶€ë¶„ë§Œ ì¶”ì¶œ
                    elif '\t' in line:
                        parts = line.split('\t', 1)
                        captions_list.append(parts[1] if len(parts) > 1 else parts[0])
                    else:
                        captions_list.append(line)
    
    if not captions_list:
        print("ê²½ê³ : ìº¡ì…˜ íŒŒì¼ì´ ë¹„ì–´ìˆê±°ë‚˜ ì—†ìŠµë‹ˆë‹¤. ë”ë¯¸ ë°ì´í„°ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")
        captions_list = ["a cat sitting on a mat"] * 100  # ë”ë¯¸ ë°ì´í„°
    
    word_map, rev_word_map = build_vocab(captions_list, min_freq=MIN_WORD_FREQ)
    vocab_size = len(word_map)
    print("ë‹¨ì–´ì¥ í¬ê¸°: {}".format(vocab_size))
    print("ì£¼ìš” ë‹¨ì–´ ì˜ˆì‹œ: {}".format(list(word_map.items())[:10]))
    
    # ì‚¬ì „ í•™ìŠµëœ ì„ë² ë”© ë¡œë“œ (ìœ í‹¸ í•¨ìˆ˜ ì‚¬ìš©)
    use_pretrained = USE_PRETRAINED_EMBEDDING
    glove_embeddings = None
    actual_embed_dim = EMBED_DIM
    
    if use_pretrained:
        glove_embeddings, actual_embed_dim = load_glove_embeddings_with_fallback(
            GLOVE_PATH, GLOVE_OPTIMIZED_PATH, EMBED_DIM
        )
        if glove_embeddings is None:
            print("âš ï¸ ì‚¬ì „ í•™ìŠµëœ ì„ë² ë”©ì„ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ëœë¤ ì´ˆê¸°í™”ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")
            use_pretrained = False
    
    # ì„ë² ë”© í–‰ë ¬ ìƒì„±
    embedding_matrix = None
    if use_pretrained and glove_embeddings:
        embedding_matrix = create_embedding_matrix(word_map, glove_embeddings, embed_dim=actual_embed_dim)
    else:
        # ëœë¤ ì´ˆê¸°í™” ì‚¬ìš© ì‹œì—ë„ embed_dimì€ ì„¤ì •ê°’ ì‚¬ìš©
        pass
    
    # 2. ë°ì´í„°ì…‹ ë° ë°ì´í„° ë¡œë” ì¤€ë¹„
    print("ë°ì´í„°ì…‹ ë¡œë“œ ì¤‘...")
    dataset = CaptionDataset(
        images_dir=IMAGES_DIR,
        captions_file=CAPTIONS_FILE,
        transform=transform,
        word_map=word_map,
        max_len=MAX_CAPTION_LEN
    )
    
    if len(dataset) == 0:
        raise ValueError("ë°ì´í„°ì…‹ì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤. {} í´ë”ì— ì´ë¯¸ì§€ê°€ ìˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”.".format(IMAGES_DIR))
    
    # ê²€ì¦ ì…‹ ë¶„ë¦¬ (80% í•™ìŠµ, 20% ê²€ì¦)
    val_split_ratio = 0.1
    val_size = max(1, int(len(dataset) * val_split_ratio))
    train_size = len(dataset) - val_size
    
    # ì‹œë“œ ê³ ì •ìœ¼ë¡œ ì¬í˜„ì„± ë³´ì¥
    torch.manual_seed(42)
    train_dataset, val_dataset = torch.utils.data.random_split(
        dataset, 
        [train_size, val_size]
    )
    
    print("âœ… ë°ì´í„°ì…‹ ë¶„í•  ì™„ë£Œ:")
    print("   â€¢ í•™ìŠµ ì…‹: {}ê°œ ìƒ˜í”Œ".format(len(train_dataset)))
    print("   â€¢ ê²€ì¦ ì…‹: {}ê°œ ìƒ˜í”Œ".format(len(val_dataset)))
    
    # ìµœì í™”ëœ DataLoader ì„¤ì •
    train_dataloader = DataLoader(
        train_dataset, 
        batch_size=BATCH_SIZE, 
        shuffle=True, 
        num_workers=NUM_WORKERS,
        pin_memory=PIN_MEMORY,
        persistent_workers=True if NUM_WORKERS > 0 else False,
        prefetch_factor=2 if NUM_WORKERS > 0 else None
    )
    
    # ê²€ì¦ ë°ì´í„° ë¡œë” (ì…”í”Œ ë¶ˆí•„ìš”)
    val_dataloader = DataLoader(
        val_dataset, 
        batch_size=BATCH_SIZE, 
        shuffle=False, 
        num_workers=NUM_WORKERS,
        pin_memory=PIN_MEMORY,
        persistent_workers=True if NUM_WORKERS > 0 else False,
        prefetch_factor=2 if NUM_WORKERS > 0 else None
    )
    
    # 3. ëª¨ë¸ ì¤€ë¹„ (MobileNet + Decoder)
    print("ëª¨ë¸ ì´ˆê¸°í™” ì¤‘...")
    model = MobileNetCaptioningModel(vocab_size=vocab_size, embed_dim=EMBED_DIM).to(device)
    
    # ì‚¬ì „ í•™ìŠµëœ ì„ë² ë”© ê°€ì¤‘ì¹˜ ì„¤ì •
    if use_pretrained and embedding_matrix is not None:
        print("ì‚¬ì „ í•™ìŠµëœ ì„ë² ë”© ê°€ì¤‘ì¹˜ ì„¤ì • ì¤‘...")
        model.decoder.embedding.weight.data.copy_(torch.from_numpy(embedding_matrix))
        # ì„ë² ë”©ì„ í•™ìŠµ ê°€ëŠ¥í•˜ê²Œ í• ì§€ ê³ ì •í• ì§€ ì„ íƒ (True: í•™ìŠµ, False: ê³ ì •)
        model.decoder.embedding.weight.requires_grad = True
        print("âœ… ì‚¬ì „ í•™ìŠµëœ ì„ë² ë”© ê°€ì¤‘ì¹˜ ì„¤ì • ì™„ë£Œ")
    
    # ì²´í¬í¬ì¸íŠ¸ì—ì„œ ëª¨ë¸ ë¡œë“œ (ìˆëŠ” ê²½ìš°)
    checkpoint_path = os.path.join(MODEL_SAVE_DIR, "lightweight_captioning_model.pth")
    start_epoch = 0
    if os.path.exists(checkpoint_path):
        print("ğŸ“‚ ì²´í¬í¬ì¸íŠ¸ ë°œê²¬: {}".format(checkpoint_path))
        try:
            # Python/PyTorch ë²„ì „ í˜¸í™˜ì„±
            try:
                # Python 3.11+: weights_only íŒŒë¼ë¯¸í„° í•„ìš”
                checkpoint = torch.load(checkpoint_path, map_location=device, weights_only=False)
            except TypeError:
                # Python 3.6-3.10: weights_only íŒŒë¼ë¯¸í„° ë¯¸ì§€ì›
                checkpoint = torch.load(checkpoint_path, map_location=device)
            
            if isinstance(checkpoint, dict) and 'model_state_dict' in checkpoint:
                model.load_state_dict(checkpoint['model_state_dict'])
                start_epoch = checkpoint.get('epoch', 0)
                print("âœ… ì²´í¬í¬ì¸íŠ¸ì—ì„œ ëª¨ë¸ ë¡œë“œ ì™„ë£Œ (Epoch {}ë¶€í„° ì´ì–´ì„œ í•™ìŠµ)".format(start_epoch))
            else:
                # ë”•ì…”ë„ˆë¦¬ê°€ ì•„ë‹Œ ê²½ìš° (êµ¬ë²„ì „ ì²´í¬í¬ì¸íŠ¸)
                model.load_state_dict(checkpoint)
                print("âœ… ì²´í¬í¬ì¸íŠ¸ì—ì„œ ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
        except Exception as e:
            print("âš ï¸ ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ ì‹¤íŒ¨: {}".format(e))
            print("   ìƒˆë¡œ í•™ìŠµì„ ì‹œì‘í•©ë‹ˆë‹¤.")
    else:
        print("ğŸ“ ì²´í¬í¬ì¸íŠ¸ ì—†ìŒ - ìƒˆë¡œ í•™ìŠµ ì‹œì‘")
    
    model.to(device)
    
    # [í•µì‹¬] 4. ì¸ì½”ë” ì–¼ë¦¬ê¸° (Encoder Freezing)
    # MobileNet ë¶€ë¶„ì€ í•™ìŠµë˜ì§€ ì•Šë„ë¡ ì„¤ì • (ì´ë¯¸ì§€ë„· ì§€ì‹ ë³´ì¡´)
    for param in model.encoder.parameters():
        param.requires_grad = False
        
    # 5. ìµœì í™” ë„êµ¬ ì„¤ì •
    # filterë¥¼ ì¨ì„œ requires_grad=Trueì¸ íŒŒë¼ë¯¸í„°(ë””ì½”ë”)ë§Œ ì—…ë°ì´íŠ¸ ëª©ë¡ì— ë„£ìŒ
    optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=LEARNING_RATE)
    
    # ìŠ¤ì¼€ì¤„ëŸ¬: METEOR ì ìˆ˜ ê¸°ë°˜ìœ¼ë¡œ í•™ìŠµë¥  ë™ì  ì¡°ì •
    scheduler = ReduceLROnPlateau(
        optimizer, 
        mode='max',           # METEOR(max)ì´ ê¸°ì¤€ (METEORê°€ ë†’ì„ìˆ˜ë¡ ì¢‹ìŒ)
        factor=0.66,          # í•™ìŠµë¥ ì„ 0.66ë°° ê°ì†Œ
        patience=2,           # 2 ì—í¬í¬ ë™ì•ˆ ê°œì„  ì—†ìœ¼ë©´ í•™ìŠµë¥  ê°ì†Œ
        min_lr=1e-6           # ìµœì†Œ í•™ìŠµë¥ 
    )
    # 6. ì†ì‹¤ í•¨ìˆ˜ (Padding=0 ë¬´ì‹œ)
    criterion = nn.CrossEntropyLoss(ignore_index=0)
    
    # 7. Mixed Precision Scaler ì„¤ì •
    scaler = None
    use_mixed_precision = USE_MIXED_PRECISION  # ë¡œì»¬ ë³€ìˆ˜ë¡œ ë³µì‚¬

    if use_mixed_precision:
        if device.type == "cuda":
            scaler = torch.cuda.amp.GradScaler()
            print("Mixed Precision (FP16) í™œì„±í™” - CUDA")
        elif device.type == "mps":
            # MPSëŠ” autocastë§Œ ì§€ì›í•˜ê³  GradScalerëŠ” ì—†ìŒ
            print("Mixed Precision (FP16) í™œì„±í™” - MPS")
        else:
            use_mixed_precision = False
    
    # 8. í•™ìŠµ ë£¨í”„
    print("í•™ìŠµ ì‹œì‘ (Encoder Frozen)... ì´ {}ê°œ ìƒ˜í”Œ, {} ì—í¬í¬".format(len(train_dataset), EPOCHS))
    print("ë°°ì¹˜ í¬ê¸°: {}, ë””ë°”ì´ìŠ¤: {}, Mixed Precision: {}".format(BATCH_SIZE, device, use_mixed_precision))
    
    # ê²€ì¦ ì„¤ì •
    VAL_NUM_SAMPLES = max(5, len(val_dataset))  # ê²€ì¦ì— ì‚¬ìš©í•  ìƒ˜í”Œ ìˆ˜
    
    # í•™ìŠµ ì´ë ¥ ì¶”ì 
    train_losses = []
    val_losses = []
    
    # ì²´í¬í¬ì¸íŠ¸ì—ì„œ ì´ì–´ì„œ í•™ìŠµí•˜ëŠ” ê²½ìš°
    for epoch in range(start_epoch, EPOCHS):
        print("\n{'='*70}")
        print("Epoch {}/{} ì‹œì‘".format(epoch+1, EPOCHS))
        print("{'='*70}")
        
        # í•™ìŠµ ì—í¬í¬
        avg_train_loss = train_epoch(model, train_dataloader, criterion, optimizer, epoch, vocab_size, scaler, use_mixed_precision)
        train_losses.append(avg_train_loss)
        print("âœ… í•™ìŠµ ì™„ë£Œ. í‰ê·  Loss: {}".format(avg_train_loss:.4f))
        
        # ê²€ì¦ ì—í¬í¬ (Loss + METEOR ì ìˆ˜ ê³„ì‚°)
        print("\nğŸ” ê²€ì¦ ì‹œì‘...")
        avg_val_loss, avg_meteor = validate_epoch(
            model, val_dataloader, criterion, epoch, vocab_size, 
            word_map=word_map, rev_word_map=rev_word_map
        )
        val_losses.append(avg_val_loss)
        print("âœ… ê²€ì¦ ì™„ë£Œ. í‰ê·  Loss: {}".format(avg_val_loss:.4f))
        print("â­ í‰ê·  METEOR: {}".format(avg_meteor:.4f))
        
        # ìŠ¤ì¼€ì¤„ëŸ¬ ì—…ë°ì´íŠ¸ (METEOR ì ìˆ˜ ê¸°ë°˜)
        scheduler.step(avg_meteor)
        current_lr = optimizer.param_groups[0]['lr']
        print("ğŸ“Š ìŠ¤ì¼€ì¤„ëŸ¬ ì—…ë°ì´íŠ¸ - METEOR: {}, Learning Rate: {}".format(avg_meteor:.4f, current_lr:.2e))
        
        # [ì˜µì…˜] íŠ¹ì • Epoch ì´í›„ì— ì¸ì½”ë”ë„ ê°™ì´ í•™ìŠµì‹œí‚¤ê³  ì‹¶ë‹¤ë©´? (Fine-tuning)
        if ENCODER_FINE_TUNING and epoch == 5:
            print(">>> ì¸ì½”ë” ë¯¸ì„¸ ì¡°ì • ì‹œì‘ (Fine-tuning Start) <<<")
            # ì¸ì½”ë”ì˜ ë’·ë¶€ë¶„ ë ˆì´ì–´ë§Œ í’€ê±°ë‚˜ ì „ì²´ë¥¼ í’‚
            for param in model.encoder.parameters():
                param.requires_grad = True
            
            # ì˜µí‹°ë§ˆì´ì €ì— ì¸ì½”ë” íŒŒë¼ë¯¸í„°ë„ ì¶”ê°€ (í•™ìŠµë¥ ì€ ë” ë‚®ê²Œ ì¡ëŠ” ê²Œ ì¢‹ìŒ)
            optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE * 0.1)
            scheduler = ReduceLROnPlateau(
                optimizer, 
                mode='max',      # METEOR(max)ì´ ê¸°ì¤€
                factor=0.66, 
                patience=2,
            )

        # ì£¼ê¸°ì ìœ¼ë¡œ ëª¨ë¸ ì €ì¥
        save_path = os.path.join(MODEL_SAVE_DIR, "lightweight_captioning_model_{}_epoch_meteor_{}.pth".format(epoch+1, avg_meteor:.4f))
        try:
            torch.save({
                'model_state_dict': model.state_dict(),
                'word_map': word_map,
                'rev_word_map': rev_word_map,
                'vocab_size': vocab_size,
                'epoch': epoch + 1,
                'train_loss': avg_train_loss,
                'val_loss': avg_val_loss
            }, save_path)
            print("âœ… ëª¨ë¸ ì €ì¥ ì™„ë£Œ: {}".format(save_path))
        except Exception as e:
            print("âŒ ëª¨ë¸ ì €ì¥ ì‹¤íŒ¨: {}".format(e))
            print("   ì €ì¥ ê²½ë¡œ: {}".format(save_path))
        
        print("{'='*70}\n")
    
    # 8. ìµœì¢… ëª¨ë¸ ì €ì¥
    final_save_path = os.path.join(MODEL_SAVE_DIR, "lightweight_captioning_model.pth")
    try:
        torch.save({
            'model_state_dict': model.state_dict(),
            'word_map': word_map,
            'rev_word_map': rev_word_map,
            'vocab_size': vocab_size,
            'epoch': EPOCHS,
            'train_losses': train_losses,
            'val_losses': val_losses
        }, final_save_path)
        print("\nâœ… ìµœì¢… ëª¨ë¸ ì €ì¥ ì™„ë£Œ: {}".format(final_save_path))
        
        # í•™ìŠµ í†µê³„ ì¶œë ¥
        print("\n{'='*70}")
        print("ğŸ“Š í•™ìŠµ ì™„ë£Œ í†µê³„:")
        print("{'='*70}")
        print("  â€¢ ìµœì¢… í•™ìŠµ ì†ì‹¤: {}".format(train_losses[-1]:.4f))
        print("  â€¢ ìµœì¢… ê²€ì¦ ì†ì‹¤: {}".format(val_losses[-1]:.4f))
        print("  â€¢ ìµœì†Œ ê²€ì¦ ì†ì‹¤: {} (Epoch {})".format(min(val_losses):.4f, val_losses.index(min(val_losses))+1))
        print("  â€¢ í•™ìŠµ ì†ì‹¤ ê°œì„ ë„: {}%".format(((train_losses[0]-train_losses[-1])/train_losses[0]*100):.2f))
        print("  â€¢ ê²€ì¦ ì†ì‹¤ ê°œì„ ë„: {}%".format(((val_losses[0]-val_losses[-1])/val_losses[0]*100):.2f))
        print("{'='*70}\n")
    except Exception as e:
        print("âŒ ìµœì¢… ëª¨ë¸ ì €ì¥ ì‹¤íŒ¨: {}".format(e))
        print("   ì €ì¥ ê²½ë¡œ: {}".format(final_save_path))

if __name__ == "__main__":
    main()