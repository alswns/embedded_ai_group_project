import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, Dataset
from torchvision import transforms
from PIL import Image
import os
import re
import random
import numpy as np
from collections import Counter, defaultdict
from src.muti_modal_model.model import MobileNetCaptioningModel

# METEOR ì ìˆ˜ ê³„ì‚°ì„ ìœ„í•œ nltk
try:
    from nltk.translate.meteor_score import meteor_score
    from nltk.tokenize import word_tokenize
    import nltk
    # í•„ìš”í•œ ë°ì´í„° ë‹¤ìš´ë¡œë“œ
    try:
        nltk.data.find('tokenizers/punkt')
    except LookupError:
        nltk.download('punkt', quiet=True)
    try:
        nltk.data.find('wordnet')
    except LookupError:
        nltk.download('wordnet', quiet=True)
    METEOR_AVAILABLE = True
except ImportError:
    print("âš ï¸ nltkê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. METEOR ì ìˆ˜ë¥¼ ê³„ì‚°í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    print("   ì„¤ì¹˜: pip install nltk")
    METEOR_AVAILABLE = False
    meteor_score = None

# --- [0] ì„¤ì • (Configuration) ---
# ë””ë°”ì´ìŠ¤ ì„ íƒ: CUDA > MPS > CPU
if torch.cuda.is_available():
    device = torch.device("cuda")
    print("CUDA ì‚¬ìš© ê°€ëŠ¥ - GPU ê°€ì† í™œì„±í™”")
elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():
    device = torch.device("mps")
    print("MPS ì‚¬ìš© ê°€ëŠ¥ - Apple Silicon GPU ê°€ì† í™œì„±í™”")
else:
    device = torch.device("cpu")
    print("CPU ëª¨ë“œë¡œ ì‹¤í–‰")

LEARNING_RATE = 2e-4  # í•™ìŠµë¥  (ë„ˆë¬´ í¬ë©´ ë°œì‚°í•¨)
BATCH_SIZE = 64 if device.type != "cpu" else 16  # GPU/MPS ì‚¬ìš© ì‹œ ë” í° ë°°ì¹˜
EPOCHS = 100          # ì „ì²´ ë°˜ë³µ íšŸìˆ˜
MAX_CAPTION_LEN = 50  # ìµœëŒ€ ìº¡ì…˜ ê¸¸ì´
MIN_WORD_FREQ = 2     # ë‹¨ì–´ì¥ì— í¬í•¨ë  ìµœì†Œ ë¹ˆë„
ENCODER_FINE_TUNING = True
USE_MIXED_PRECISION = device.type in ["cuda", "mps"]  # Mixed precision (FP16) ì‚¬ìš©
NUM_WORKERS = 0 if device.type == "mps" else 4  # MPSì—ì„œëŠ” 0ì´ ì•ˆì „, CUDAì—ì„œëŠ” ë©€í‹°í”„ë¡œì„¸ì‹±
PIN_MEMORY = device.type != "cpu"  # GPU ì‚¬ìš© ì‹œ ë©”ëª¨ë¦¬ ê³ ì •

# === ê²½ë¡œ ì„¤ì • (Colab í™˜ê²½ ìë™ ê°ì§€) ===
# Colab í™˜ê²½ ê°ì§€
IS_COLAB = 'COLAB_GPU' in os.environ or 'COLAB_TPU' in os.environ

if IS_COLAB:
    # Colab Google Drive ê²½ë¡œ
    BASE_DIR = "/content/drive/MyDrive"
    IMAGES_DIR = os.path.join(BASE_DIR, "assets/images")
    CAPTIONS_FILE = os.path.join(BASE_DIR, "assets/captions.txt")
    MODEL_SAVE_DIR = os.path.join(BASE_DIR, "models")
    ASSETS_DIR = os.path.join(BASE_DIR, "assets")
    
    print(f"ğŸ”µ Colab í™˜ê²½ ê°ì§€ë¨")
    print(f"   ì´ë¯¸ì§€ ê²½ë¡œ: {IMAGES_DIR}")
    print(f"   ìº¡ì…˜ íŒŒì¼: {CAPTIONS_FILE}")
    print(f"   ëª¨ë¸ ì €ì¥ ê²½ë¡œ: {MODEL_SAVE_DIR}")
    
    # ëª¨ë¸ ì €ì¥ ë””ë ‰í† ë¦¬ ìƒì„±
    os.makedirs(MODEL_SAVE_DIR, exist_ok=True)
else:
    # ë¡œì»¬ í™˜ê²½
    IMAGES_DIR = "assets/images"
    CAPTIONS_FILE = "assets/captions.txt"
    MODEL_SAVE_DIR = "models"  # models í´ë”ì— ì €ì¥
    ASSETS_DIR = "assets"
    print(f"ğŸŸ¢ ë¡œì»¬ í™˜ê²½")
    
    # ëª¨ë¸ ì €ì¥ ë””ë ‰í† ë¦¬ ìƒì„±
    os.makedirs(MODEL_SAVE_DIR, exist_ok=True)

# ì‚¬ì „ í•™ìŠµëœ ì„ë² ë”© ì„¤ì •
EMBED_DIM = 300  # GloVe 6B.300d ì‚¬ìš©
USE_PRETRAINED_EMBEDDING = True  # ì‚¬ì „ í•™ìŠµëœ ì„ë² ë”© ì‚¬ìš© ì—¬ë¶€
# GloVe íŒŒì¼ ê²½ë¡œ (assets í•˜ìœ„ì— ìœ„ì¹˜)
# ë‹¤ìš´ë¡œë“œ: wget http://nlp.stanford.edu/data/glove.6B.zip && unzip glove.6B.zip
# íŒŒì¼ì„ assets/glove.6B.300d.txt ìœ„ì¹˜ì— ì €ì¥
GLOVE_PATH = os.path.join(ASSETS_DIR, "glove.6B.300d.txt")

# --- [1] ì´ë¯¸ì§€ ì „ì²˜ë¦¬ ---
transform = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406], 
                         std=[0.229, 0.224, 0.225])  # ImageNet ì •ê·œí™”
])

# --- [2] ìº¡ì…˜ ì „ì²˜ë¦¬ í•¨ìˆ˜ ---
def load_glove_embeddings(glove_path, embed_dim=300):
    """GloVe ì„ë² ë”© íŒŒì¼ ë¡œë“œ"""
    print(f"GloVe ì„ë² ë”© ë¡œë“œ ì¤‘: {glove_path}")
    embeddings_dict = {}
    
    if not os.path.exists(glove_path):
        print(f"âš ï¸ GloVe íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {glove_path}")
        print("\nğŸ“¥ GloVe ë‹¤ìš´ë¡œë“œ ë°©ë²•:")
        print("  ë°©ë²• 1 (í„°ë¯¸ë„):")
        print(f"    wget http://nlp.stanford.edu/data/glove.6B.zip")
        print(f"    unzip glove.6B.zip")
        print(f"    mv glove.6B.300d.txt {ASSETS_DIR}/")
        print("  ë°©ë²• 2 (Colab):")
        print(f"    !wget http://nlp.stanford.edu/data/glove.6B.zip")
        print(f"    !unzip glove.6B.zip")
        print(f"    !mv glove.6B.300d.txt {ASSETS_DIR}/")
        print("  ë°©ë²• 3 (ìˆ˜ë™):")
        print("    https://nlp.stanford.edu/projects/glove/ ì—ì„œ ë‹¤ìš´ë¡œë“œ")
        print(f"    ë‹¤ìš´ë¡œë“œí•œ glove.6B.300d.txt íŒŒì¼ì„ {ASSETS_DIR}/ í´ë”ì— ì €ì¥")
        print(f"\nğŸ’¡ GloVe íŒŒì¼ì´ ì—†ìœ¼ë©´ ëœë¤ ì´ˆê¸°í™”ëœ ì„ë² ë”©ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.")
        print(f"   ì˜ˆìƒ ê²½ë¡œ: {glove_path}\n")
        return None
    
    try:
        with open(glove_path, 'r', encoding='utf-8') as f:
            for line in f:
                values = line.split()
                word = values[0]
                vector = np.asarray(values[1:], dtype='float32')
                if len(vector) == embed_dim:
                    embeddings_dict[word] = vector
        
        print(f"âœ… GloVe ì„ë² ë”© ë¡œë“œ ì™„ë£Œ: {len(embeddings_dict)}ê°œ ë‹¨ì–´")
        return embeddings_dict
    except Exception as e:
        print(f"âš ï¸ GloVe ë¡œë“œ ì‹¤íŒ¨: {e}")
        return None

def create_embedding_matrix(word_map, glove_embeddings=None, embed_dim=300):
    """ë‹¨ì–´ì¥ì— ë§ëŠ” ì„ë² ë”© í–‰ë ¬ ìƒì„±"""
    vocab_size = len(word_map)
    embedding_matrix = np.random.normal(scale=0.6, size=(vocab_size, embed_dim))
    
    if glove_embeddings is None:
        print("âš ï¸ ì‚¬ì „ í•™ìŠµëœ ì„ë² ë”© ì—†ìŒ - ëœë¤ ì´ˆê¸°í™” ì‚¬ìš©")
        return embedding_matrix
    
    # íŠ¹ìˆ˜ í† í°ì€ ëœë¤ ì´ˆê¸°í™” ìœ ì§€
    found_count = 0
    for word, idx in word_map.items():
        if word in ['<pad>', '<start>', '<end>', '<unk>']:
            continue  # íŠ¹ìˆ˜ í† í°ì€ ëœë¤ ì´ˆê¸°í™” ìœ ì§€
        
        if word in glove_embeddings:
            embedding_matrix[idx] = glove_embeddings[word]
            found_count += 1
        elif word.lower() in glove_embeddings:
            embedding_matrix[idx] = glove_embeddings[word.lower()]
            found_count += 1
    
    print(f"âœ… ì„ë² ë”© í–‰ë ¬ ìƒì„± ì™„ë£Œ: {found_count}/{vocab_size-4}ê°œ ë‹¨ì–´ ë§¤ì¹­ (íŠ¹ìˆ˜ í† í° ì œì™¸)")
    return embedding_matrix

def build_vocab(captions, min_freq=MIN_WORD_FREQ):
    """ìº¡ì…˜ ë¦¬ìŠ¤íŠ¸ë¡œë¶€í„° ë‹¨ì–´ì¥ ìƒì„±"""
    # ëª¨ë“  ë‹¨ì–´ ìˆ˜ì§‘
    word_counts = Counter()
    for caption in captions:
        # ì†Œë¬¸ì ë³€í™˜ ë° ë‹¨ì–´ ë¶„ë¦¬
        words = re.findall(r'\w+', caption.lower())
        word_counts.update(words)
    
    # ìµœì†Œ ë¹ˆë„ ì´ìƒì¸ ë‹¨ì–´ë§Œ ì„ íƒ
    vocab = {'<pad>': 0, '<start>': 1, '<end>': 2, '<unk>': 3}
    idx = 4
    for word, count in word_counts.items():
        if count >= min_freq:
            vocab[word] = idx
            idx += 1
    
    return vocab, {v: k for k, v in vocab.items()}

def encode_caption(caption, word_map, max_len=MAX_CAPTION_LEN):
    """ìº¡ì…˜ í…ìŠ¤íŠ¸ë¥¼ ì •ìˆ˜ ì‹œí€€ìŠ¤ë¡œ ë³€í™˜"""
    words = re.findall(r'\w+', caption.lower())
    encoded = [word_map.get('<start>', 1)]
    
    for word in words:
        encoded.append(word_map.get(word, word_map.get('<unk>', 3)))
    
    encoded.append(word_map.get('<end>', 2))
    
    # íŒ¨ë”© ë˜ëŠ” ìë¥´ê¸°
    if len(encoded) > max_len:
        encoded = encoded[:max_len]
        encoded[-1] = word_map.get('<end>', 2)  # ë§ˆì§€ë§‰ì„ <end>ë¡œ
    else:
        encoded.extend([word_map.get('<pad>', 0)] * (max_len - len(encoded)))
    
    return torch.tensor(encoded, dtype=torch.long)

# --- [3] ì‹¤ì œ ë°ì´í„°ì…‹ í´ë˜ìŠ¤ ---
class CaptionDataset(Dataset):
    def __init__(self, images_dir, captions_file, transform=None, word_map=None, max_len=MAX_CAPTION_LEN):
        self.images_dir = images_dir
        self.transform = transform
        self.word_map = word_map
        self.max_len = max_len
        
        # ì´ë¯¸ì§€ íŒŒì¼ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°
        available_images = set([f for f in os.listdir(images_dir) 
                               if f.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp'))])
        
        # ìº¡ì…˜ íŒŒì¼ ì½ê¸° (CSV í˜•ì‹ ì§€ì›)
        # ì´ë¯¸ì§€ íŒŒì¼ëª… -> ìº¡ì…˜ ë¦¬ìŠ¤íŠ¸ ë§¤í•‘
        image_to_captions = defaultdict(list)
        
        if os.path.exists(captions_file):
            with open(captions_file, 'r', encoding='utf-8') as f:
                lines = f.readlines()
                # ì²« ë²ˆì§¸ ì¤„ì´ í—¤ë”ì¸ì§€ í™•ì¸
                first_line = lines[0].strip() if lines else ""
                start_idx = 1 if first_line.lower().startswith('image') or first_line.lower().startswith('filename') else 0
                
                for line in lines[start_idx:]:
                    line = line.strip()
                    if not line:
                        continue
                    
                    # CSV í˜•ì‹ (ì‰¼í‘œë¡œ êµ¬ë¶„)
                    if ',' in line:
                        parts = line.split(',', 1)  # ì²« ë²ˆì§¸ ì‰¼í‘œë§Œìœ¼ë¡œ ë¶„ë¦¬
                        if len(parts) == 2:
                            img_name = parts[0].strip()
                            caption = parts[1].strip()
                            if img_name and caption and img_name in available_images:
                                image_to_captions[img_name].append(caption)
                    # íƒ­ìœ¼ë¡œ êµ¬ë¶„ëœ ê²½ìš°
                    elif '\t' in line:
                        parts = line.split('\t', 1)
                        if len(parts) == 2:
                            img_name = parts[0].strip()
                            caption = parts[1].strip()
                            if img_name and caption and img_name in available_images:
                                image_to_captions[img_name].append(caption)
                    # ë‹¨ìˆœ ìº¡ì…˜ë§Œ ìˆëŠ” ê²½ìš° (ì´ë¯¸ì§€ íŒŒì¼ëª… ìˆœì„œëŒ€ë¡œ ë§¤ì¹­)
                    else:
                        # ì´ ê²½ìš°ëŠ” ë‚˜ì¤‘ì— ì²˜ë¦¬
                        pass
        
        # ì´ë¯¸ì§€-ìº¡ì…˜ ìŒ ìƒì„± (í•˜ë‚˜ì˜ ì´ë¯¸ì§€ì— ì—¬ëŸ¬ ìº¡ì…˜ì´ ìˆìœ¼ë©´ ê°ê° ë³„ë„ ìƒ˜í”Œë¡œ)
        self.image_caption_pairs = []
        for img_name, captions in image_to_captions.items():
            if captions:
                # ê° ìº¡ì…˜ì„ ë³„ë„ ìƒ˜í”Œë¡œ ì¶”ê°€
                for caption in captions:
                    self.image_caption_pairs.append((img_name, caption))
        # ë‹¨ìˆœ ìº¡ì…˜ë§Œ ìˆëŠ” ê²½ìš° ì²˜ë¦¬ (ì´ë¯¸ì§€ íŒŒì¼ ìˆœì„œëŒ€ë¡œ ë§¤ì¹­)
        if not self.image_caption_pairs:
            image_files = sorted(available_images)
            if os.path.exists(captions_file):
                with open(captions_file, 'r', encoding='utf-8') as f:
                    lines = f.readlines()
                    captions_only = [line.strip() for line in lines if line.strip() and not line.strip().startswith('image')]
                    for img_file, caption in zip(image_files, captions_only):
                        if caption:
                            self.image_caption_pairs.append((img_file, caption))
        
        print(f"ë¡œë“œëœ ë°ì´í„°: {len(self.image_caption_pairs)}ê°œì˜ ì´ë¯¸ì§€-ìº¡ì…˜ ìŒ")
        print(f"ê³ ìœ  ì´ë¯¸ì§€ ìˆ˜: {len(set([pair[0] for pair in self.image_caption_pairs]))}")
        
    def __getitem__(self, idx):
        # ì´ë¯¸ì§€ ë¡œë“œ
        img_name, caption_text = self.image_caption_pairs[idx]
        img_path = os.path.join(self.images_dir, img_name)
        
        try:
            image = Image.open(img_path).convert('RGB')
            if self.transform:
                image = self.transform(image)
        except Exception as e:
            print(f"ì´ë¯¸ì§€ ë¡œë“œ ì‹¤íŒ¨: {img_path}, ì˜¤ë¥˜: {e}")
            # ì˜¤ë¥˜ ì‹œ ê²€ì€ ì´ë¯¸ì§€ ë°˜í™˜
            image = torch.zeros(3, 224, 224)
        # ìº¡ì…˜ ì¸ì½”ë”©
        if self.word_map:
            caption = encode_caption(caption_text, self.word_map, self.max_len)
        else:
            # ë‹¨ì–´ì¥ì´ ì—†ìœ¼ë©´ ë”ë¯¸ ìº¡ì…˜
            caption = torch.zeros(self.max_len, dtype=torch.long)
        
        return image, caption
    
    def __len__(self):
        return len(self.image_caption_pairs)

# --- [2] í•™ìŠµ í•¨ìˆ˜ ì •ì˜ ---
def train_epoch(model, dataloader, criterion, optimizer, epoch, vocab_size, scaler=None, use_mixed_precision=False):
    model.train() # í•™ìŠµ ëª¨ë“œ ì„¤ì •
    total_loss = 0
    
    for i, (imgs, caps) in enumerate(dataloader):
        imgs = imgs.to(device, non_blocking=True)
        caps = caps.to(device, non_blocking=True)
        
        # 1. ê¸°ìš¸ê¸° ì´ˆê¸°í™”
        optimizer.zero_grad()
        
        # 2. Mixed Precision Training (FP16)
        if use_mixed_precision:
            if device.type == "cuda" and scaler is not None:
                with torch.cuda.amp.autocast():
                    # ëª¨ë¸ ì˜ˆì¸¡ (Forward)
                    outputs, alphas = model(imgs, caps)
                    
                    # ì •ë‹µê³¼ ë¹„êµë¥¼ ìœ„í•œ ì°¨ì› ì¡°ì ˆ
                    targets = caps[:, 1:] 
                    outputs = outputs[:, :targets.shape[1], :]
                    
                    # ì†ì‹¤ ê³„ì‚°
                    loss = criterion(outputs.reshape(-1, vocab_size), targets.reshape(-1))
                
                # ì—­ì „íŒŒ (Scaled)
                scaler.scale(loss).backward()
                scaler.step(optimizer)
                scaler.update()
            elif device.type == "mps":
                # MPSëŠ” autocastë§Œ ì§€ì›
                with torch.amp.autocast(device_type="mps", dtype=torch.float16):
                    # ëª¨ë¸ ì˜ˆì¸¡ (Forward)
                    outputs, alphas = model(imgs, caps)
                    
                    # ì •ë‹µê³¼ ë¹„êµë¥¼ ìœ„í•œ ì°¨ì› ì¡°ì ˆ
                    targets = caps[:, 1:] 
                    outputs = outputs[:, :targets.shape[1], :]
                    
                    # ì†ì‹¤ ê³„ì‚°
                    loss = criterion(outputs.reshape(-1, vocab_size), targets.reshape(-1))
                
                # ì—­ì „íŒŒ
                loss.backward()
                optimizer.step()
            else:
                # ì¼ë°˜ í•™ìŠµìœ¼ë¡œ í´ë°±
                outputs, alphas = model(imgs, caps)
                targets = caps[:, 1:] 
                outputs = outputs[:, :targets.shape[1], :]
                loss = criterion(outputs.reshape(-1, vocab_size), targets.reshape(-1))
                loss.backward()
                optimizer.step()
        else:
            # ì¼ë°˜ í•™ìŠµ (FP32)
            # ëª¨ë¸ ì˜ˆì¸¡ (Forward)
            outputs, alphas = model(imgs, caps)
            
            # ì •ë‹µê³¼ ë¹„êµë¥¼ ìœ„í•œ ì°¨ì› ì¡°ì ˆ
            targets = caps[:, 1:] 
            outputs = outputs[:, :targets.shape[1], :]
            
            # ì†ì‹¤ ê³„ì‚°
            loss = criterion(outputs.reshape(-1, vocab_size), targets.reshape(-1))
            
            # ì—­ì „íŒŒ
            loss.backward()
            
            # ê°€ì¤‘ì¹˜ ì—…ë°ì´íŠ¸
            optimizer.step()
        
        total_loss += loss.item()
        
        if i % 10 == 0:
            print(f"Epoch [{epoch+1}/{EPOCHS}], Step [{i}/{len(dataloader)}], Loss: {loss.item():.4f}")
    return total_loss / len(dataloader)

# --- [3] ì—¬ëŸ¬ ìƒ˜í”Œë¡œ ìº¡ì…˜ ìƒì„± ë° ê²€ì¦ ì¶œë ¥ ---
def evaluate_multiple_samples(model, dataset, word_map, rev_word_map, num_samples=5, start_idx=0):
    """ì—¬ëŸ¬ ìƒ˜í”Œ ì´ë¯¸ì§€ë¡œ ìº¡ì…˜ì„ ìƒì„±í•˜ê³  METEOR ì ìˆ˜ë¡œ ê²€ì¦"""
    model.eval()
    
    results = []
    meteor_scores = []
    
    print(f"\n{'='*70}")
    print(f"ğŸ” ê²€ì¦: {num_samples}ê°œ ìƒ˜í”Œë¡œ ìº¡ì…˜ ìƒì„± ë° METEOR í‰ê°€")
    print(f"{'='*70}")
    
    with torch.no_grad():
        for i in range(num_samples):
            idx = (start_idx + i) % len(dataset)
            
            img_name, original_caption = dataset.image_caption_pairs[idx]
            image, _ = dataset[idx]
            
            # ì´ë¯¸ì§€ íŒŒì¼ ì „ì²´ ê²½ë¡œ
            img_path = os.path.join(dataset.images_dir, img_name)
            
            # ë°°ì¹˜ ì°¨ì› ì¶”ê°€ [1, 3, 224, 224]
            image = image.unsqueeze(0).to(device)
            
            try:
                # ìº¡ì…˜ ìƒì„±
                generated_words = model.generate(image, word_map, rev_word_map, max_len=MAX_CAPTION_LEN)
                
                # í† í° ì œê±°í•˜ê³  ë¬¸ì¥ìœ¼ë¡œ ë³€í™˜
                generated_caption = ' '.join([w for w in generated_words if w not in ['<start>', '<end>', '<pad>', '<unk>']])
                
                # METEOR ì ìˆ˜ ê³„ì‚°
                meteor = 0.0
                if METEOR_AVAILABLE and meteor_score:
                    try:
                        # METEORëŠ” referenceë¥¼ ë¦¬ìŠ¤íŠ¸ë¡œ ë°›ìŒ (ì—¬ëŸ¬ ì°¸ì¡° ê°€ëŠ¥)
                        reference = [original_caption.lower().split()]
                        hypothesis = generated_caption.lower().split()
                        meteor = meteor_score(reference, hypothesis)
                    except Exception as e:
                        # METEOR ê³„ì‚° ì‹¤íŒ¨ ì‹œ ë‹¨ì–´ ì¼ì¹˜ìœ¨ë¡œ ëŒ€ì²´
                        original_words = set(original_caption.lower().split())
                        generated_words_set = set(generated_caption.lower().split())
                        common_words = original_words & generated_words_set
                        meteor = len(common_words) / len(original_words) if len(original_words) > 0 else 0.0
                else:
                    # nltkê°€ ì—†ìœ¼ë©´ ë‹¨ì–´ ì¼ì¹˜ìœ¨ë¡œ ëŒ€ì²´
                    original_words = set(original_caption.lower().split())
                    generated_words_set = set(generated_caption.lower().split())
                    common_words = original_words & generated_words_set
                    meteor = len(common_words) / len(original_words) if len(original_words) > 0 else 0.0
                
                meteor_scores.append(meteor)
                
                results.append({
                    'img_name': img_name,
                    'original': original_caption,
                    'generated': generated_caption,
                    'meteor': meteor
                })
                
                # ê° ìƒ˜í”Œ ì¶œë ¥
                print(f"\n[ìƒ˜í”Œ {i+1}/{num_samples}]")
                print(f"  ğŸ“¸ ì´ë¯¸ì§€: {img_name}")
                print(f"  ğŸ“ ì›ë³¸: {original_caption}")
                print(f"  ğŸ¤– ìƒì„±: {generated_caption}")
                print(f"  â­ METEOR: {meteor:.4f}")
                
            except Exception as e:
                print(f"  âš ï¸ ìƒ˜í”Œ {i+1} ìƒì„± ì‹¤íŒ¨: {e}")
                meteor_scores.append(0.0)
                results.append({
                    'img_name': img_name,
                    'original': original_caption,
                    'generated': 'ìƒì„± ì‹¤íŒ¨',
                    'meteor': 0.0
                })
    
    # ì „ì²´ í†µê³„ ì¶œë ¥
    avg_meteor = sum(meteor_scores) / len(meteor_scores) if meteor_scores else 0.0
    good_results = sum([1 for score in meteor_scores if score > 0.3])  # 0.3 ì´ìƒì„ ì¢‹ì€ ê²°ê³¼ë¡œ ê°„ì£¼
    
    print(f"\n{'='*70}")
    print(f"ğŸ“ˆ METEOR ê²€ì¦ í†µê³„:")
    print(f"  â€¢ í‰ê·  METEOR ì ìˆ˜: {avg_meteor:.4f}")
    print(f"  â€¢ ìµœê³  METEOR ì ìˆ˜: {max(meteor_scores):.4f}")
    print(f"  â€¢ ìµœì € METEOR ì ìˆ˜: {min(meteor_scores):.4f}")
    print(f"  â€¢ ì¢‹ì€ ê²°ê³¼ ë¹„ìœ¨: {good_results}/{num_samples} ({good_results/num_samples*100:.1f}%)")
    print(f"  â€¢ METEOR ì ìˆ˜ ë¶„í¬:")
    print(f"    - 0.5 ì´ìƒ (ìš°ìˆ˜): {sum([1 for s in meteor_scores if s >= 0.5])}ê°œ")
    print(f"    - 0.3-0.5 (ì–‘í˜¸): {sum([1 for s in meteor_scores if 0.3 <= s < 0.5])}ê°œ")
    print(f"    - 0.3 ë¯¸ë§Œ (ê°œì„  í•„ìš”): {sum([1 for s in meteor_scores if s < 0.3])}ê°œ")
    print(f"{'='*70}\n")
    
    model.train()  # ë‹¤ì‹œ í•™ìŠµ ëª¨ë“œë¡œ
    
    return {
        'avg_meteor': avg_meteor,
        'max_meteor': max(meteor_scores) if meteor_scores else 0.0,
        'min_meteor': min(meteor_scores) if meteor_scores else 0.0,
        'good_results': good_results / num_samples if num_samples > 0 else 0.0
    }

# --- [4] ë©”ì¸ ì‹¤í–‰ ì½”ë“œ ---
def main():
    # 1. ìº¡ì…˜ íŒŒì¼ ì½ì–´ì„œ ë‹¨ì–´ì¥ ìƒì„±
    print("ë‹¨ì–´ì¥ ìƒì„± ì¤‘...")
    captions_list = []
    if os.path.exists(CAPTIONS_FILE):
        with open(CAPTIONS_FILE, 'r', encoding='utf-8') as f:
            lines = f.readlines()
            # ì²« ë²ˆì§¸ ì¤„ì´ í—¤ë”ì¸ì§€ í™•ì¸
            first_line = lines[0].strip() if lines else ""
            start_idx = 1 if first_line.lower().startswith('image') or first_line.lower().startswith('filename') else 0
            
            for line in lines[start_idx:]:
                line = line.strip()
                if line:
                    # CSV í˜•ì‹ (ì‰¼í‘œë¡œ êµ¬ë¶„)
                    if ',' in line:
                        parts = line.split(',', 1)
                        if len(parts) == 2:
                            captions_list.append(parts[1].strip())
                    # íƒ­ìœ¼ë¡œ êµ¬ë¶„ëœ ê²½ìš° ìº¡ì…˜ ë¶€ë¶„ë§Œ ì¶”ì¶œ
                    elif '\t' in line:
                        parts = line.split('\t', 1)
                        captions_list.append(parts[1] if len(parts) > 1 else parts[0])
                    else:
                        captions_list.append(line)
    
    if not captions_list:
        print("ê²½ê³ : ìº¡ì…˜ íŒŒì¼ì´ ë¹„ì–´ìˆê±°ë‚˜ ì—†ìŠµë‹ˆë‹¤. ë”ë¯¸ ë°ì´í„°ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")
        captions_list = ["a cat sitting on a mat"] * 100  # ë”ë¯¸ ë°ì´í„°
    
    word_map, rev_word_map = build_vocab(captions_list, min_freq=MIN_WORD_FREQ)
    vocab_size = len(word_map)
    print(f"ë‹¨ì–´ì¥ í¬ê¸°: {vocab_size}")
    print(f"ì£¼ìš” ë‹¨ì–´ ì˜ˆì‹œ: {list(word_map.items())[:10]}")
    
    # ì‚¬ì „ í•™ìŠµëœ ì„ë² ë”© ë¡œë“œ
    use_pretrained = USE_PRETRAINED_EMBEDDING  # ë¡œì»¬ ë³€ìˆ˜ë¡œ ë³µì‚¬
    glove_embeddings = None
    if use_pretrained:
        glove_embeddings = load_glove_embeddings(GLOVE_PATH, embed_dim=EMBED_DIM)
        if glove_embeddings is None:
            print("âš ï¸ ì‚¬ì „ í•™ìŠµëœ ì„ë² ë”©ì„ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ëœë¤ ì´ˆê¸°í™”ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")
            use_pretrained = False
    
    # ì„ë² ë”© í–‰ë ¬ ìƒì„±
    embedding_matrix = None
    if use_pretrained and glove_embeddings:
        embedding_matrix = create_embedding_matrix(word_map, glove_embeddings, embed_dim=EMBED_DIM)
    else:
        # ëœë¤ ì´ˆê¸°í™” ì‚¬ìš© ì‹œì—ë„ embed_dimì€ ì„¤ì •ê°’ ì‚¬ìš©
        pass
    
    # 2. ë°ì´í„°ì…‹ ë° ë°ì´í„° ë¡œë” ì¤€ë¹„
    print("ë°ì´í„°ì…‹ ë¡œë“œ ì¤‘...")
    dataset = CaptionDataset(
        images_dir=IMAGES_DIR,
        captions_file=CAPTIONS_FILE,
        transform=transform,
        word_map=word_map,
        max_len=MAX_CAPTION_LEN
    )
    
    if len(dataset) == 0:
        raise ValueError(f"ë°ì´í„°ì…‹ì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤. {IMAGES_DIR} í´ë”ì— ì´ë¯¸ì§€ê°€ ìˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”.")
    
    # ìµœì í™”ëœ DataLoader ì„¤ì •
    dataloader = DataLoader(
        dataset, 
        batch_size=BATCH_SIZE, 
        shuffle=True, 
        num_workers=NUM_WORKERS,
        pin_memory=PIN_MEMORY,
        persistent_workers=True if NUM_WORKERS > 0 else False,
        prefetch_factor=2 if NUM_WORKERS > 0 else None
    )
    
    # 3. ëª¨ë¸ ì¤€ë¹„ (MobileNet + Decoder)
    print("ëª¨ë¸ ì´ˆê¸°í™” ì¤‘...")
    model = MobileNetCaptioningModel(vocab_size=vocab_size, embed_dim=EMBED_DIM).to(device)
    
    # ì‚¬ì „ í•™ìŠµëœ ì„ë² ë”© ê°€ì¤‘ì¹˜ ì„¤ì •
    if use_pretrained and embedding_matrix is not None:
        print("ì‚¬ì „ í•™ìŠµëœ ì„ë² ë”© ê°€ì¤‘ì¹˜ ì„¤ì • ì¤‘...")
        model.decoder.embedding.weight.data.copy_(torch.from_numpy(embedding_matrix))
        # ì„ë² ë”©ì„ í•™ìŠµ ê°€ëŠ¥í•˜ê²Œ í• ì§€ ê³ ì •í• ì§€ ì„ íƒ (True: í•™ìŠµ, False: ê³ ì •)
        model.decoder.embedding.weight.requires_grad = True
        print("âœ… ì‚¬ì „ í•™ìŠµëœ ì„ë² ë”© ê°€ì¤‘ì¹˜ ì„¤ì • ì™„ë£Œ")
    
    # ì²´í¬í¬ì¸íŠ¸ì—ì„œ ëª¨ë¸ ë¡œë“œ (ìˆëŠ” ê²½ìš°)
    checkpoint_path = os.path.join(MODEL_SAVE_DIR, "lightweight_captioning_model.pth")
    start_epoch = 0
    if os.path.exists(checkpoint_path):
        print(f"ğŸ“‚ ì²´í¬í¬ì¸íŠ¸ ë°œê²¬: {checkpoint_path}")
        try:
            checkpoint = torch.load(checkpoint_path, map_location=device)
            if isinstance(checkpoint, dict) and 'model_state_dict' in checkpoint:
                model.load_state_dict(checkpoint['model_state_dict'])
                start_epoch = checkpoint.get('epoch', 0)
                print(f"âœ… ì²´í¬í¬ì¸íŠ¸ì—ì„œ ëª¨ë¸ ë¡œë“œ ì™„ë£Œ (Epoch {start_epoch}ë¶€í„° ì´ì–´ì„œ í•™ìŠµ)")
            else:
                # ë”•ì…”ë„ˆë¦¬ê°€ ì•„ë‹Œ ê²½ìš° (êµ¬ë²„ì „ ì²´í¬í¬ì¸íŠ¸)
                model.load_state_dict(checkpoint)
                print(f"âœ… ì²´í¬í¬ì¸íŠ¸ì—ì„œ ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
        except Exception as e:
            print(f"âš ï¸ ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
            print("   ìƒˆë¡œ í•™ìŠµì„ ì‹œì‘í•©ë‹ˆë‹¤.")
    else:
        print("ğŸ“ ì²´í¬í¬ì¸íŠ¸ ì—†ìŒ - ìƒˆë¡œ í•™ìŠµ ì‹œì‘")
    
    model.to(device)
    
    # [í•µì‹¬] 4. ì¸ì½”ë” ì–¼ë¦¬ê¸° (Encoder Freezing)
    # MobileNet ë¶€ë¶„ì€ í•™ìŠµë˜ì§€ ì•Šë„ë¡ ì„¤ì • (ì´ë¯¸ì§€ë„· ì§€ì‹ ë³´ì¡´)
    for param in model.encoder.parameters():
        param.requires_grad = False
        
    # 5. ìµœì í™” ë„êµ¬ ì„¤ì •
    # filterë¥¼ ì¨ì„œ requires_grad=Trueì¸ íŒŒë¼ë¯¸í„°(ë””ì½”ë”)ë§Œ ì—…ë°ì´íŠ¸ ëª©ë¡ì— ë„£ìŒ
    optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=LEARNING_RATE)
    
    # 6. ì†ì‹¤ í•¨ìˆ˜ (Padding=0 ë¬´ì‹œ)
    criterion = nn.CrossEntropyLoss(ignore_index=0)
    
    # 7. Mixed Precision Scaler ì„¤ì •
    scaler = None
    use_mixed_precision = USE_MIXED_PRECISION  # ë¡œì»¬ ë³€ìˆ˜ë¡œ ë³µì‚¬

    if use_mixed_precision:
        if device.type == "cuda":
            scaler = torch.cuda.amp.GradScaler()
            print("Mixed Precision (FP16) í™œì„±í™” - CUDA")
        elif device.type == "mps":
            # MPSëŠ” autocastë§Œ ì§€ì›í•˜ê³  GradScalerëŠ” ì—†ìŒ
            print("Mixed Precision (FP16) í™œì„±í™” - MPS")
        else:
            use_mixed_precision = False
    
    # 8. í•™ìŠµ ë£¨í”„
    print(f"í•™ìŠµ ì‹œì‘ (Encoder Frozen)... ì´ {len(dataset)}ê°œ ìƒ˜í”Œ, {EPOCHS} ì—í¬í¬")
    print(f"ë°°ì¹˜ í¬ê¸°: {BATCH_SIZE}, ë””ë°”ì´ìŠ¤: {device}, Mixed Precision: {use_mixed_precision}")
    
    # ê²€ì¦ ì„¤ì •
    VAL_NUM_SAMPLES = 5  # ê²€ì¦ì— ì‚¬ìš©í•  ìƒ˜í”Œ ìˆ˜
    val_start_idx = 0  # ê²€ì¦ ì‹œì‘ ì¸ë±ìŠ¤ (ë§¤ epochë§ˆë‹¤ ë³€ê²½ ê°€ëŠ¥)
    
    # ì²´í¬í¬ì¸íŠ¸ì—ì„œ ì´ì–´ì„œ í•™ìŠµí•˜ëŠ” ê²½ìš°
    for epoch in range(start_epoch, EPOCHS):
        avg_loss = train_epoch(model, dataloader, criterion, optimizer, epoch, vocab_size, scaler, use_mixed_precision)
        print(f"=== Epoch {epoch+1}/{EPOCHS} ì™„ë£Œ. í‰ê·  Loss: {avg_loss:.4f} ===")
        
        # ì—¬ëŸ¬ ìƒ˜í”Œë¡œ ê²€ì¦ ë° ì¶œë ¥
        val_results = evaluate_multiple_samples(
            model, dataset, word_map, rev_word_map, 
            num_samples=VAL_NUM_SAMPLES, 
            start_idx=(val_start_idx + epoch * VAL_NUM_SAMPLES) % len(dataset)
        )
        
        # [ì˜µì…˜] íŠ¹ì • Epoch ì´í›„ì— ì¸ì½”ë”ë„ ê°™ì´ í•™ìŠµì‹œí‚¤ê³  ì‹¶ë‹¤ë©´? (Fine-tuning)
        if ENCODER_FINE_TUNING and epoch == 5:
            print(">>> ì¸ì½”ë” ë¯¸ì„¸ ì¡°ì • ì‹œì‘ (Fine-tuning Start) <<<")
            # ì¸ì½”ë”ì˜ ë’·ë¶€ë¶„ ë ˆì´ì–´ë§Œ í’€ê±°ë‚˜ ì „ì²´ë¥¼ í’‚
            for param in model.encoder.parameters():
                param.requires_grad = True
            
            # ì˜µí‹°ë§ˆì´ì €ì— ì¸ì½”ë” íŒŒë¼ë¯¸í„°ë„ ì¶”ê°€ (í•™ìŠµë¥ ì€ ë” ë‚®ê²Œ ì¡ëŠ” ê²Œ ì¢‹ìŒ)
            optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE * 0.1)

        # ì£¼ê¸°ì ìœ¼ë¡œ ëª¨ë¸ ì €ì¥
        save_path = os.path.join(MODEL_SAVE_DIR, f"lightweight_captioning_model_{epoch+1}_epoch.pth")
        try:
            torch.save({
                'model_state_dict': model.state_dict(),
                'word_map': word_map,
                'rev_word_map': rev_word_map,
                'vocab_size': vocab_size,
                'epoch': epoch + 1
            }, save_path)
            print(f"âœ… ëª¨ë¸ ì €ì¥ ì™„ë£Œ: {save_path}")
        except Exception as e:
            print(f"âŒ ëª¨ë¸ ì €ì¥ ì‹¤íŒ¨: {e}")
            print(f"   ì €ì¥ ê²½ë¡œ: {save_path}")
    
    # 8. ìµœì¢… ëª¨ë¸ ì €ì¥
    final_save_path = os.path.join(MODEL_SAVE_DIR, "lightweight_captioning_model.pth")
    try:
        torch.save({
            'model_state_dict': model.state_dict(),
            'word_map': word_map,
            'rev_word_map': rev_word_map,
            'vocab_size': vocab_size,
            'epoch': EPOCHS
        }, final_save_path)
        print(f"âœ… ìµœì¢… ëª¨ë¸ ì €ì¥ ì™„ë£Œ: {final_save_path}")
    except Exception as e:
        print(f"âŒ ìµœì¢… ëª¨ë¸ ì €ì¥ ì‹¤íŒ¨: {e}")
        print(f"   ì €ì¥ ê²½ë¡œ: {final_save_path}")

if __name__ == "__main__":
    main()