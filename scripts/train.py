import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, Dataset
from torchvision import transforms
from PIL import Image
import os
import re
import random
from collections import Counter, defaultdict
from src.muti_modal_model.model import MobileNetCaptioningModel

# --- [0] ì„¤ì • (Configuration) ---
# ë””ë°”ì´ìŠ¤ ì„ íƒ: CUDA > MPS > CPU
if torch.cuda.is_available():
    device = torch.device("cuda")
    print("CUDA ì‚¬ìš© ê°€ëŠ¥ - GPU ê°€ì† í™œì„±í™”")
elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():
    device = torch.device("mps")
    print("MPS ì‚¬ìš© ê°€ëŠ¥ - Apple Silicon GPU ê°€ì† í™œì„±í™”")
else:
    device = torch.device("cpu")
    print("CPU ëª¨ë“œë¡œ ì‹¤í–‰")

LEARNING_RATE = 4e-4  # í•™ìŠµë¥  (ë„ˆë¬´ í¬ë©´ ë°œì‚°í•¨)
BATCH_SIZE = 64 if device.type != "cpu" else 16  # GPU/MPS ì‚¬ìš© ì‹œ ë” í° ë°°ì¹˜
EPOCHS = 10           # ì „ì²´ ë°˜ë³µ íšŸìˆ˜
MAX_CAPTION_LEN = 50  # ìµœëŒ€ ìº¡ì…˜ ê¸¸ì´
MIN_WORD_FREQ = 2     # ë‹¨ì–´ì¥ì— í¬í•¨ë  ìµœì†Œ ë¹ˆë„
ENCODER_FINE_TUNING = True
USE_MIXED_PRECISION = device.type in ["cuda", "mps"]  # Mixed precision (FP16) ì‚¬ìš©
NUM_WORKERS = 0 if device.type == "mps" else 4  # MPSì—ì„œëŠ” 0ì´ ì•ˆì „, CUDAì—ì„œëŠ” ë©€í‹°í”„ë¡œì„¸ì‹±
PIN_MEMORY = device.type != "cpu"  # GPU ì‚¬ìš© ì‹œ ë©”ëª¨ë¦¬ ê³ ì •

# === ê²½ë¡œ ì„¤ì • (Colab í™˜ê²½ ìë™ ê°ì§€) ===
# Colab í™˜ê²½ ê°ì§€
IS_COLAB = 'COLAB_GPU' in os.environ or 'COLAB_TPU' in os.environ

if IS_COLAB:
    # Colab Google Drive ê²½ë¡œ
    BASE_DIR = "/content/drive/MyDrive"
    IMAGES_DIR = os.path.join(BASE_DIR, "assets/images")
    CAPTIONS_FILE = os.path.join(BASE_DIR, "assets/captions.txt")
    MODEL_SAVE_DIR = os.path.join(BASE_DIR, "models")
    
    print(f"ğŸ”µ Colab í™˜ê²½ ê°ì§€ë¨")
    print(f"   ì´ë¯¸ì§€ ê²½ë¡œ: {IMAGES_DIR}")
    print(f"   ìº¡ì…˜ íŒŒì¼: {CAPTIONS_FILE}")
    print(f"   ëª¨ë¸ ì €ì¥ ê²½ë¡œ: {MODEL_SAVE_DIR}")
    
    # ëª¨ë¸ ì €ì¥ ë””ë ‰í† ë¦¬ ìƒì„±
    os.makedirs(MODEL_SAVE_DIR, exist_ok=True)
else:
    # ë¡œì»¬ í™˜ê²½
    IMAGES_DIR = "assets/images"
    CAPTIONS_FILE = "assets/captions.txt"
    MODEL_SAVE_DIR = "."
    print(f"ğŸŸ¢ ë¡œì»¬ í™˜ê²½")

# --- [1] ì´ë¯¸ì§€ ì „ì²˜ë¦¬ ---
transform = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406], 
                         std=[0.229, 0.224, 0.225])  # ImageNet ì •ê·œí™”
])

# --- [2] ìº¡ì…˜ ì „ì²˜ë¦¬ í•¨ìˆ˜ ---
def build_vocab(captions, min_freq=MIN_WORD_FREQ):
    """ìº¡ì…˜ ë¦¬ìŠ¤íŠ¸ë¡œë¶€í„° ë‹¨ì–´ì¥ ìƒì„±"""
    # ëª¨ë“  ë‹¨ì–´ ìˆ˜ì§‘
    word_counts = Counter()
    for caption in captions:
        # ì†Œë¬¸ì ë³€í™˜ ë° ë‹¨ì–´ ë¶„ë¦¬
        words = re.findall(r'\w+', caption.lower())
        word_counts.update(words)
    
    # ìµœì†Œ ë¹ˆë„ ì´ìƒì¸ ë‹¨ì–´ë§Œ ì„ íƒ
    vocab = {'<pad>': 0, '<start>': 1, '<end>': 2, '<unk>': 3}
    idx = 4
    for word, count in word_counts.items():
        if count >= min_freq:
            vocab[word] = idx
            idx += 1
    
    return vocab, {v: k for k, v in vocab.items()}

def encode_caption(caption, word_map, max_len=MAX_CAPTION_LEN):
    """ìº¡ì…˜ í…ìŠ¤íŠ¸ë¥¼ ì •ìˆ˜ ì‹œí€€ìŠ¤ë¡œ ë³€í™˜"""
    words = re.findall(r'\w+', caption.lower())
    encoded = [word_map.get('<start>', 1)]
    
    for word in words:
        encoded.append(word_map.get(word, word_map.get('<unk>', 3)))
    
    encoded.append(word_map.get('<end>', 2))
    
    # íŒ¨ë”© ë˜ëŠ” ìë¥´ê¸°
    if len(encoded) > max_len:
        encoded = encoded[:max_len]
        encoded[-1] = word_map.get('<end>', 2)  # ë§ˆì§€ë§‰ì„ <end>ë¡œ
    else:
        encoded.extend([word_map.get('<pad>', 0)] * (max_len - len(encoded)))
    
    return torch.tensor(encoded, dtype=torch.long)

# --- [3] ì‹¤ì œ ë°ì´í„°ì…‹ í´ë˜ìŠ¤ ---
class CaptionDataset(Dataset):
    def __init__(self, images_dir, captions_file, transform=None, word_map=None, max_len=MAX_CAPTION_LEN):
        self.images_dir = images_dir
        self.transform = transform
        self.word_map = word_map
        self.max_len = max_len
        
        # ì´ë¯¸ì§€ íŒŒì¼ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°
        available_images = set([f for f in os.listdir(images_dir) 
                               if f.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp'))])
        
        # ìº¡ì…˜ íŒŒì¼ ì½ê¸° (CSV í˜•ì‹ ì§€ì›)
        # ì´ë¯¸ì§€ íŒŒì¼ëª… -> ìº¡ì…˜ ë¦¬ìŠ¤íŠ¸ ë§¤í•‘
        image_to_captions = defaultdict(list)
        
        if os.path.exists(captions_file):
            with open(captions_file, 'r', encoding='utf-8') as f:
                lines = f.readlines()
                # ì²« ë²ˆì§¸ ì¤„ì´ í—¤ë”ì¸ì§€ í™•ì¸
                first_line = lines[0].strip() if lines else ""
                start_idx = 1 if first_line.lower().startswith('image') or first_line.lower().startswith('filename') else 0
                
                for line in lines[start_idx:]:
                    line = line.strip()
                    if not line:
                        continue
                    
                    # CSV í˜•ì‹ (ì‰¼í‘œë¡œ êµ¬ë¶„)
                    if ',' in line:
                        parts = line.split(',', 1)  # ì²« ë²ˆì§¸ ì‰¼í‘œë§Œìœ¼ë¡œ ë¶„ë¦¬
                        if len(parts) == 2:
                            img_name = parts[0].strip()
                            caption = parts[1].strip()
                            if img_name and caption and img_name in available_images:
                                image_to_captions[img_name].append(caption)
                    # íƒ­ìœ¼ë¡œ êµ¬ë¶„ëœ ê²½ìš°
                    elif '\t' in line:
                        parts = line.split('\t', 1)
                        if len(parts) == 2:
                            img_name = parts[0].strip()
                            caption = parts[1].strip()
                            if img_name and caption and img_name in available_images:
                                image_to_captions[img_name].append(caption)
                    # ë‹¨ìˆœ ìº¡ì…˜ë§Œ ìˆëŠ” ê²½ìš° (ì´ë¯¸ì§€ íŒŒì¼ëª… ìˆœì„œëŒ€ë¡œ ë§¤ì¹­)
                    else:
                        # ì´ ê²½ìš°ëŠ” ë‚˜ì¤‘ì— ì²˜ë¦¬
                        pass
        
        # ì´ë¯¸ì§€-ìº¡ì…˜ ìŒ ìƒì„± (í•˜ë‚˜ì˜ ì´ë¯¸ì§€ì— ì—¬ëŸ¬ ìº¡ì…˜ì´ ìˆìœ¼ë©´ ê°ê° ë³„ë„ ìƒ˜í”Œë¡œ)
        self.image_caption_pairs = []
        for img_name, captions in image_to_captions.items():
            if captions:
                # ê° ìº¡ì…˜ì„ ë³„ë„ ìƒ˜í”Œë¡œ ì¶”ê°€
                for caption in captions:
                    self.image_caption_pairs.append((img_name, caption))
        # ë‹¨ìˆœ ìº¡ì…˜ë§Œ ìˆëŠ” ê²½ìš° ì²˜ë¦¬ (ì´ë¯¸ì§€ íŒŒì¼ ìˆœì„œëŒ€ë¡œ ë§¤ì¹­)
        if not self.image_caption_pairs:
            image_files = sorted(available_images)
            if os.path.exists(captions_file):
                with open(captions_file, 'r', encoding='utf-8') as f:
                    lines = f.readlines()
                    captions_only = [line.strip() for line in lines if line.strip() and not line.strip().startswith('image')]
                    for img_file, caption in zip(image_files, captions_only):
                        if caption:
                            self.image_caption_pairs.append((img_file, caption))
        
        print(f"ë¡œë“œëœ ë°ì´í„°: {len(self.image_caption_pairs)}ê°œì˜ ì´ë¯¸ì§€-ìº¡ì…˜ ìŒ")
        print(f"ê³ ìœ  ì´ë¯¸ì§€ ìˆ˜: {len(set([pair[0] for pair in self.image_caption_pairs]))}")
        
    def __getitem__(self, idx):
        # ì´ë¯¸ì§€ ë¡œë“œ
        img_name, caption_text = self.image_caption_pairs[idx]
        img_path = os.path.join(self.images_dir, img_name)
        
        try:
            image = Image.open(img_path).convert('RGB')
            if self.transform:
                image = self.transform(image)
        except Exception as e:
            print(f"ì´ë¯¸ì§€ ë¡œë“œ ì‹¤íŒ¨: {img_path}, ì˜¤ë¥˜: {e}")
            # ì˜¤ë¥˜ ì‹œ ê²€ì€ ì´ë¯¸ì§€ ë°˜í™˜
            image = torch.zeros(3, 224, 224)
        # ìº¡ì…˜ ì¸ì½”ë”©
        if self.word_map:
            caption = encode_caption(caption_text, self.word_map, self.max_len)
        else:
            # ë‹¨ì–´ì¥ì´ ì—†ìœ¼ë©´ ë”ë¯¸ ìº¡ì…˜
            caption = torch.zeros(self.max_len, dtype=torch.long)
        
        return image, caption
    
    def __len__(self):
        return len(self.image_caption_pairs)

# --- [2] í•™ìŠµ í•¨ìˆ˜ ì •ì˜ ---
def train_epoch(model, dataloader, criterion, optimizer, epoch, vocab_size, scaler=None, use_mixed_precision=False):
    model.train() # í•™ìŠµ ëª¨ë“œ ì„¤ì •
    total_loss = 0
    
    for i, (imgs, caps) in enumerate(dataloader):
        imgs = imgs.to(device, non_blocking=True)
        caps = caps.to(device, non_blocking=True)
        
        # 1. ê¸°ìš¸ê¸° ì´ˆê¸°í™”
        optimizer.zero_grad()
        
        # 2. Mixed Precision Training (FP16)
        if use_mixed_precision:
            if device.type == "cuda" and scaler is not None:
                with torch.cuda.amp.autocast():
                    # ëª¨ë¸ ì˜ˆì¸¡ (Forward)
                    outputs, alphas = model(imgs, caps)
                    
                    # ì •ë‹µê³¼ ë¹„êµë¥¼ ìœ„í•œ ì°¨ì› ì¡°ì ˆ
                    targets = caps[:, 1:] 
                    outputs = outputs[:, :targets.shape[1], :]
                    
                    # ì†ì‹¤ ê³„ì‚°
                    loss = criterion(outputs.reshape(-1, vocab_size), targets.reshape(-1))
                
                # ì—­ì „íŒŒ (Scaled)
                scaler.scale(loss).backward()
                scaler.step(optimizer)
                scaler.update()
            elif device.type == "mps":
                # MPSëŠ” autocastë§Œ ì§€ì›
                with torch.amp.autocast(device_type="mps", dtype=torch.float16):
                    # ëª¨ë¸ ì˜ˆì¸¡ (Forward)
                    outputs, alphas = model(imgs, caps)
                    
                    # ì •ë‹µê³¼ ë¹„êµë¥¼ ìœ„í•œ ì°¨ì› ì¡°ì ˆ
                    targets = caps[:, 1:] 
                    outputs = outputs[:, :targets.shape[1], :]
                    
                    # ì†ì‹¤ ê³„ì‚°
                    loss = criterion(outputs.reshape(-1, vocab_size), targets.reshape(-1))
                
                # ì—­ì „íŒŒ
                loss.backward()
                optimizer.step()
            else:
                # ì¼ë°˜ í•™ìŠµìœ¼ë¡œ í´ë°±
                outputs, alphas = model(imgs, caps)
                targets = caps[:, 1:] 
                outputs = outputs[:, :targets.shape[1], :]
                loss = criterion(outputs.reshape(-1, vocab_size), targets.reshape(-1))
                loss.backward()
                optimizer.step()
        else:
            # ì¼ë°˜ í•™ìŠµ (FP32)
            # ëª¨ë¸ ì˜ˆì¸¡ (Forward)
            outputs, alphas = model(imgs, caps)
            
            # ì •ë‹µê³¼ ë¹„êµë¥¼ ìœ„í•œ ì°¨ì› ì¡°ì ˆ
            targets = caps[:, 1:] 
            outputs = outputs[:, :targets.shape[1], :]
            
            # ì†ì‹¤ ê³„ì‚°
            loss = criterion(outputs.reshape(-1, vocab_size), targets.reshape(-1))
            
            # ì—­ì „íŒŒ
            loss.backward()
            
            # ê°€ì¤‘ì¹˜ ì—…ë°ì´íŠ¸
            optimizer.step()
        
        total_loss += loss.item()
        
        if i % 10 == 0:
            print(f"Epoch [{epoch+1}/{EPOCHS}], Step [{i}/{len(dataloader)}], Loss: {loss.item():.4f}")
            
    return total_loss / len(dataloader)

# --- [3] ì˜ˆì‹œ ì´ë¯¸ì§€ë¡œ ìº¡ì…˜ ìƒì„± ë° ì¶œë ¥ ---
def generate_example_caption(model, dataset, word_map, rev_word_map, example_idx=0):
    """ì˜ˆì‹œ ì´ë¯¸ì§€ë¡œ ìº¡ì…˜ì„ ìƒì„±í•˜ê³  ì¶œë ¥"""
    model.eval()
    
    # ì˜ˆì‹œ ì´ë¯¸ì§€ì™€ ì›ë³¸ ìº¡ì…˜ ê°€ì ¸ì˜¤ê¸°
    if example_idx >= len(dataset):
        example_idx = 0
    
    img_name, original_caption = dataset.image_caption_pairs[example_idx]
    image, _ = dataset[example_idx]
    
    # ì´ë¯¸ì§€ íŒŒì¼ ì „ì²´ ê²½ë¡œ
    img_path = os.path.join(dataset.images_dir, img_name)
    
    # ë°°ì¹˜ ì°¨ì› ì¶”ê°€ [1, 3, 224, 224]
    image = image.unsqueeze(0).to(device)
    
    # ìº¡ì…˜ ìƒì„±
    try:
        with torch.no_grad():
            generated_words = model.generate(image, word_map, rev_word_map, max_len=MAX_CAPTION_LEN)
        
        # í† í° ì œê±°í•˜ê³  ë¬¸ì¥ìœ¼ë¡œ ë³€í™˜
        generated_caption = ' '.join([w for w in generated_words if w not in ['<start>', '<end>', '<pad>', '<unk>']])
        
        print(f"\n{'='*60}")
        print(f"ğŸ“¸ ì´ë¯¸ì§€ íŒŒì¼ëª…: {img_name}")
        print(f"ğŸ“ ì´ë¯¸ì§€ ê²½ë¡œ: {img_path}")
        print(f"ğŸ“ ì›ë³¸ ìº¡ì…˜: {original_caption}")
        print(f"ğŸ¤– ìƒì„±ëœ ìº¡ì…˜: {generated_caption}")
        print(f"{'='*60}\n")
        
    except Exception as e:
        print(f"âš ï¸ ìº¡ì…˜ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()
    
    model.train()  # ë‹¤ì‹œ í•™ìŠµ ëª¨ë“œë¡œ

# --- [4] ë©”ì¸ ì‹¤í–‰ ì½”ë“œ ---
def main():
    # 1. ìº¡ì…˜ íŒŒì¼ ì½ì–´ì„œ ë‹¨ì–´ì¥ ìƒì„±
    print("ë‹¨ì–´ì¥ ìƒì„± ì¤‘...")
    captions_list = []
    if os.path.exists(CAPTIONS_FILE):
        with open(CAPTIONS_FILE, 'r', encoding='utf-8') as f:
            lines = f.readlines()
            # ì²« ë²ˆì§¸ ì¤„ì´ í—¤ë”ì¸ì§€ í™•ì¸
            first_line = lines[0].strip() if lines else ""
            start_idx = 1 if first_line.lower().startswith('image') or first_line.lower().startswith('filename') else 0
            
            for line in lines[start_idx:]:
                line = line.strip()
                if line:
                    # CSV í˜•ì‹ (ì‰¼í‘œë¡œ êµ¬ë¶„)
                    if ',' in line:
                        parts = line.split(',', 1)
                        if len(parts) == 2:
                            captions_list.append(parts[1].strip())
                    # íƒ­ìœ¼ë¡œ êµ¬ë¶„ëœ ê²½ìš° ìº¡ì…˜ ë¶€ë¶„ë§Œ ì¶”ì¶œ
                    elif '\t' in line:
                        parts = line.split('\t', 1)
                        captions_list.append(parts[1] if len(parts) > 1 else parts[0])
                    else:
                        captions_list.append(line)
    
    if not captions_list:
        print("ê²½ê³ : ìº¡ì…˜ íŒŒì¼ì´ ë¹„ì–´ìˆê±°ë‚˜ ì—†ìŠµë‹ˆë‹¤. ë”ë¯¸ ë°ì´í„°ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")
        captions_list = ["a cat sitting on a mat"] * 100  # ë”ë¯¸ ë°ì´í„°
    
    word_map, rev_word_map = build_vocab(captions_list, min_freq=MIN_WORD_FREQ)
    vocab_size = len(word_map)
    print(f"ë‹¨ì–´ì¥ í¬ê¸°: {vocab_size}")
    print(f"ì£¼ìš” ë‹¨ì–´ ì˜ˆì‹œ: {list(word_map.items())[:10]}")
    
    # 2. ë°ì´í„°ì…‹ ë° ë°ì´í„° ë¡œë” ì¤€ë¹„
    print("ë°ì´í„°ì…‹ ë¡œë“œ ì¤‘...")
    dataset = CaptionDataset(
        images_dir=IMAGES_DIR,
        captions_file=CAPTIONS_FILE,
        transform=transform,
        word_map=word_map,
        max_len=MAX_CAPTION_LEN
    )
    
    if len(dataset) == 0:
        raise ValueError(f"ë°ì´í„°ì…‹ì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤. {IMAGES_DIR} í´ë”ì— ì´ë¯¸ì§€ê°€ ìˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”.")
    
    # ìµœì í™”ëœ DataLoader ì„¤ì •
    dataloader = DataLoader(
        dataset, 
        batch_size=BATCH_SIZE, 
        shuffle=True, 
        num_workers=NUM_WORKERS,
        pin_memory=PIN_MEMORY,
        persistent_workers=True if NUM_WORKERS > 0 else False,
        prefetch_factor=2 if NUM_WORKERS > 0 else None
    )
    
    # 3. ëª¨ë¸ ì¤€ë¹„ (MobileNet + Decoder)
    print("ëª¨ë¸ ì´ˆê¸°í™” ì¤‘...")
    model = MobileNetCaptioningModel(vocab_size=vocab_size).to(device)
    
    # [í•µì‹¬] 4. ì¸ì½”ë” ì–¼ë¦¬ê¸° (Encoder Freezing)
    # MobileNet ë¶€ë¶„ì€ í•™ìŠµë˜ì§€ ì•Šë„ë¡ ì„¤ì • (ì´ë¯¸ì§€ë„· ì§€ì‹ ë³´ì¡´)
    for param in model.encoder.parameters():
        param.requires_grad = False
        
    # 5. ìµœì í™” ë„êµ¬ ì„¤ì •
    # filterë¥¼ ì¨ì„œ requires_grad=Trueì¸ íŒŒë¼ë¯¸í„°(ë””ì½”ë”)ë§Œ ì—…ë°ì´íŠ¸ ëª©ë¡ì— ë„£ìŒ
    optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=LEARNING_RATE)
    
    # 6. ì†ì‹¤ í•¨ìˆ˜ (Padding=0 ë¬´ì‹œ)
    criterion = nn.CrossEntropyLoss(ignore_index=0)
    
    # 7. Mixed Precision Scaler ì„¤ì •
    scaler = None
    use_mixed_precision = USE_MIXED_PRECISION  # ë¡œì»¬ ë³€ìˆ˜ë¡œ ë³µì‚¬

    if use_mixed_precision:
        if device.type == "cuda":
            scaler = torch.cuda.amp.GradScaler()
            print("Mixed Precision (FP16) í™œì„±í™” - CUDA")
        elif device.type == "mps":
            # MPSëŠ” autocastë§Œ ì§€ì›í•˜ê³  GradScalerëŠ” ì—†ìŒ
            print("Mixed Precision (FP16) í™œì„±í™” - MPS")
        else:
            use_mixed_precision = False
    
    # 8. í•™ìŠµ ë£¨í”„
    print(f"í•™ìŠµ ì‹œì‘ (Encoder Frozen)... ì´ {len(dataset)}ê°œ ìƒ˜í”Œ, {EPOCHS} ì—í¬í¬")
    print(f"ë°°ì¹˜ í¬ê¸°: {BATCH_SIZE}, ë””ë°”ì´ìŠ¤: {device}, Mixed Precision: {use_mixed_precision}")
    # ì˜ˆì‹œ ì´ë¯¸ì§€ ì¸ë±ìŠ¤ (ê³ ì •ëœ ì´ë¯¸ì§€ë¡œ í•™ìŠµ ì§„í–‰ ìƒí™© í™•ì¸)
    example_idx = 0
    
    for epoch in range(EPOCHS):
        avg_loss = train_epoch(model, dataloader, criterion, optimizer, epoch, vocab_size, scaler, use_mixed_precision)
        print(f"=== Epoch {epoch+1}/{EPOCHS} ì™„ë£Œ. í‰ê·  Loss: {avg_loss:.4f} ===")
        
        # ì˜ˆì‹œ ì´ë¯¸ì§€ë¡œ ìº¡ì…˜ ìƒì„± ë° ì¶œë ¥
        generate_example_caption(model, dataset, word_map, rev_word_map, example_idx)
        
        # [ì˜µì…˜] íŠ¹ì • Epoch ì´í›„ì— ì¸ì½”ë”ë„ ê°™ì´ í•™ìŠµì‹œí‚¤ê³  ì‹¶ë‹¤ë©´? (Fine-tuning)
        if ENCODER_FINE_TUNING and epoch == 5:
            print(">>> ì¸ì½”ë” ë¯¸ì„¸ ì¡°ì • ì‹œì‘ (Fine-tuning Start) <<<")
            # ì¸ì½”ë”ì˜ ë’·ë¶€ë¶„ ë ˆì´ì–´ë§Œ í’€ê±°ë‚˜ ì „ì²´ë¥¼ í’‚
            for param in model.encoder.parameters():
                param.requires_grad = True
            
            # ì˜µí‹°ë§ˆì´ì €ì— ì¸ì½”ë” íŒŒë¼ë¯¸í„°ë„ ì¶”ê°€ (í•™ìŠµë¥ ì€ ë” ë‚®ê²Œ ì¡ëŠ” ê²Œ ì¢‹ìŒ)
            optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE * 0.1)

        # ì£¼ê¸°ì ìœ¼ë¡œ ëª¨ë¸ ì €ì¥
        if (epoch + 1) % 5 == 0 or epoch == EPOCHS - 1:
            save_path = os.path.join(MODEL_SAVE_DIR, f"lightweight_captioning_model_{epoch+1}_epoch.pth")
            torch.save({
                'model_state_dict': model.state_dict(),
                'word_map': word_map,
                'rev_word_map': rev_word_map,
                'vocab_size': vocab_size,
                'epoch': epoch + 1
            }, save_path)
            print(f"ëª¨ë¸ ì €ì¥: {save_path}")
    
    # 8. ìµœì¢… ëª¨ë¸ ì €ì¥
    final_save_path = os.path.join(MODEL_SAVE_DIR, "lightweight_captioning_model.pth")
    torch.save({
        'model_state_dict': model.state_dict(),
        'word_map': word_map,
        'rev_word_map': rev_word_map,
        'vocab_size': vocab_size,
        'epoch': EPOCHS
    }, final_save_path)
    print(f"ìµœì¢… ëª¨ë¸ ì €ì¥ ì™„ë£Œ: {final_save_path}")

if __name__ == "__main__":
    main()