"""
ν”„λ£¨λ‹λ λ¨λΈ μ²΄ν¬ν¬μΈνΈ λ³µκµ¬ μ¤ν¬λ¦½νΈ
μ›λ³Έ λ¨λΈκ³Ό ν”„λ£¨λ‹λ λ¨λΈμ νΈν™μ„±μ„ λ§μ¶”κΈ° μ„ν•΄ μ²΄ν¬ν¬μΈνΈλ¥Ό μμ •ν•©λ‹λ‹¤.
"""
import torch
import os

def fix_pruned_checkpoint():
    """ν”„λ£¨λ‹λ μ²΄ν¬ν¬μΈνΈ λ³µκµ¬"""
    checkpoint_path = "pruning_results/Pruning_epoch_1_checkpoint.pt"
    
    if not os.path.exists(checkpoint_path):
        print("β μ²΄ν¬ν¬μΈνΈλ¥Ό μ°Ύμ„ μ μ—†μµλ‹λ‹¤: {}".format(checkpoint_path))
        return
    
    print("π“‚ μ²΄ν¬ν¬μΈνΈ λ΅λ“ μ¤‘: {}".format(checkpoint_path))
    
    # Python/PyTorch λ²„μ „ νΈν™μ„±
    try:
        # Python 3.11+: weights_only νλΌλ―Έν„° ν•„μ”
        checkpoint = torch.load(checkpoint_path, weights_only=False)
    except TypeError:
        # Python 3.6-3.10: weights_only νλΌλ―Έν„° λ―Έμ§€μ›
        checkpoint = torch.load(checkpoint_path)
    
    # μ²΄ν¬ν¬μΈνΈ κµ¬μ΅° ν™•μΈ
    print("\nπ“‹ μ²΄ν¬ν¬μΈνΈ κµ¬μ΅°:")
    for key in checkpoint.keys():
        print("   β€Ά {}".format(key))
    
    # model_state_dict ν¬κΈ° μ •λ³΄ μ¶λ ¥
    if 'model_state_dict' in checkpoint:
        print("\nπ“ λ¨λΈ λ μ΄μ–΄ μ •λ³΄:")
        state_dict = checkpoint['model_state_dict']
        
        # μ¤‘μ” λ μ΄μ–΄ ν¬κΈ° ν™•μΈ
        decoder_keys = [k for k in state_dict.keys() if 'decoder' in k]
        print("\n   Decoder λ μ΄μ–΄ ({}κ°):".format(len(decoder_keys)))
        for key in sorted(decoder_keys)[:5]:
            print("      β€Ά {}: {}".format(key, state_dict[key].shape))
        
        # μ›λ³Έ λ¨λΈκ³Όμ μ°¨μ΄μ  νμ•…
        print("\nπ’΅ μ²΄ν¬ν¬μΈνΈ μμ •:")
        
        # μμ„± λ¨λΈκ³Ό νΈν™λλ„λ΅ λ©”νƒ€λ°μ΄ν„° μ¶”κ°€
        if 'decoder_dim' not in checkpoint:
            # state_dictμ—μ„ decoder_dim μ¶”μ¶
            if 'decoder.decode_step.weight_ih' in state_dict:
                decoder_dim = state_dict['decoder.decode_step.weight_ih'].shape[0] // 3
                checkpoint['decoder_dim'] = decoder_dim
                print("   β“ decoder_dim μ¶”κ°€: {}".format(decoder_dim))
        
        if 'attention_dim' not in checkpoint:
            if 'decoder.encoder_att.weight' in state_dict:
                attention_dim = state_dict['decoder.encoder_att.weight'].shape[0]
                checkpoint['attention_dim'] = attention_dim
                print("   β“ attention_dim μ¶”κ°€: {}".format(attention_dim))
        
        # μ €μ¥
        torch.save(checkpoint, checkpoint_path)
        print("\nβ… μ²΄ν¬ν¬μΈνΈ μμ • μ™„λ£: {}".format(checkpoint_path))
    else:
        print("β model_state_dictλ¥Ό μ°Ύμ„ μ μ—†μµλ‹λ‹¤.")

if __name__ == "__main__":
    fix_pruned_checkpoint()
