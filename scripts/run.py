import cv2
import torch
import numpy as np
import os
import threading
import tempfile
import time
import psutil
import gc
import sys

print("ğŸ“¦ ëª¨ë“ˆ ë¡œë“œ ì‹œì‘...", file=sys.stderr)

try:
    from PIL import Image
    print("   âœ… PIL ë¡œë“œ", file=sys.stderr)
except ImportError as e:
    print("âŒ PIL í•„ìš”: {}".format(e), file=sys.stderr)
    sys.exit(1)

try:
    from gtts import gTTS
    print("   âœ… gtts ë¡œë“œ", file=sys.stderr)
except ImportError:
    print("   âš ï¸  gtts ë¯¸ì‚¬ìš©", file=sys.stderr)

try:
    import pygame
    print("   âœ… pygame ë¡œë“œ", file=sys.stderr)
except ImportError:
    print("   âš ï¸  pygame ë¯¸ì‚¬ìš©", file=sys.stderr)

try:
    from src.muti_modal_model.model import MobileNetCaptioningModel
    from src.utils.quantization_utils import apply_dynamic_quantization
    print("   âœ… í”„ë¡œì íŠ¸ ëª¨ë“ˆ ë¡œë“œ", file=sys.stderr)
except ImportError as e:
    print("âŒ í”„ë¡œì íŠ¸ ëª¨ë“ˆ ì˜¤ë¥˜: {}".format(e), file=sys.stderr)
    sys.exit(1)

print("âœ… ëª¨ë“  ëª¨ë“ˆ ë¡œë“œ ì™„ë£Œ", file=sys.stderr)

# ============================================================================
# í™˜ê²½ ì„¤ì • (CRITICAL - í¬ë˜ì‹œ ë°©ì§€)
# ============================================================================
print("âš™ï¸  í™˜ê²½ ì„¤ì • ì¤‘...", file=sys.stderr)

# GPU ì™„ì „ ë¹„í™œì„±í™” (CPU ì „ìš©)
os.environ['CUDA_VISIBLE_DEVICES'] = ''
torch.backends.cudnn.enabled = False
torch.backends.cudnn.benchmark = False

# CPU ìŠ¤ë ˆë“œ ì œí•œ
torch.set_num_threads(2)
torch.set_num_interop_threads(1)

# ë””ë°”ì´ìŠ¤ ì„¤ì • (ê°•ì œ CPU)
device = torch.device("cpu")
print("ğŸ“ ë””ë°”ì´ìŠ¤: CPU (GPU ë¹„í™œì„±í™”ë¨)", file=sys.stderr)

# ============================================================================
# ì´ë¯¸ì§€ ì „ì²˜ë¦¬ í•¨ìˆ˜ (torchvision ëŒ€ì²´)
# ============================================================================
def preprocess_image_manual(frame):
    """torchvision ì—†ì´ ì´ë¯¸ì§€ ì „ì²˜ë¦¬"""
    # BGR â†’ RGB
    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
    pil_image = Image.fromarray(rgb_frame)
    
    # ë¦¬ì‚¬ì´ì¦ˆ
    pil_image = pil_image.resize((224, 224), Image.BILINEAR)
    
    # numpy array
    image_array = np.array(pil_image, dtype=np.float32) / 255.0
    
    # ì •ê·œí™”
    image_array -= np.array([0.485, 0.456, 0.406], dtype=np.float32)
    image_array /= np.array([0.229, 0.224, 0.225], dtype=np.float32)
    
    # CHW í˜•ì‹
    image_array = np.transpose(image_array, (2, 0, 1))
    
    # í…ì„œë¡œ ë³€í™˜
    image_tensor = torch.from_numpy(image_array).float().unsqueeze(0)
    
    return image_tensor

preprocess_image = preprocess_image_manual

# ëª¨ë¸ ê²½ë¡œ ì„¤ì •
MODELS = {
    '1': {
        'name': 'Original Model',
        'path': 'models/lightweight_captioning_model.pth',
        'fallback': 'lightweight_captioning_model.pth'
    },
    '2': {
        'name': 'Pruned Model (Struct 30% + Mag 10%)',
        'path': 'pruning_results/Pruning_epoch_1_checkpoint.pt',
        'fallback': None
    }
}

# ì–‘ìí™” ì˜µì…˜
QUANTIZE_OPTIONS = {
    '1': {'name': 'FP32 (ì›ë³¸)', 'enabled': False},
    '2': {'name': 'FP16 (Half Precision)', 'enabled': True},
    '3': {'name': 'INT8 (Dynamic Quantization)', 'enabled': True}
}

print("âœ… í™˜ê²½ ì„¤ì • ì™„ë£Œ", file=sys.stderr)
# ============================================================================
# ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ í´ë˜ìŠ¤
# ============================================================================
class PerformanceMonitor:
    """ëª¨ë¸ ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§"""
    def __init__(self,model):
        self.inference_times = []
        self.memory_usage = []
        self.gpu_memory = []
        self.process = psutil.Process(os.getpid())
        print("ëª¨ë¸ í¬ê¸° : {:.2f} MB".format(sum(p.numel() for p in model.parameters()) * 4 / 1024 / 1024))
    def record_inference(self, inference_time):
        """ì¶”ë¡  ì‹œê°„ ê¸°ë¡"""
        self.inference_times.append(inference_time)
    
    def get_cpu_memory_mb(self):
        """CPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ (MB)"""
        try:
            mem_info = self.process.memory_info()
            return mem_info.rss / 1024 / 1024
        except:
            return 0.0
    
    def get_gpu_memory_mb(self):
        """GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ (MB)"""
        if device.type == 'cuda':
            return torch.cuda.memory_allocated() / 1024 / 1024
        elif device.type == 'mps':
            try:
                return torch.mps.current_allocated_memory() / 1024 / 1024
            except:
                return 0.0
        return 0.0
    
    def record_memory(self):
        """ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ê¸°ë¡"""
        self.memory_usage.append(self.get_cpu_memory_mb())
        self.gpu_memory.append(self.get_gpu_memory_mb())
    
    def get_stats(self):
        """í†µê³„ ê³„ì‚°"""
        if not self.inference_times:
            return None
        
        inf_times = np.array(self.inference_times[-30:])  # ìµœê·¼ 30ê°œ
        
        stats = {
            'mean_latency_ms': float(np.mean(inf_times)),
            'median_latency_ms': float(np.median(inf_times)),
            'min_latency_ms': float(np.min(inf_times)),
            'max_latency_ms': float(np.max(inf_times)),
            'std_latency_ms': float(np.std(inf_times)),
            'fps': float(1000.0 / np.mean(inf_times)),
            'cpu_memory_mb': float(np.mean(self.memory_usage[-30:]) if self.memory_usage else 0),
            'gpu_memory_mb': float(np.mean(self.gpu_memory[-30:]) if self.gpu_memory else 0),
            'total_inferences': len(self.inference_times)
        }
        return stats
    
    def print_stats(self):
        """ì„±ëŠ¥ í†µê³„ ì¶œë ¥"""
        stats = self.get_stats()
        if stats is None:
            print("ì•„ì§ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        print("\n" + "="*70)
        print("=== ì„±ëŠ¥ í†µê³„ (JTOPS ìŠ¤íƒ€ì¼) ===")
        print("="*70)
        print("â±ï¸  ì¶”ë¡  ì‹œê°„ (Latency):")
        print("    â€¢ í‰ê· : {:.2f} ms".format(stats['mean_latency_ms']))
        print("    â€¢ ì¤‘ì•™ê°’: {:.2f} ms".format(stats['median_latency_ms']))
        print("    â€¢ ìµœì†Œ/ìµœëŒ€: {:.2f} / {:.2f} ms".format(stats['min_latency_ms'], stats['max_latency_ms']))
        print("    â€¢ í‘œì¤€í¸ì°¨: {:.2f} ms".format(stats['std_latency_ms']))
        print("\nğŸ¬ ì²˜ë¦¬ ì†ë„ (Throughput):")
        print("    â€¢ FPS: {:.1f} frame/sec".format(stats['fps']))
        print("    â€¢ 1í”„ë ˆì„ ì²˜ë¦¬: {:.2f} ms".format(stats['mean_latency_ms']))
        print("\nğŸ’¾ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰:")
        print("    â€¢ CPU: {:.1f} MB".format(stats['cpu_memory_mb']))
        if device.type in ['cuda', 'mps']:
            print("    â€¢ GPU: {:.1f} MB".format(stats['gpu_memory_mb']))
        print("\nğŸ“Š ëˆ„ì  í†µê³„:")
        print("    â€¢ ì´ ì¶”ë¡  íšŸìˆ˜: {}íšŒ".format(stats['total_inferences']))
        print("="*70 + "\n")

# ============================================================================
# ëª¨ë¸ ì„ íƒ í•¨ìˆ˜
# ============================================================================
def select_model():
    """ì‚¬ìš©í•  ëª¨ë¸ ì„ íƒ"""
    print("\n" + "="*70)
    print("=== ì‚¬ìš©í•  ëª¨ë¸ ì„ íƒ ===")
    print("="*70)
    
    for key, model_info in MODELS.items():
        path = model_info['path']
        exists = os.path.exists(path)
        status = "âœ… ì‚¬ìš© ê°€ëŠ¥" if exists else "âŒ ì—†ìŒ"
        print("{}. {} {}".format(key, model_info['name'], status))
    
    print()
    while True:
        choice = input("ëª¨ë¸ì„ ì„ íƒí•˜ì„¸ìš” (1-2): ").strip()
        if choice in MODELS:
            return choice
        print("âŒ ì˜ëª»ëœ ì…ë ¥ì…ë‹ˆë‹¤. ë‹¤ì‹œ ì„ íƒí•´ì£¼ì„¸ìš”.")

# ============================================================================
# ì–‘ìí™” ì„ íƒ í•¨ìˆ˜
# ============================================================================
def select_quantization():
    """ì‚¬ìš©í•  ì–‘ìí™” ì˜µì…˜ ì„ íƒ"""
    print("\n" + "="*70)
    print("=== ì–‘ìí™” ì˜µì…˜ ì„ íƒ ===")
    print("="*70)
    
    for key, quant_info in QUANTIZE_OPTIONS.items():
        enabled = "âœ…" if quant_info['enabled'] else "âŒ"
        print("{}. {} {}".format(key, quant_info['name'], enabled))
    
    print()
    while True:
        choice = input("ì–‘ìí™” ì˜µì…˜ì„ ì„ íƒí•˜ì„¸ìš” (1-3): ").strip()
        if choice in QUANTIZE_OPTIONS:
            return choice
        print("âŒ ì˜ëª»ëœ ì…ë ¥ì…ë‹ˆë‹¤. ë‹¤ì‹œ ì„ íƒí•´ì£¼ì„¸ìš”.")

# ============================================================================
# ìŒì„± ì¶œë ¥ í•¨ìˆ˜
# ============================================================================
def speak_text_gtts(text):
    """TTS ìŒì„± ì¶œë ¥"""
    def _speak():
        temp_file = None
        try:
            pygame.mixer.init()
            tts = gTTS(text=text, lang='en', slow=False)
            temp_file = tempfile.NamedTemporaryFile(delete=False, suffix='.mp3')
            temp_filename = temp_file.name
            temp_file.close()
            
            tts.save(temp_filename)
            pygame.mixer.music.load(temp_filename)
            pygame.mixer.music.play()
            
            while pygame.mixer.music.get_busy():
                pygame.time.Clock().tick(10)
            
        except Exception as e:
            print("TTS Error: {}".format(e))
        finally:
            try:
                if temp_file and os.path.exists(temp_filename):
                    pygame.mixer.music.unload()
                    os.remove(temp_filename)
            except:
                pass
    
    thread = threading.Thread(target=_speak)
    thread.daemon = True
    thread.start()

# ============================================================================
# ëª¨ë¸ ë¡œë“œ
# ============================================================================
def load_model(model_choice):
    """í•™ìŠµëœ ìº¡ì…”ë‹ ëª¨ë¸ ë¡œë“œ"""
    model_info = MODELS[model_choice]
    model_path = model_info['path']
    
    # íŒŒì¼ ì¡´ì¬ í™•ì¸
    if not os.path.exists(model_path):
        if model_info['fallback']:
            model_path = model_info['fallback']
            if not os.path.exists(model_path):
                print("âŒ ëª¨ë¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {}".format(model_info['path']))
                return None, None, None, None
        else:
            print("âŒ ëª¨ë¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {}".format(model_path))
            return None, None, None, None
    
    try:
        print("\nğŸ“‚ ëª¨ë¸ ë¡œë“œ ì¤‘: {}".format(model_path))
        
        # CPUì—ì„œ ë¡œë“œ (ë©”ëª¨ë¦¬ ì•ˆì „) - Python/PyTorch ë²„ì „ í˜¸í™˜ì„±
        try:
            # Python 3.11+: weights_only íŒŒë¼ë¯¸í„° í•„ìš”
            checkpoint = torch.load(model_path, map_location='cpu', weights_only=False)
        except TypeError:
            # Python 3.6-3.10: weights_only íŒŒë¼ë¯¸í„° ë¯¸ì§€ì›
            checkpoint = torch.load(model_path, map_location='cpu')
        
        if isinstance(checkpoint, dict) and 'model_state_dict' in checkpoint:
            word_map = checkpoint.get('word_map')
            rev_word_map = checkpoint.get('rev_word_map')
            vocab_size = checkpoint.get('vocab_size')
            
            if word_map is None or rev_word_map is None:
                print("âŒ ë‹¨ì–´ì¥ ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤.")
                return None, None, None, None
            
            # ì²´í¬í¬ì¸íŠ¸ì—ì„œ ëª¨ë¸ í¬ê¸° ì •ë³´ ì¶”ì¶œ
            state_dict = checkpoint['model_state_dict']
            
            decoder_dim = checkpoint.get('decoder_dim', 512)
            attention_dim = checkpoint.get('attention_dim', 256)
            
            # state_dictì—ì„œ í¬ê¸° ì •ë³´ê°€ ì—†ìœ¼ë©´ ìë™ ì¶”ì¶œ
            if 'decoder.decode_step.weight_ih' in state_dict:
                decoder_dim = state_dict['decoder.decode_step.weight_ih'].shape[0] // 3
            
            if 'decoder.encoder_att.weight' in state_dict:
                attention_dim = state_dict['decoder.encoder_att.weight'].shape[0]
            
            print("   ğŸ“ ê°ì§€ëœ ëª¨ë¸ êµ¬ì¡°:")
            print("      â€¢ Decoder Dim: {}".format(decoder_dim))
            print("      â€¢ Attention Dim: {}".format(attention_dim))
            
            # ì˜¬ë°”ë¥¸ í¬ê¸°ë¡œ ëª¨ë¸ ìƒì„± (CPUì—ì„œë§Œ)
            try:
                model = MobileNetCaptioningModel(
                    vocab_size=vocab_size, 
                    embed_dim=300,
                    decoder_dim=decoder_dim,
                    attention_dim=attention_dim
                )
                model = model.to(device)
            except Exception as e:
                print("âŒ ëª¨ë¸ ìƒì„± ì‹¤íŒ¨: {}".format(e))
                return None, None, None, None
            
            # state_dict ë¡œë“œ (strict=Falseë¡œ í˜¸í™˜ë˜ëŠ” ë ˆì´ì–´ë§Œ ë¡œë“œ)
            try:
                model.load_state_dict(state_dict, strict=False)
                print("âœ… ëª¨ë¸ ìƒíƒœ ë¡œë“œ ì™„ë£Œ")
            except Exception as e:
                print("âš ï¸  ìƒíƒœ ë¡œë“œ ì¤‘ ê²½ê³ : {}".format(e))
                import traceback
                traceback.print_exc()
            
            # ë©”ëª¨ë¦¬ ì •ë¦¬
            del checkpoint, state_dict
            gc.collect()
            
            model.eval()
            
            # ëª¨ë¸ to CPU ëª…ì‹œ
            try:
                model = model.cpu()
                model.eval()
            except:
                pass
            
            model_name = model_info['name']
            
            print("\nâœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
            print("   ëª¨ë¸: {}".format(model_name))
            print("   ê²½ë¡œ: {}".format(model_path))
            
            return model, word_map, rev_word_map, model_name
        else:
            print("âŒ ì˜ëª»ëœ ëª¨ë¸ íŒŒì¼ í˜•ì‹ì…ë‹ˆë‹¤.")
            return None, None, None, None
            
    except Exception as e:
        print("âŒ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {}".format(e))
        import traceback
        traceback.print_exc()
        return None, None, None, None

# ============================================================================
# ì–‘ìí™” ì ìš© í•¨ìˆ˜
# ============================================================================
def apply_quantization(model, quant_choice, model_name):
    """ëª¨ë¸ì— ì–‘ìí™” ì ìš©"""
    quant_info = QUANTIZE_OPTIONS[quant_choice]
    quant_name = quant_info['name']
    
    if quant_choice == '1':
        # FP32 - ì–‘ìí™” ì—†ìŒ
        print("\nâœ… FP32 (ì–‘ìí™” ì—†ìŒ)")
        model = model.cpu()
        model.eval()
        return model, model_name
    
    elif quant_choice == '2':
        # FP16 - Half Precision (CPUì—ì„œëŠ” ì œí•œì )
        print("\nğŸ“Š ì–‘ìí™” ì ìš© ì¤‘: {}".format(quant_name))
        try:
            # CPUì—ì„œëŠ” FP16ì´ ì§€ì›ë˜ì§€ ì•Šìœ¼ë¯€ë¡œ FP32 ìœ ì§€
            print("âš ï¸  CPUì—ì„œëŠ” FP16ì´ ì§€ì›ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤. FP32ë¡œ ìœ ì§€í•©ë‹ˆë‹¤.")
            model = model.cpu()
            model.eval()
            return model, model_name
        except Exception as e:
            print("âš ï¸ FP16 ë³€í™˜ ì‹¤íŒ¨: {}".format(e))
            model = model.cpu()
            model.eval()
            return model, model_name
    
    elif quant_choice == '3':
        # INT8 - Dynamic Quantization
        print("\nğŸ“Š ì–‘ìí™” ì ìš© ì¤‘: {}".format(quant_name))
        try:
            # CPU ê¸°ë°˜ INT8 ì–‘ìí™” (ì•ˆì „ ë²„ì „)
            model = model.cpu()
            model.eval()
            
            # Dynamic Quantization ì ìš© (CPU ì•ˆì „)
            try:
                quantized_model = apply_dynamic_quantization(model)
                print("âœ… INT8 ì–‘ìí™” ì™„ë£Œ")
                model_name = "{} + INT8".format(model_name)
                return quantized_model, model_name
            except Exception as e2:
                print("âš ï¸  INT8 ì ìš© ì‹¤íŒ¨, FP32ë¡œ ì§„í–‰í•©ë‹ˆë‹¤: {}".format(e2))
                return model, model_name
        except Exception as e:
            print("âš ï¸ INT8 ì–‘ìí™” ì‹¤íŒ¨: {}. ì›ë³¸ ëª¨ë¸ë¡œ ê³„ì†í•©ë‹ˆë‹¤.".format(e))
            model = model.cpu()
            model.eval()
            return model, model_name
    
    model = model.cpu()
    model.eval()
    return model, model_name

# ============================================================================
# ìº¡ì…˜ ìƒì„± í•¨ìˆ˜
# ============================================================================
# ìº¡ì…˜ ìƒì„± í•¨ìˆ˜
# ============================================================================
def generate_caption_from_image(model, word_map, rev_word_map, frame):
    """ì´ë¯¸ì§€ë¡œë¶€í„° ìº¡ì…˜ ìƒì„±"""
    image_tensor = None
    try:
        # ëª¨ë¸ì„ CPUë¡œ ì´ë™
        model = model.cpu()
        model.eval()
        
        # ì´ë¯¸ì§€ ì „ì²˜ë¦¬
        image_tensor = preprocess_image(frame)
        
        # ìº¡ì…˜ ìƒì„±
        start_time = time.time()
        try:
            with torch.no_grad():
                # ë©”ëª¨ë¦¬ ì•ˆì „ì„±ì„ ìœ„í•´ ë°°ì¹˜ í¬ê¸° = 1ë¡œ ì œí•œ
                generated_words = model.generate(image_tensor, word_map, rev_word_map, max_len=50)
        except RuntimeError as e:
            print("ê²½ê³ : ì¶”ë¡  ì‹¤íŒ¨ - {}".format(e))
            gc.collect()
            return None, 0.0
        except Exception as e:
            print("ê²½ê³ : ì˜ˆìƒ ë¶ˆê°€ëŠ¥í•œ ì˜¤ë¥˜ - {}".format(e))
            import traceback
            traceback.print_exc()
            gc.collect()
            return None, 0.0
        finally:
            # ì´ë¯¸ì§€ í…ì„œ ë©”ëª¨ë¦¬ í•´ì œ
            if image_tensor is not None:
                del image_tensor
            gc.collect()
        
        inference_time = (time.time() - start_time) * 1000
        
        # í† í° ì œê±°í•˜ê³  ë¬¸ì¥ìœ¼ë¡œ ë³€í™˜
        caption = ' '.join([w for w in generated_words if w not in ['<start>', '<end>', '<pad>', '<unk>']])
        
        return caption, inference_time
    except Exception as e:
        print("ìº¡ì…˜ ìƒì„± ì˜¤ë¥˜: {}".format(e))
        import traceback
        traceback.print_exc()
        if image_tensor is not None:
            del image_tensor
        gc.collect()
        return None, 0.0

# ============================================================================
# ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜
# ============================================================================
def main():
    print("\nğŸ“Š Jetson Nano ì´ë¯¸ì§€ ìº¡ì…”ë‹ ì‹œìŠ¤í…œ")
    print("="*70)
    
    # ëª¨ë¸ ì„ íƒ
    model_choice = select_model()
    
    # ëª¨ë¸ ë¡œë“œ
    model, word_map, rev_word_map, model_name = load_model(model_choice)
    if model is None:
        print("âŒ ëª¨ë¸ì„ ë¡œë“œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    # ì–‘ìí™” ì„ íƒ ë° ì ìš©
    quant_choice = select_quantization()
    model, model_name = apply_quantization(model, quant_choice, model_name)
    
    # CPU ëª¨ë“œ ëª…ì‹œì  ì„¤ì •
    model = model.cpu()
    model.eval()
    
    # ì„±ëŠ¥ ëª¨ë‹ˆí„° ìƒì„±
    try:
        monitor = PerformanceMonitor(model)
    except Exception as e:
        print("âš ï¸  ì„±ëŠ¥ ëª¨ë‹ˆí„° ì´ˆê¸°í™” ì‹¤íŒ¨: {}".format(e))
        monitor = None

    # ì¹´ë©”ë¼ ì´ˆê¸°í™”
    print("\nğŸ“¹ ì¹´ë©”ë¼ ì´ˆê¸°í™” ì¤‘...")
    cap = cv2.VideoCapture(0)
    if not cap.isOpened():
        print("âŒ ì¹´ë©”ë¼ë¥¼ ì—´ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    # ì¹´ë©”ë¼ ì„¤ì • ìµœì í™”
    cap.set(cv2.CAP_PROP_FRAME_WIDTH, 640)
    cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 480)
    cap.set(cv2.CAP_PROP_FPS, 30)
    
    print("\n" + "="*70)
    print("=== ì´ë¯¸ì§€ ìº¡ì…”ë‹ ì‹œìŠ¤í…œ ({}) ===".format(model_name))
    print("="*70)
    print("\nâŒ¨ï¸  í‚¤ë³´ë“œ ëª…ë ¹ì–´:")
    print("  's' : í˜„ì¬ í”„ë ˆì„ì—ì„œ ìº¡ì…˜ ìƒì„± ë° ìŒì„± ì¶œë ¥")
    print("  'r' : ë§ˆì§€ë§‰ ìº¡ì…˜ ë‹¤ì‹œ ë“£ê¸°")
    print("  'p' : ì„±ëŠ¥ í†µê³„ ì¶œë ¥")
    print("  'm' : ëª¨ë¸ ë³€ê²½")
    print("  'q' : ì¢…ë£Œ\n")
    
    last_caption = None
    is_processing = False
    current_model_name = model_name
    frame_count = 0
    
    while True:
        ret, frame = cap.read()
        if not ret:
            print("ì¹´ë©”ë¼ ì½ê¸° ì‹¤íŒ¨")
            break
        
        # ë©”ëª¨ë¦¬ ê¸°ë¡
        if monitor:
            monitor.record_memory()
        
        # ë©”ëª¨ë¦¬ ëª¨ë‹ˆí„°ë§ (5í”„ë ˆì„ë§ˆë‹¤)
        frame_count += 1
        if frame_count % 5 == 0:
            if monitor:
                current_mem = monitor.get_cpu_memory_mb()
                if current_mem > 2500:  # Jetson Nano 4GB ê¸°ì¤€
                    print("âš ï¸  ë†’ì€ ë©”ëª¨ë¦¬ ì‚¬ìš©: {:.0f}MB - ì •ë¦¬ ì¤‘...".format(current_mem))
                    gc.collect()
        
        # ì²˜ë¦¬ ì¤‘ í‘œì‹œ
        if is_processing:
            cv2.putText(frame, "Processing...", (10, 30),
                       cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 165, 255), 3)
        
        # ëª¨ë¸ ì •ë³´ í‘œì‹œ
        cv2.rectangle(frame, (5, frame.shape[0] - 75), (550, frame.shape[0] - 5), (50, 50, 50), -1)
        cv2.putText(frame, "Model: {}".format(current_model_name[:40]), (10, frame.shape[0] - 52),
                   cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 255), 2)
        
        # ì„±ëŠ¥ ì§€í‘œ í‘œì‹œ
        stats = monitor.get_stats()
        if stats:
            fps_text = "FPS: {:.1f}".format(stats['fps'])
            latency_text = "Latency: {:.1f}ms".format(stats['mean_latency_ms'])
            mem_text = "CPU: {:.0f}MB".format(stats['cpu_memory_mb'])
            
            cv2.putText(frame, fps_text, (10, frame.shape[0] - 32),
                       cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)
            cv2.putText(frame, latency_text, (10, frame.shape[0] - 12),
                       cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)
            cv2.putText(frame, mem_text, (frame.shape[1] - 250, frame.shape[0] - 32),
                       cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)
            
            if device.type in ['cuda', 'mps']:
                gpu_text = "GPU: {:.0f}MB".format(stats['gpu_memory_mb'])
                cv2.putText(frame, gpu_text, (frame.shape[1] - 250, frame.shape[0] - 12),
                           cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)
        
        # ë§ˆì§€ë§‰ ìº¡ì…˜ í‘œì‹œ
        if last_caption and not is_processing:
            caption_y = 60
            max_width = frame.shape[1] - 20
            words = last_caption.split()
            line = ""
            line_num = 0
            
            for word in words:
                test_line = line + word + " "
                text_size = cv2.getTextSize(test_line, cv2.FONT_HERSHEY_SIMPLEX, 0.5, 1)[0]
                
                if text_size[0] > max_width:
                    text_size_actual = cv2.getTextSize(line, cv2.FONT_HERSHEY_SIMPLEX, 0.5, 1)[0]
                    cv2.rectangle(frame, (5, caption_y + line_num * 25 - 20), 
                                (15 + text_size_actual[0], caption_y + line_num * 25 + 5), 
                                (0, 0, 0), -1)
                    cv2.putText(frame, line, (10, caption_y + line_num * 25),
                               cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 200, 0), 2)
                    line = word + " "
                    line_num += 1
                else:
                    line = test_line
            
            if line:
                text_size_actual = cv2.getTextSize(line, cv2.FONT_HERSHEY_SIMPLEX, 0.5, 1)[0]
                cv2.rectangle(frame, (5, caption_y + line_num * 25 - 20), 
                            (15 + text_size_actual[0], caption_y + line_num * 25 + 5), 
                            (0, 0, 0), -1)
                cv2.putText(frame, line, (10, caption_y + line_num * 25),
                           cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 200, 0), 2)
        
        cv2.imshow('Image Captioning', frame)
        
        # í‚¤ ì…ë ¥ ì²˜ë¦¬
        key = cv2.waitKey(1) & 0xFF
        
        if key == ord('q'):
            print("\nì¢…ë£Œ")
            break
            
        elif key == ord('s') and not is_processing:
            is_processing = True
            print("\n" + "="*70)
            print("ìº¡ì…˜ ìƒì„± ì¤‘...")
            
            caption, inf_time = generate_caption_from_image(model, word_map, rev_word_map, frame)
            monitor.record_inference(inf_time)
            
            if caption:
                last_caption = caption
                print("\nìƒì„±ëœ ìº¡ì…˜: {}".format(caption))
                print("ì¶”ë¡  ì‹œê°„: {:.2f}ms".format(inf_time))
                
                # ìº¡ì…˜ ìŒì„± ì¶œë ¥
                speak_text_gtts(caption)
            else:
                print("ìº¡ì…˜ ìƒì„± ì‹¤íŒ¨")
            
            print("="*70 + "\n")
            is_processing = False
            
        elif key == ord('r') and last_caption:
            print("\nğŸ”Š ë§ˆì§€ë§‰ ìº¡ì…˜: \"{}\"".format(last_caption))
            speak_text_gtts(last_caption)
            
        elif key == ord('p'):
            monitor.print_stats()
            
        elif key == ord('m'):
            print("\nëª¨ë¸ì„ ë³€ê²½í•©ë‹ˆë‹¤...")
            cap.release()
            cv2.destroyAllWindows()
            
            model_choice = select_model()
            model, word_map, rev_word_map, model_name = load_model(model_choice)
            
            if model is None:
                print("âŒ ëª¨ë¸ì„ ë¡œë“œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                return
            
            # ì–‘ìí™” ì„ íƒ ë° ì ìš©
            quant_choice = select_quantization()
            model, model_name = apply_quantization(model, quant_choice, model_name)
            
            current_model_name = model_name
            last_caption = None
            monitor = PerformanceMonitor(model)  # ìƒˆ ëª¨ë‹ˆí„° ìƒì„±
            
            cap = cv2.VideoCapture(0)
            if not cap.isOpened():
                print("âŒ ì¹´ë©”ë¼ë¥¼ ì—´ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                return
            
            print("\nâœ… {} ëª¨ë¸ë¡œ ë³€ê²½ë˜ì—ˆìŠµë‹ˆë‹¤.\n".format(model_name))
    
    cap.release()
    cv2.destroyAllWindows()

if __name__ == "__main__":
    main()