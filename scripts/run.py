import cv2
import torch
import numpy as np
import os
import threading
import tempfile
import time
import psutil
from PIL import Image
from torchvision import transforms
from gtts import gTTS
import pygame
from src.muti_modal_model.model import MobileNetCaptioningModel
from src.utils.quantization_utils import apply_dynamic_quantization

# ============================================================================
# í™˜ê²½ ì„¤ì •
# ============================================================================
# ë””ë°”ì´ìŠ¤ ì„ íƒ
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print("ë””ë°”ì´ìŠ¤: {}".format(device))

# ëª¨ë¸ ê²½ë¡œ ì„¤ì •
MODELS = {
    '1': {
        'name': 'Original Model',
        'path': 'models/lightweight_captioning_model.pth',
        'fallback': 'lightweight_captioning_model.pth'
    },
    '2': {
        'name': 'Pruned Model (Struct 30% + Mag 10%)',
        'path': 'pruning_results/Pruning_epoch_1_checkpoint.pt',
        'fallback': None
    }
}

# ì´ë¯¸ì§€ ì „ì²˜ë¦¬
transform = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406], 
                       std=[0.229, 0.224, 0.225])
])

# ============================================================================
# ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ í´ë˜ìŠ¤
# ============================================================================
class PerformanceMonitor:
    """ëª¨ë¸ ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§"""
    def __init__(self):
        self.inference_times = []
        self.memory_usage = []
        self.gpu_memory = []
        self.process = psutil.Process(os.getpid())
    
    def record_inference(self, inference_time):
        """ì¶”ë¡  ì‹œê°„ ê¸°ë¡"""
        self.inference_times.append(inference_time)
    
    def get_cpu_memory_mb(self):
        """CPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ (MB)"""
        try:
            mem_info = self.process.memory_info()
            return mem_info.rss / 1024 / 1024
        except:
            return 0.0
    
    def get_gpu_memory_mb(self):
        """GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ (MB)"""
        if device.type == 'cuda':
            return torch.cuda.memory_allocated() / 1024 / 1024
        elif device.type == 'mps':
            try:
                return torch.mps.current_allocated_memory() / 1024 / 1024
            except:
                return 0.0
        return 0.0
    
    def record_memory(self):
        """ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ê¸°ë¡"""
        self.memory_usage.append(self.get_cpu_memory_mb())
        self.gpu_memory.append(self.get_gpu_memory_mb())
    
    def get_stats(self):
        """í†µê³„ ê³„ì‚°"""
        if not self.inference_times:
            return None
        
        inf_times = np.array(self.inference_times[-30:])  # ìµœê·¼ 30ê°œ
        
        stats = {
            'mean_latency_ms': float(np.mean(inf_times)),
            'median_latency_ms': float(np.median(inf_times)),
            'min_latency_ms': float(np.min(inf_times)),
            'max_latency_ms': float(np.max(inf_times)),
            'std_latency_ms': float(np.std(inf_times)),
            'fps': float(1000.0 / np.mean(inf_times)),
            'cpu_memory_mb': float(np.mean(self.memory_usage[-30:]) if self.memory_usage else 0),
            'gpu_memory_mb': float(np.mean(self.gpu_memory[-30:]) if self.gpu_memory else 0),
            'total_inferences': len(self.inference_times)
        }
        return stats
    
    def print_stats(self):
        """ì„±ëŠ¥ í†µê³„ ì¶œë ¥"""
        stats = self.get_stats()
        if stats is None:
            print("ì•„ì§ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        print("\n" + "="*70)
        print("=== ì„±ëŠ¥ í†µê³„ (JTOPS ìŠ¤íƒ€ì¼) ===")
        print("="*70)
        print(f"â±ï¸  ì¶”ë¡  ì‹œê°„ (Latency):")
        print(f"    â€¢ í‰ê· : {stats['mean_latency_ms']:.2f} ms")
        print(f"    â€¢ ì¤‘ì•™ê°’: {stats['median_latency_ms']:.2f} ms")
        print(f"    â€¢ ìµœì†Œ/ìµœëŒ€: {stats['min_latency_ms']:.2f} / {stats['max_latency_ms']:.2f} ms")
        print(f"    â€¢ í‘œì¤€í¸ì°¨: {stats['std_latency_ms']:.2f} ms")
        print(f"\nğŸ¬ ì²˜ë¦¬ ì†ë„ (Throughput):")
        print(f"    â€¢ FPS: {stats['fps']:.1f} frame/sec")
        print(f"    â€¢ 1í”„ë ˆì„ ì²˜ë¦¬: {stats['mean_latency_ms']:.2f} ms")
        print(f"\nğŸ’¾ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰:")
        print(f"    â€¢ CPU: {stats['cpu_memory_mb']:.1f} MB")
        if device.type in ['cuda', 'mps']:
            print(f"    â€¢ GPU: {stats['gpu_memory_mb']:.1f} MB")
        print(f"\nğŸ“Š ëˆ„ì  í†µê³„:")
        print(f"    â€¢ ì´ ì¶”ë¡  íšŸìˆ˜: {stats['total_inferences']}íšŒ")
        print("="*70 + "\n")

# ============================================================================
# ëª¨ë¸ ì„ íƒ í•¨ìˆ˜
# ============================================================================
def select_model():
    """ì‚¬ìš©í•  ëª¨ë¸ ì„ íƒ"""
    print("\n" + "="*70)
    print("=== ì‚¬ìš©í•  ëª¨ë¸ ì„ íƒ ===")
    print("="*70)
    
    for key, model_info in MODELS.items():
        path = model_info['path']
        exists = os.path.exists(path)
        status = "âœ… ì‚¬ìš© ê°€ëŠ¥" if exists else "âŒ ì—†ìŒ"
        print("{}. {} {}".format(key, model_info['name'], status))
    
    print()
    while True:
        choice = input("ëª¨ë¸ì„ ì„ íƒí•˜ì„¸ìš” (1-2): ").strip()
        if choice in MODELS:
            return choice
        print("âŒ ì˜ëª»ëœ ì…ë ¥ì…ë‹ˆë‹¤. ë‹¤ì‹œ ì„ íƒí•´ì£¼ì„¸ìš”.")

# ============================================================================
# ìŒì„± ì¶œë ¥ í•¨ìˆ˜
# ============================================================================
def speak_text_gtts(text):
    """TTS ìŒì„± ì¶œë ¥"""
    def _speak():
        temp_file = None
        try:
            pygame.mixer.init()
            tts = gTTS(text=text, lang='en', slow=False)
            temp_file = tempfile.NamedTemporaryFile(delete=False, suffix='.mp3')
            temp_filename = temp_file.name
            temp_file.close()
            
            tts.save(temp_filename)
            pygame.mixer.music.load(temp_filename)
            pygame.mixer.music.play()
            
            while pygame.mixer.music.get_busy():
                pygame.time.Clock().tick(10)
            
        except Exception as e:
            print("TTS Error: {}".format(e))
        finally:
            try:
                if temp_file and os.path.exists(temp_filename):
                    pygame.mixer.music.unload()
                    os.remove(temp_filename)
            except:
                pass
    
    thread = threading.Thread(target=_speak)
    thread.daemon = True
    thread.start()

# ============================================================================
# ëª¨ë¸ ë¡œë“œ
# ============================================================================
def load_model(model_choice):
    """í•™ìŠµëœ ìº¡ì…”ë‹ ëª¨ë¸ ë¡œë“œ"""
    model_info = MODELS[model_choice]
    model_path = model_info['path']
    
    # íŒŒì¼ ì¡´ì¬ í™•ì¸
    if not os.path.exists(model_path):
        if model_info['fallback']:
            model_path = model_info['fallback']
            if not os.path.exists(model_path):
                print(f"âŒ ëª¨ë¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {model_info['path']}")
                return None, None, None, None
        else:
            print("âŒ ëª¨ë¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {}".format(model_path))
            return None, None, None, None
    
    try:
        print("\nğŸ“‚ ëª¨ë¸ ë¡œë“œ ì¤‘: {}".format(model_path))
        checkpoint = torch.load(model_path, map_location=device, weights_only=False)
        
        if isinstance(checkpoint, dict) and 'model_state_dict' in checkpoint:
            word_map = checkpoint.get('word_map')
            rev_word_map = checkpoint.get('rev_word_map')
            vocab_size = checkpoint.get('vocab_size')
            
            if word_map is None or rev_word_map is None:
                print("âŒ ë‹¨ì–´ì¥ ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤.")
                return None, None, None, None
            
            # ì²´í¬í¬ì¸íŠ¸ì—ì„œ ëª¨ë¸ í¬ê¸° ì •ë³´ ì¶”ì¶œ
            state_dict = checkpoint['model_state_dict']
            
            decoder_dim = checkpoint.get('decoder_dim', 512)
            attention_dim = checkpoint.get('attention_dim', 256)
            
            # state_dictì—ì„œ í¬ê¸° ì •ë³´ê°€ ì—†ìœ¼ë©´ ìë™ ì¶”ì¶œ
            if 'decoder.decode_step.weight_ih' in state_dict:
                decoder_dim = state_dict['decoder.decode_step.weight_ih'].shape[0] // 3
            
            if 'decoder.encoder_att.weight' in state_dict:
                attention_dim = state_dict['decoder.encoder_att.weight'].shape[0]
            
            print(f"   ğŸ“ ê°ì§€ëœ ëª¨ë¸ êµ¬ì¡°:")
            print("      â€¢ Decoder Dim: {}".format(decoder_dim))
            print("      â€¢ Attention Dim: {}".format(attention_dim))
            
            # ì˜¬ë°”ë¥¸ í¬ê¸°ë¡œ ëª¨ë¸ ìƒì„±
            model = MobileNetCaptioningModel(
                vocab_size=vocab_size, 
                embed_dim=300,
                decoder_dim=decoder_dim,
                attention_dim=attention_dim
            ).to(device)
            
            # state_dict ë¡œë“œ (strict=Falseë¡œ í˜¸í™˜ë˜ëŠ” ë ˆì´ì–´ë§Œ ë¡œë“œ)
            try:
                model.load_state_dict(state_dict, strict=False)
                print(f"âœ… ëª¨ë¸ ìƒíƒœ ë¡œë“œ ì™„ë£Œ")
            except Exception as e:
                print("âš ï¸  ìƒíƒœ ë¡œë“œ ì¤‘ ê²½ê³ : {}".format(e))
            
            model.eval()
            
            model_name = model_info['name']
            
            # ëª¨ë¸ í¬ê¸° ê³„ì‚°
            param_size = sum(p.numel() * p.element_size() for p in model.parameters()) / 1024 / 1024
            buffer_size = sum(b.numel() * b.element_size() for b in model.buffers()) / 1024 / 1024
            total_params = sum(p.numel() for p in model.parameters())
            
            print(f"\nâœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
            print("   ëª¨ë¸: {}".format(model_name))
            print("   ë‹¨ì–´ì¥ í¬ê¸°: {}".format(vocab_size))
            print("   ì´ íŒŒë¼ë¯¸í„°: {}".format(total_params:,))
            print("   ëª¨ë¸ í¬ê¸°: {} MB".format(param_size + buffer_size:.2f))
            print("   ê²½ë¡œ: {}".format(model_path))
            
            return model, word_map, rev_word_map, model_name
        else:
            print("âŒ ì˜ëª»ëœ ëª¨ë¸ íŒŒì¼ í˜•ì‹ì…ë‹ˆë‹¤.")
            return None, None, None, None
            
    except Exception as e:
        print("âŒ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {}".format(e))
        import traceback
        traceback.print_exc()
        return None, None, None, None

# ============================================================================
# ìº¡ì…˜ ìƒì„± í•¨ìˆ˜
# ============================================================================
def generate_caption_from_image(model, word_map, rev_word_map, frame):
    """ì´ë¯¸ì§€ë¡œë¶€í„° ìº¡ì…˜ ìƒì„±"""
    try:
        # OpenCV BGRì„ RGBë¡œ ë³€í™˜
        rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
        pil_image = Image.fromarray(rgb_frame)
        
        # ì „ì²˜ë¦¬
        image_tensor = transform(pil_image).unsqueeze(0).to(device)
        
        # ìº¡ì…˜ ìƒì„±
        start_time = time.time()
        with torch.no_grad():
            generated_words = model.generate(image_tensor, word_map, rev_word_map, max_len=50)
        
        inference_time = (time.time() - start_time) * 1000
        
        # í† í° ì œê±°í•˜ê³  ë¬¸ì¥ìœ¼ë¡œ ë³€í™˜
        caption = ' '.join([w for w in generated_words if w not in ['<start>', '<end>', '<pad>', '<unk>']])
        
        return caption, inference_time
    except Exception as e:
        print("ìº¡ì…˜ ìƒì„± ì˜¤ë¥˜: {}".format(e))
        return None, 0.0

# ============================================================================
# ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜
# ============================================================================
def main():
    # ì„±ëŠ¥ ëª¨ë‹ˆí„° ìƒì„±
    monitor = PerformanceMonitor()
    
    # ëª¨ë¸ ì„ íƒ
    model_choice = select_model()
    
    # ëª¨ë¸ ë¡œë“œ
    model, word_map, rev_word_map, model_name = load_model(model_choice)
    if model is None:
        print("âŒ ëª¨ë¸ì„ ë¡œë“œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    # ì¹´ë©”ë¼ ì´ˆê¸°í™”
    cap = cv2.VideoCapture(0)
    if not cap.isOpened():
        print("âŒ ì¹´ë©”ë¼ë¥¼ ì—´ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    print("\n" + "="*70)
    print("=== ì´ë¯¸ì§€ ìº¡ì…”ë‹ ì‹¤ì‹œê°„ ì‹¤í–‰ ({}) ===".format(model_name))
    print("="*70)
    print("\nâŒ¨ï¸  í‚¤ë³´ë“œ ëª…ë ¹ì–´:")
    print("  's' : í˜„ì¬ í”„ë ˆì„ì—ì„œ ìº¡ì…˜ ìƒì„± ë° ìŒì„± ì¶œë ¥")
    print("  'r' : ë§ˆì§€ë§‰ ìº¡ì…˜ ë‹¤ì‹œ ë“£ê¸°")
    print("  'p' : ì„±ëŠ¥ í†µê³„ ì¶œë ¥ (JTOPS ìŠ¤íƒ€ì¼)")
    print("  'm' : ëª¨ë¸ ë³€ê²½")
    print("  'q' : ì¢…ë£Œ\n")
    
    last_caption = None
    is_processing = False
    current_model_name = model_name
    
    while True:
        ret, frame = cap.read()
        if not ret:
            print("ì¹´ë©”ë¼ ì½ê¸° ì‹¤íŒ¨")
            break
        
        # ë©”ëª¨ë¦¬ ê¸°ë¡
        monitor.record_memory()
        
        # ì²˜ë¦¬ ì¤‘ í‘œì‹œ
        if is_processing:
            cv2.putText(frame, "Processing...", (10, 30),
                       cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 165, 255), 3)
        
        # ëª¨ë¸ ì •ë³´ í‘œì‹œ
        cv2.rectangle(frame, (5, frame.shape[0] - 75), (550, frame.shape[0] - 5), (50, 50, 50), -1)
        cv2.putText(frame, "Model: {}".format(current_model_name[:40]), (10, frame.shape[0] - 52),
                   cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 255), 2)
        
        # ì„±ëŠ¥ ì§€í‘œ í‘œì‹œ
        stats = monitor.get_stats()
        if stats:
            fps_text = f"FPS: {stats['fps']:.1f}"
            latency_text = f"Latency: {stats['mean_latency_ms']:.1f}ms"
            mem_text = f"CPU: {stats['cpu_memory_mb']:.0f}MB"
            
            cv2.putText(frame, fps_text, (10, frame.shape[0] - 32),
                       cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)
            cv2.putText(frame, latency_text, (10, frame.shape[0] - 12),
                       cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)
            cv2.putText(frame, mem_text, (frame.shape[1] - 250, frame.shape[0] - 32),
                       cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)
            
            if device.type in ['cuda', 'mps']:
                gpu_text = f"GPU: {stats['gpu_memory_mb']:.0f}MB"
                cv2.putText(frame, gpu_text, (frame.shape[1] - 250, frame.shape[0] - 12),
                           cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)
        
        # ë§ˆì§€ë§‰ ìº¡ì…˜ í‘œì‹œ
        if last_caption and not is_processing:
            caption_y = 60
            max_width = frame.shape[1] - 20
            words = last_caption.split()
            line = ""
            line_num = 0
            
            for word in words:
                test_line = line + word + " "
                text_size = cv2.getTextSize(test_line, cv2.FONT_HERSHEY_SIMPLEX, 0.5, 1)[0]
                
                if text_size[0] > max_width:
                    text_size_actual = cv2.getTextSize(line, cv2.FONT_HERSHEY_SIMPLEX, 0.5, 1)[0]
                    cv2.rectangle(frame, (5, caption_y + line_num * 25 - 20), 
                                (15 + text_size_actual[0], caption_y + line_num * 25 + 5), 
                                (0, 0, 0), -1)
                    cv2.putText(frame, line, (10, caption_y + line_num * 25),
                               cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 200, 0), 2)
                    line = word + " "
                    line_num += 1
                else:
                    line = test_line
            
            if line:
                text_size_actual = cv2.getTextSize(line, cv2.FONT_HERSHEY_SIMPLEX, 0.5, 1)[0]
                cv2.rectangle(frame, (5, caption_y + line_num * 25 - 20), 
                            (15 + text_size_actual[0], caption_y + line_num * 25 + 5), 
                            (0, 0, 0), -1)
                cv2.putText(frame, line, (10, caption_y + line_num * 25),
                           cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 200, 0), 2)
        
        cv2.imshow('Image Captioning', frame)
        
        # í‚¤ ì…ë ¥ ì²˜ë¦¬
        key = cv2.waitKey(1) & 0xFF
        
        if key == ord('q'):
            print("\nì¢…ë£Œ")
            break
            
        elif key == ord('s') and not is_processing:
            is_processing = True
            print("\n" + "="*70)
            print("ìº¡ì…˜ ìƒì„± ì¤‘...")
            
            caption, inf_time = generate_caption_from_image(model, word_map, rev_word_map, frame)
            monitor.record_inference(inf_time)
            
            if caption:
                last_caption = caption
                print("\nìƒì„±ëœ ìº¡ì…˜: {}".format(caption))
                print("ì¶”ë¡  ì‹œê°„: {}ms".format(inf_time:.2f))
                
                # ìº¡ì…˜ ìŒì„± ì¶œë ¥
                speak_text_gtts(caption)
            else:
                print("ìº¡ì…˜ ìƒì„± ì‹¤íŒ¨")
            
            print("="*70 + "\n")
            is_processing = False
            
        elif key == ord('r') and last_caption:
            print("\nğŸ”Š ë§ˆì§€ë§‰ ìº¡ì…˜: \"{}\"".format(last_caption))
            speak_text_gtts(last_caption)
            
        elif key == ord('p'):
            monitor.print_stats()
            
        elif key == ord('m'):
            print("\nëª¨ë¸ì„ ë³€ê²½í•©ë‹ˆë‹¤...")
            cap.release()
            cv2.destroyAllWindows()
            
            model_choice = select_model()
            model, word_map, rev_word_map, model_name = load_model(model_choice)
            
            if model is None:
                print("âŒ ëª¨ë¸ì„ ë¡œë“œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                return
            
            current_model_name = model_name
            last_caption = None
            monitor = PerformanceMonitor()  # ìƒˆ ëª¨ë‹ˆí„° ìƒì„±
            
            cap = cv2.VideoCapture(0)
            if not cap.isOpened():
                print("âŒ ì¹´ë©”ë¼ë¥¼ ì—´ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                return
            
            print("\nâœ… {} ëª¨ë¸ë¡œ ë³€ê²½ë˜ì—ˆìŠµë‹ˆë‹¤.\n".format(model_name))
    
    cap.release()
    cv2.destroyAllWindows()

if __name__ == "__main__":
    main()