import cv2
import torch
import numpy as np
import os
import threading
import tempfile
import time
import psutil
import gc
import sys

print("ğŸ“¦ ëª¨ë“ˆ ë¡œë“œ ì‹œì‘...", file=sys.stderr)

try:
    from PIL import Image
    print("   âœ… PIL ë¡œë“œ", file=sys.stderr)
except ImportError as e:
    print("âŒ PIL í•„ìš”: {}".format(e), file=sys.stderr)
    sys.exit(1)

try:
    from gtts import gTTS
    print("   âœ… gtts ë¡œë“œ", file=sys.stderr)
except ImportError:
    print("   âš ï¸  gtts ë¯¸ì‚¬ìš©", file=sys.stderr)

try:
    import pygame
    print("   âœ… pygame ë¡œë“œ", file=sys.stderr)
except ImportError:
    print("   âš ï¸  pygame ë¯¸ì‚¬ìš©", file=sys.stderr)

# í”„ë¡œì íŠ¸ ëª¨ë“ˆ ì§€ì—° ë¡œë“œ
print("   â„¹ï¸  í”„ë¡œì íŠ¸ ëª¨ë“ˆ (ì§€ì—° ë¡œë“œ ì¤€ë¹„)", file=sys.stderr)

# ì§€ì—° ë¡œë” import (ë§¤ìš° ê°„ë‹¨í•¨)
from src.utils.memory_safe_import import load_model_class
print("   âœ… ì§€ì—° ë¡œë” ë¡œë“œ", file=sys.stderr)

# ì•„ì§ ì‹¤ì œ ë¡œë“œëŠ” ì•ˆ ë¨
_model_class_loader = load_model_class
    


print("âœ… ëª¨ë“  ëª¨ë“ˆ ë¡œë“œ ì™„ë£Œ", file=sys.stderr)

# ============================================================================
# í™˜ê²½ ì„¤ì • (CRITICAL - í¬ë˜ì‹œ ë°©ì§€)
# ============================================================================
print("âš™ï¸  í™˜ê²½ ì„¤ì • ì¤‘...", file=sys.stderr)
torch.backends.cudnn.enabled = False  # ë¶ˆì•ˆì •ì„± ë°©ì§€
torch.backends.cudnn.benchmark = True # ì…ë ¥ í¬ê¸°ê°€ ê³ ì •(224x224)ì´ë¯€ë¡œ í•„ìˆ˜

# CPU/GPU ë””ë°”ì´ìŠ¤ ìë™ ê°ì§€ ë° ê°•ì œ ì„¤ì •
if torch.cuda.is_available():
    device = torch.device("cuda")
    print("ğŸš€ ë””ë°”ì´ìŠ¤: GPU (NVIDIA Maxwell) ê°€ì† ëª¨ë“œ", file=sys.stderr)
else:
    device = torch.device("cpu")
    print("ğŸ“ ë””ë°”ì´ìŠ¤: CPU (ê²½ê³ : ì„±ëŠ¥ì´ ë‚®ì„ ìˆ˜ ìˆìŒ)", file=sys.stderr)

# ìŠ¤ë ˆë“œ ìµœì í™”
torch.set_num_threads(4)
torch.set_num_interop_threads(4)

sys.modules['numpy._core'] = np.core
sys.modules['numpy._core.multiarray'] = np.core.multiarray
dtypes = torch.float32
# ============================================================================
# ì´ë¯¸ì§€ ì „ì²˜ë¦¬ í•¨ìˆ˜ (torchvision ëŒ€ì²´)
# ============================================================================
def preprocess_image_optimized(frame):
    """
    Jetson Nano ìµœì í™” ì „ì²˜ë¦¬:
    1. PIL ì œê±° (ëŠë¦¼) -> OpenCV ì‚¬ìš© (ë¹ ë¦„)
    2. CPU ì—°ì‚° ìµœì†Œí™” -> GPUë¡œ ë°”ë¡œ ì—…ë¡œë“œ
    """
    # 1. OpenCV ë¦¬ì‚¬ì´ì¦ˆ (CPU ë¶€í•˜ ê°ì†Œ)
    img = cv2.resize(frame, (224, 224), interpolation=cv2.INTER_LINEAR)
    
    # 2. BGR -> RGB ë° ì •ê·œí™” (Numpy)
    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB).astype(np.float32) / 255.0
    
    # 3. ì •ê·œí™” (Mean/Std)
    img -= np.array([0.485, 0.456, 0.406], dtype=np.float32)
    img /= np.array([0.229, 0.224, 0.225], dtype=np.float32)
    
    # 4. (H, W, C) -> (C, H, W)
    img = np.transpose(img, (2, 0, 1))
    
    # 5. Tensor ë³€í™˜ ë° GPU ì—…ë¡œë“œ
    image_tensor = torch.from_numpy(img).unsqueeze(0).to(device)
    
    # â˜… í•µì‹¬: GPU ëª¨ë“œì¼ ê²½ìš° Half Precision(FP16) ì ìš©
    return image_tensor.float()

preprocess_image = preprocess_image_optimized

# ëª¨ë¸ ê²½ë¡œ ì„¤ì •
MODELS = {
    '1': {
        'name': 'Original Model',
        'path': 'models/lightweight_captioning_model.pth',
        'fallback': 'lightweight_captioning_model.pth'
    },
    '2': {
        'name': 'Pruned Model (Struct 30% + Mag 10%)',
        'path': 'pruning_results/Pruning_epoch_1_checkpoint.pt',
        'fallback': None
    }
}

# ì–‘ìí™” ì˜µì…˜
QUANTIZE_OPTIONS = {
    '1': {'name': 'FP32 (ì›ë³¸)', 'enabled': False},
    '2': {'name': 'FP16 (Half Precision)', 'enabled': True},
}

print("âœ… í™˜ê²½ ì„¤ì • ì™„ë£Œ", file=sys.stderr)
# ============================================================================
# ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ í´ë˜ìŠ¤
# ============================================================================
class PerformanceMonitor:
    """ëª¨ë¸ ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§"""
    def __init__(self,model):
        self.inference_times = []
        self.memory_usage = []
        self.gpu_memory = []
        self.process = psutil.Process(os.getpid())
        print("ëª¨ë¸ í¬ê¸° : {:.2f} MB".format(sum(p.numel() for p in model.parameters()) * 4 / 1024 / 1024))
    def record_inference(self, inference_time):
        """ì¶”ë¡  ì‹œê°„ ê¸°ë¡"""
        self.inference_times.append(inference_time)
    
    def get_cpu_memory_mb(self):
        """CPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ (MB)"""
        try:
            mem_info = self.process.memory_info()
            return mem_info.rss / 1024 / 1024
        except:
            return 0.0
    
    def get_gpu_memory_mb(self):
        """GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ (MB)"""
        if device.type == 'cuda':
            return torch.cuda.memory_allocated() / 1024 / 1024
        elif device.type == 'mps':
            try:
                return torch.mps.current_allocated_memory() / 1024 / 1024
            except:
                return 0.0
        return 0.0
    
    def record_memory(self):
        """ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ê¸°ë¡"""
        self.memory_usage.append(self.get_cpu_memory_mb())
        self.gpu_memory.append(self.get_gpu_memory_mb())
    
    def get_stats(self):
        """í†µê³„ ê³„ì‚°"""
        if not self.inference_times:
            return None
        
        inf_times = np.array(self.inference_times[-30:])  # ìµœê·¼ 30ê°œ
        
        stats = {
            'mean_latency_ms': float(np.mean(inf_times)),
            'median_latency_ms': float(np.median(inf_times)),
            'min_latency_ms': float(np.min(inf_times)),
            'max_latency_ms': float(np.max(inf_times)),
            'std_latency_ms': float(np.std(inf_times)),
            'fps': float(1000.0 / np.mean(inf_times)),
            'cpu_memory_mb': float(np.mean(self.memory_usage[-30:]) if self.memory_usage else 0),
            'gpu_memory_mb': float(np.mean(self.gpu_memory[-30:]) if self.gpu_memory else 0),
            'total_inferences': len(self.inference_times)
        }
        return stats
    
    def print_stats(self):
        """ì„±ëŠ¥ í†µê³„ ì¶œë ¥"""
        stats = self.get_stats()
        if stats is None:
            print("ì•„ì§ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        print("\n" + "="*70)
        print("=== ì„±ëŠ¥ í†µê³„ (JTOPS ìŠ¤íƒ€ì¼) ===")
        print("="*70)
        print("â±ï¸  ì¶”ë¡  ì‹œê°„ (Latency):")
        print("    â€¢ í‰ê· : {:.2f} ms".format(stats['mean_latency_ms']))
        print("    â€¢ ì¤‘ì•™ê°’: {:.2f} ms".format(stats['median_latency_ms']))
        print("    â€¢ ìµœì†Œ/ìµœëŒ€: {:.2f} / {:.2f} ms".format(stats['min_latency_ms'], stats['max_latency_ms']))
        print("    â€¢ í‘œì¤€í¸ì°¨: {:.2f} ms".format(stats['std_latency_ms']))
        print("\nğŸ¬ ì²˜ë¦¬ ì†ë„ (Throughput):")
        print("    â€¢ FPS: {:.1f} frame/sec".format(stats['fps']))
        print("    â€¢ 1í”„ë ˆì„ ì²˜ë¦¬: {:.2f} ms".format(stats['mean_latency_ms']))
        print("\nğŸ’¾ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰:")
        print("    â€¢ CPU: {:.1f} MB".format(stats['cpu_memory_mb']))
        if device.type in ['cuda', 'mps']:
            print("    â€¢ GPU: {:.1f} MB".format(stats['gpu_memory_mb']))
        print("\nğŸ“Š ëˆ„ì  í†µê³„:")
        print("    â€¢ ì´ ì¶”ë¡  íšŸìˆ˜: {}íšŒ".format(stats['total_inferences']))
        print("="*70 + "\n")

# ============================================================================
# ëª¨ë¸ ì„ íƒ í•¨ìˆ˜
# ============================================================================
def select_model():
    """ì‚¬ìš©í•  ëª¨ë¸ ì„ íƒ"""
    print("\n" + "="*70)
    print("=== ì‚¬ìš©í•  ëª¨ë¸ ì„ íƒ ===")
    print("="*70)
    
    for key, model_info in MODELS.items():
        path = model_info['path']
        exists = os.path.exists(path)
        status = "âœ… ì‚¬ìš© ê°€ëŠ¥" if exists else "âŒ ì—†ìŒ"
        print("{}. {} {}".format(key, model_info['name'], status))
    
    print()
    while True:
        choice = input("ëª¨ë¸ì„ ì„ íƒí•˜ì„¸ìš” (1-2): ").strip()
        if choice in MODELS:
            return choice
        print("âŒ ì˜ëª»ëœ ì…ë ¥ì…ë‹ˆë‹¤. ë‹¤ì‹œ ì„ íƒí•´ì£¼ì„¸ìš”.")

# ============================================================================
# ì–‘ìí™” ì„ íƒ í•¨ìˆ˜
# ============================================================================
def select_quantization():
    """ì‚¬ìš©í•  ì–‘ìí™” ì˜µì…˜ ì„ íƒ"""
    print("\n" + "="*70)
    print("=== ì–‘ìí™” ì˜µì…˜ ì„ íƒ ===")
    print("="*70)
    
    for key, quant_info in QUANTIZE_OPTIONS.items():
        enabled = "âœ…" if quant_info['enabled'] else "âŒ"
        print("{}. {} {}".format(key, quant_info['name'], enabled))
    
    print()
    while True:
        choice = input("ì–‘ìí™” ì˜µì…˜ì„ ì„ íƒí•˜ì„¸ìš” (1-3): ").strip()
        if choice in QUANTIZE_OPTIONS:
            return choice
        print("âŒ ì˜ëª»ëœ ì…ë ¥ì…ë‹ˆë‹¤. ë‹¤ì‹œ ì„ íƒí•´ì£¼ì„¸ìš”.")

# ============================================================================
# ìŒì„± ì¶œë ¥ í•¨ìˆ˜
# ============================================================================
def speak_text_gtts(text):
    """TTS ìŒì„± ì¶œë ¥"""
    def _speak():
        temp_file = None
        try:
            pygame.mixer.init()
            tts = gTTS(text=text, lang='en', slow=False)
            temp_file = tempfile.NamedTemporaryFile(delete=False, suffix='.mp3')
            temp_filename = temp_file.name
            temp_file.close()
            
            tts.save(temp_filename)
            pygame.mixer.music.load(temp_filename)
            pygame.mixer.music.play()
            
            while pygame.mixer.music.get_busy():
                pygame.time.Clock().tick(10)
            
        except Exception as e:
            print("TTS Error: {}".format(e))
        finally:
            try:
                if temp_file and os.path.exists(temp_filename):
                    pygame.mixer.music.unload()
                    os.remove(temp_filename)
            except:
                pass
    
    thread = threading.Thread(target=_speak)
    thread.daemon = True
    thread.start()

# ============================================================================
# ëª¨ë¸ ë¡œë“œ
# ============================================================================
def load_model(model_choice):
    """í•™ìŠµëœ ìº¡ì…”ë‹ ëª¨ë¸ ë¡œë“œ"""
    model_info = MODELS[model_choice]
    model_path = model_info['path']
    
    # íŒŒì¼ ì¡´ì¬ í™•ì¸
    if not os.path.exists(model_path):
        if model_info['fallback']:
            model_path = model_info['fallback']
            if not os.path.exists(model_path):
                print("âŒ ëª¨ë¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {}".format(model_info['path']))
                return None, None, None, None
        else:
            print("âŒ ëª¨ë¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {}".format(model_path))
            return None, None, None, None
    
    try:
        print("\nğŸ“‚ ëª¨ë¸ ë¡œë“œ ì¤‘: {}".format(model_path))
        
        # í”„ë¡œì íŠ¸ ëª¨ë“ˆ ì‹¤ì œ ë¡œë“œ (ì§€ì—° ë¡œë“œ)
        print("  1ï¸âƒ£  ëª¨ë¸ í´ë˜ìŠ¤ ë¡œë“œ...", file=sys.stderr)
        try:
            Model = _model_class_loader()
            print("     âœ… ë¡œë“œ ì™„ë£Œ", file=sys.stderr)
        except Exception as e:
            print("     âŒ ë¡œë“œ ì‹¤íŒ¨: {}".format(e), file=sys.stderr)
            return None, None, None, None
        
        print("  2ï¸âƒ£  ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ...", file=sys.stderr)
        
        # CPUì—ì„œ ë¡œë“œ (ë©”ëª¨ë¦¬ ì•ˆì „) - Python/PyTorch ë²„ì „ í˜¸í™˜ì„±
        try:
            # Python 3.11+: weights_only íŒŒë¼ë¯¸í„° í•„ìš”
            checkpoint = torch.load(model_path, map_location=device, weights_only=False)
        except TypeError:
            # Python 3.6-3.10: weights_only íŒŒë¼ë¯¸í„° ë¯¸ì§€ì›
            checkpoint = torch.load(model_path, map_location=device)
        
        print("     âœ… ë¡œë“œ ì™„ë£Œ", file=sys.stderr)
        
        if isinstance(checkpoint, dict) and 'model_state_dict' in checkpoint:
            word_map = checkpoint.get('word_map')
            rev_word_map = checkpoint.get('rev_word_map')
            vocab_size = checkpoint.get('vocab_size')
            
            if word_map is None or rev_word_map is None:
                print("âŒ ë‹¨ì–´ì¥ ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤.")
                return None, None, None, None
            
            # ì²´í¬í¬ì¸íŠ¸ì—ì„œ ëª¨ë¸ í¬ê¸° ì •ë³´ ì¶”ì¶œ
            state_dict = checkpoint['model_state_dict']
            
            print("  ğŸ“‹ State dict ë¶„ì„...", file=sys.stderr)
            print("     í‚¤ ê°œìˆ˜: {}".format(len(state_dict)), file=sys.stderr)
            
            # â˜… state_dict í‚¤ ì¶œë ¥ (ë””ë²„ê¹…)
            for i, key in enumerate(list(state_dict.keys())[:5]):
                shape = state_dict[key].shape
                print("     [{}/{}] {}: {}".format(i+1, min(5, len(state_dict)), key, shape), file=sys.stderr)
            if len(state_dict) > 5:
                print("     ... ì™¸ {}ê°œ".format(len(state_dict) - 5), file=sys.stderr)
            
            # â˜… í•µì‹¬: state_dictì—ì„œ **ì‹¤ì œ í”„ë£¨ë‹ëœ í¬ê¸°** ì¶”ì¶œ
            decoder_dim = checkpoint.get('decoder_dim', 512)
            attention_dim = checkpoint.get('attention_dim', 256)
            
            # state_dictì—ì„œ ì •í™•í•œ í¬ê¸° ì¶”ì¶œ (í”„ë£¨ë‹ëœ ì‹¤ì œ í¬ê¸°)
            if 'decoder.decode_step.weight_ih' in state_dict:
                # GRUì˜ input_size: (hidden_size * 3) ì´ë¯€ë¡œ ì—­ìœ¼ë¡œ ê³„ì‚°
                actual_size = state_dict['decoder.decode_step.weight_ih'].shape[0]
                actual_decoder_dim = actual_size // 3
                print("  ğŸ” decoder.decode_step.weight_ih í˜•íƒœ: {}".format(
                    state_dict['decoder.decode_step.weight_ih'].shape), file=sys.stderr)
                print("     ê³„ì‚°ëœ decoder_dim: {}".format(actual_decoder_dim), file=sys.stderr)
                decoder_dim = actual_decoder_dim
            else:
                print("  âš ï¸  decoder.decode_step.weight_ih ì—†ìŒ!", file=sys.stderr)
            
            if 'decoder.encoder_att.weight' in state_dict:
                actual_attention_dim = state_dict['decoder.encoder_att.weight'].shape[0]
                print("  ğŸ” decoder.encoder_att.weight í˜•íƒœ: {}".format(
                    state_dict['decoder.encoder_att.weight'].shape), file=sys.stderr)
                print("     ê³„ì‚°ëœ attention_dim: {}".format(actual_attention_dim), file=sys.stderr)
                attention_dim = actual_attention_dim
            else:
                print("  âš ï¸  decoder.encoder_att.weight ì—†ìŒ!", file=sys.stderr)
            
            print("   ğŸ“ ìµœì¢… ê°ì§€ëœ ëª¨ë¸ êµ¬ì¡° (í”„ë£¨ë‹ëœ í¬ê¸°):")
            print("      â€¢ Decoder Dim: {}".format(decoder_dim))
            print("      â€¢ Attention Dim: {}".format(attention_dim))
            print("      â€¢ Vocab Size: {}".format(vocab_size))
            
            # â˜… ì˜¬ë°”ë¥¸ í¬ê¸°(í”„ë£¨ë‹ëœ í¬ê¸°)ë¡œ ëª¨ë¸ ìƒì„±
            print("  3ï¸âƒ£  ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤ ìƒì„± (í”„ë£¨ë‹ëœ í¬ê¸°)...", file=sys.stderr)
            try:
                # ë©”ëª¨ë¦¬ ì •ë¦¬
                gc.collect()
                gc.collect()
                gc.collect()
                
                # í”„ë£¨ë‹ëœ í¬ê¸°ë¡œ ëª¨ë¸ ìƒì„±
                model = Model(
                    vocab_size=vocab_size,
                    embed_dim=300,
                    decoder_dim=decoder_dim,      # â˜… í”„ë£¨ë‹ëœ í¬ê¸°
                    attention_dim=attention_dim   # â˜… í”„ë£¨ë‹ëœ í¬ê¸°
                )
                del Model

                print("     âœ… ìƒì„± ì™„ë£Œ (decoder_dim={}, attention_dim={})".format(
                    decoder_dim, attention_dim), file=sys.stderr)
                
                # CPU ì „í™˜
                model = model.to(device)
                model.eval()
                
            except Exception as e:
                print("     âŒ ìƒì„± ì‹¤íŒ¨: {}".format(e), file=sys.stderr)
                import traceback
                traceback.print_exc(file=sys.stderr)
                return None, None, None, None
            
            # state_dict ë¡œë“œ (ì™„ë²½í•œ í¬ê¸° ë§¤ì¹­)
            print("  4ï¸âƒ£  ê°€ì¤‘ì¹˜ ë¡œë“œ...", file=sys.stderr)
            try:
                # â˜… ë¨¼ì € ëª¨ë¸ì˜ state_dict í™•ì¸
                model_state = model.state_dict()
                print("     ëª¨ë¸ state_dict í‚¤: {}".format(len(model_state)), file=sys.stderr)
                print("     ë¡œë“œí•  state_dict í‚¤: {}".format(len(state_dict)), file=sys.stderr)
                
                # â˜… ëˆ„ë½ëœ í‚¤ í™•ì¸
                missing_keys = set(model_state.keys()) - set(state_dict.keys())
                if missing_keys:
                    print("     âš ï¸  ëˆ„ë½ëœ í‚¤: {}".format(missing_keys), file=sys.stderr)
                
                unexpected_keys = set(state_dict.keys()) - set(model_state.keys())
                if unexpected_keys:
                    print("     âš ï¸  ì˜ˆìƒ ì™¸ í‚¤: {}".format(unexpected_keys), file=sys.stderr)
                
                # â˜… strict=True ì‚¬ìš©: ëª¨ë“  ë ˆì´ì–´ê°€ ì •í™•íˆ ë§¤ì¹­ë˜ì–´ì•¼ í•¨
                model.load_state_dict(state_dict, strict=True)
                print("     âœ… ì™„ë²½í•œ í¬ê¸° ë§¤ì¹­ìœ¼ë¡œ ë¡œë“œ ì™„ë£Œ", file=sys.stderr)
            except Exception as e:
                print("     âš ï¸  strict=True ë¡œë“œ ì‹¤íŒ¨: {}".format(e), file=sys.stderr)
                print("     strict=Falseë¡œ ì¬ì‹œë„ ì¤‘...", file=sys.stderr)
                try:
                    model.load_state_dict(state_dict, strict=False)
                    print("     âš ï¸  ì¼ë¶€ ë ˆì´ì–´ë§Œ ë¡œë“œë¨ (í”„ë£¨ë‹ íš¨ê³¼ ê°ì†Œ)", file=sys.stderr)
                except Exception as e2:
                    print("     âŒ ê°€ì¤‘ì¹˜ ë¡œë“œ ì‹¤íŒ¨: {}".format(e2), file=sys.stderr)
                    print("     ëª¨ë¸ì„ ë¬´ì‘ìœ„ ì´ˆê¸°í™” ìƒíƒœë¡œ ì‚¬ìš©í•©ë‹ˆë‹¤.", file=sys.stderr)
                    import traceback
                    traceback.print_exc(file=sys.stderr)
            
            # ë©”ëª¨ë¦¬ ì •ë¦¬
            print("  5ï¸âƒ£  ë©”ëª¨ë¦¬ ì •ë¦¬...", file=sys.stderr)
            del checkpoint, state_dict
            gc.collect()
            print("     âœ… ì •ë¦¬ ì™„ë£Œ", file=sys.stderr)
            
            model.eval()
            
            # ëª¨ë¸ to CPU ëª…ì‹œ
            try:
                model = model.to(device)
                model.eval()
            except:
                pass
            
            model_name = model_info['name']
            
            # â˜… ëª¨ë¸ í¬ê¸° ì •ë³´ ì¶œë ¥
            param_count = sum(p.numel() for p in model.parameters())
            param_size = param_count * 4 / 1024 / 1024  # FP32 ê¸°ì¤€
            
            print("\nâœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
            print("   ëª¨ë¸: {}".format(model_name))
            print("   ê²½ë¡œ: {}".format(model_path))
            print("   ì´ íŒŒë¼ë¯¸í„°: {:,}ê°œ".format(param_count))
            print("   ëª¨ë¸ í¬ê¸°: {:.2f} MB (FP32)".format(param_size))
            print("   ë””ì½”ë” ì°¨ì›: {} ".format(decoder_dim))
            print("   ì–´í…ì…˜ ì°¨ì›: {} ".format(attention_dim))
            
            return model, word_map, rev_word_map, model_name
        else:
            print("âŒ ì˜ëª»ëœ ëª¨ë¸ íŒŒì¼ í˜•ì‹ì…ë‹ˆë‹¤.")
            return None, None, None, None
            
    except Exception as e:
        print("âŒ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {}".format(e))
        import traceback
        traceback.print_exc()
        return None, None, None, None
    
def gstreamer_pipeline(
    sensor_id=0,
    capture_width=1280,
    capture_height=720,
    display_width=640,
    display_height=480,
    framerate=30,
    flip_method=0,
):
    # 'nvv4l2camerasrc' ë˜ëŠ” 'nvarguscamerasrc'ë¥¼ ì‚¬ìš©í•˜ì—¬ í•˜ë“œì›¨ì–´ ê°€ì† í™œìš©
        return (
            "nvarguscamerasrc sensor-id=%d ! "
            "video/x-raw(memory:NVMM), width=(int)%d, height=(int)%d, framerate=(fraction)%d/1 ! "
            "nvvidconv flip-method=%d ! "
            "video/x-raw, width=(int)%d, height=(int)%d, format=(string)BGRx ! "
            "videoconvert ! "
            "video/x-raw, format=(string)BGR ! appsink"
            % (
                sensor_id,
                capture_width,
                capture_height,
                framerate,
                flip_method,
                display_width,
                display_height,
            )
        )
# ============================================================================
# ì–‘ìí™” ì ìš© í•¨ìˆ˜
# ============================================================================
def apply_quantization(model, quant_choice, model_name):
    """ëª¨ë¸ì— ì–‘ìí™” ì ìš©"""
    quant_info = QUANTIZE_OPTIONS[quant_choice]
    quant_name = quant_info['name']
    
    if quant_choice == '1':
        # FP32 - ì–‘ìí™” ì—†ìŒ
        print("\nâœ… FP32 (ì–‘ìí™” ì—†ìŒ)")
        dtypes = torch.float32
        model = model.to(device)
        model.eval()
        return model, model_name
    
    elif quant_choice == '2':
        # FP16 - Half Precision (CPUì—ì„œëŠ” ì œí•œì )
        print("\nğŸ“Š ì–‘ìí™” ì ìš© ì¤‘: {}".format(quant_name))
        try:
            dtypes = torch.float16
            model = model.half().to(device)
            model.eval()
            print("âœ… FP16 ë³€í™˜ ì™„ë£Œ")
            model_name = "{} + FP16".format(model_name)
            return model, model_name
        except Exception as e:
            print("âš ï¸ FP16 ë³€í™˜ ì‹¤íŒ¨: {}".format(e))
            model = model.to(device)
            model.eval()
            return model, model_name
    
    
    model = model.to(device)
    model.eval()
    return model, model_name

# ============================================================================
# ìº¡ì…˜ ìƒì„± í•¨ìˆ˜
# ============================================================================
# ìº¡ì…˜ ìƒì„± í•¨ìˆ˜
# ============================================================================
def generate_caption_from_image(model, word_map, rev_word_map, frame):
    """ì´ë¯¸ì§€ë¡œë¶€í„° ìº¡ì…˜ ìƒì„±"""
    image_tensor = None
    try:
        # ëª¨ë¸ì„ ë””ë°”ì´ìŠ¤ë¡œ ì´ë™
        model = model.to(device)
        model.eval()
        frame=frame.to(dtypes)
        # ì´ë¯¸ì§€ ì „ì²˜ë¦¬
        image_tensor = preprocess_image(frame)
        
        # ìº¡ì…˜ ìƒì„±
        start_time = time.time()
        try:
            with torch.no_grad():
                # ë©”ëª¨ë¦¬ ì•ˆì „ì„±ì„ ìœ„í•´ ë°°ì¹˜ í¬ê¸° = 1ë¡œ ì œí•œ
                generated_words = model.generate(image_tensor, word_map, rev_word_map, max_len=50,device=device)
        except RuntimeError as e:
            print("ê²½ê³ : ì¶”ë¡  ì‹¤íŒ¨ - {}".format(e))
            gc.collect()
            return None, 0.0
        except Exception as e:
            print("ê²½ê³ : ì˜ˆìƒ ë¶ˆê°€ëŠ¥í•œ ì˜¤ë¥˜ - {}".format(e))
            import traceback
            traceback.print_exc()
            gc.collect()
            return None, 0.0
        finally:
            # ì´ë¯¸ì§€ í…ì„œ ë©”ëª¨ë¦¬ í•´ì œ
            if image_tensor is not None:
                del image_tensor
            gc.collect()
        
        inference_time = (time.time() - start_time) * 1000
        
        # í† í° ì œê±°í•˜ê³  ë¬¸ì¥ìœ¼ë¡œ ë³€í™˜
        caption = ' '.join([w for w in generated_words if w not in ['<start>', '<end>', '<pad>', '<unk>']])
        
        return caption, inference_time
    except Exception as e:
        print("ìº¡ì…˜ ìƒì„± ì˜¤ë¥˜: {}".format(e))
        import traceback
        traceback.print_exc()
        if image_tensor is not None:
            del image_tensor
        gc.collect()
        return None, 0.0

# ============================================================================
# ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜
# ============================================================================
def main():
    print("\nğŸ“Š Jetson Nano ì´ë¯¸ì§€ ìº¡ì…”ë‹ ì‹œìŠ¤í…œ")
    print("="*70)
    
    # ëª¨ë¸ ì„ íƒ
    model_choice = select_model()
    
    # ëª¨ë¸ ë¡œë“œ
    model, word_map, rev_word_map, model_name = load_model(model_choice)
    if model is None:
        print("âŒ ëª¨ë¸ì„ ë¡œë“œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    # ì–‘ìí™” ì„ íƒ ë° ì ìš©
    quant_choice = select_quantization()
    model, model_name = apply_quantization(model, quant_choice, model_name)
    
    try:
        # ë”ë¯¸ ë°ì´í„° ìƒì„± (1, 3, 224, 224)
        dummy_input = torch.zeros(1, 3, 224, 224).to(device)
        if device.type == 'cuda':
            dummy_input = dummy_input.half() # FP16 ëª¨ë“œë¼ë©´

        # ê°•ì œë¡œ í•œ ë²ˆ ì‹¤í–‰ì‹œì¼œì„œ CUDA ì»¤ë„ì„ ê¹¨ì›€
        with torch.no_grad():
            # generate í•¨ìˆ˜ê°€ ì•„ë‹ˆë¼ encoderë§Œ í†µê³¼ì‹œì¼œë„ íš¨ê³¼ ìˆìŒ
            if hasattr(model, 'encoder'):
                _ = model.encoder(dummy_input)
        
        # GPU ë™ê¸°í™” (ì™„ë£Œë  ë•Œê¹Œì§€ ëŒ€ê¸°)
        if device.type == 'cuda':
            torch.cuda.synchronize()
            
        print("âœ… ì›Œë°ì—… ì™„ë£Œ! ì´ì œ ë°”ë¡œ ìº¡ì…˜ì´ ìƒì„±ë©ë‹ˆë‹¤.")
    except Exception as e:
        print(f"âš ï¸ ì›Œë°ì—… ê±´ë„ˆëœ€: {e}")
    
    # ì„±ëŠ¥ ëª¨ë‹ˆí„° ìƒì„±
    try:
        monitor = PerformanceMonitor(model)
    except Exception as e:
        print("âš ï¸  ì„±ëŠ¥ ëª¨ë‹ˆí„° ì´ˆê¸°í™” ì‹¤íŒ¨: {}".format(e))
        monitor = None

    # ì¹´ë©”ë¼ ì´ˆê¸°í™”
    print("\nğŸ“¹ ì¹´ë©”ë¼ ì´ˆê¸°í™” ì¤‘...")
    
    cap = cv2.VideoCapture(gstreamer_pipeline(), cv2.CAP_GSTREAMER)
    if not cap.isOpened():
        cap=cv2.VideoCapture(0)
    if not cap.isOpened():
        print("âŒ ì¹´ë©”ë¼ë¥¼ ì—´ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    # ì¹´ë©”ë¼ ì„¤ì • ìµœì í™”
    cap.set(cv2.CAP_PROP_FRAME_WIDTH, 640)
    cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 480)
    cap.set(cv2.CAP_PROP_FPS, 30)
    
    print("\n" + "="*70)
    print("=== ì´ë¯¸ì§€ ìº¡ì…”ë‹ ì‹œìŠ¤í…œ ({}) ===".format(model_name))
    print("="*70)
    print("\nâŒ¨ï¸  í‚¤ë³´ë“œ ëª…ë ¹ì–´:")
    print("  's' : í˜„ì¬ í”„ë ˆì„ì—ì„œ ìº¡ì…˜ ìƒì„± ë° ìŒì„± ì¶œë ¥")
    print("  'r' : ë§ˆì§€ë§‰ ìº¡ì…˜ ë‹¤ì‹œ ë“£ê¸°")
    print("  'p' : ì„±ëŠ¥ í†µê³„ ì¶œë ¥")
    print("  'm' : ëª¨ë¸ ë³€ê²½")
    print("  'q' : ì¢…ë£Œ\n")
    
    last_caption = None
    is_processing = False
    current_model_name = model_name
    frame_count = 0
    
    while True:
        ret, frame = cap.read()
        if not ret:
            print("ì¹´ë©”ë¼ ì½ê¸° ì‹¤íŒ¨")
            break
        
        # ë©”ëª¨ë¦¬ ê¸°ë¡
        if monitor:
            monitor.record_memory()
        
        # ë©”ëª¨ë¦¬ ëª¨ë‹ˆí„°ë§ (5í”„ë ˆì„ë§ˆë‹¤)
        frame_count += 1
        if frame_count % 5 == 0:
            if monitor:
                current_mem = monitor.get_cpu_memory_mb()
                if current_mem > 2500:  # Jetson Nano 4GB ê¸°ì¤€
                    print("âš ï¸  ë†’ì€ ë©”ëª¨ë¦¬ ì‚¬ìš©: {:.0f}MB - ì •ë¦¬ ì¤‘...".format(current_mem))
                    gc.collect()
        
        # ì²˜ë¦¬ ì¤‘ í‘œì‹œ
        if is_processing:
            cv2.putText(frame, "Processing...", (10, 30),
                       cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 165, 255), 3)
        
        # ëª¨ë¸ ì •ë³´ í‘œì‹œ
        cv2.rectangle(frame, (5, frame.shape[0] - 75), (550, frame.shape[0] - 5), (50, 50, 50), -1)
        cv2.putText(frame, "Model: {}".format(current_model_name[:40]), (10, frame.shape[0] - 52),
                   cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 255), 2)
        
        # ì„±ëŠ¥ ì§€í‘œ í‘œì‹œ
        stats = monitor.get_stats()
        if stats:
            fps_text = "FPS: {:.1f}".format(stats['fps'])
            latency_text = "Latency: {:.1f}ms".format(stats['mean_latency_ms'])
            mem_text = "CPU: {:.0f}MB".format(stats['cpu_memory_mb'])
            
            cv2.putText(frame, fps_text, (10, frame.shape[0] - 32),
                       cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)
            cv2.putText(frame, latency_text, (10, frame.shape[0] - 12),
                       cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)
            cv2.putText(frame, mem_text, (frame.shape[1] - 250, frame.shape[0] - 32),
                       cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)
            
            if device.type in ['cuda', 'mps']:
                gpu_text = "GPU: {:.0f}MB".format(stats['gpu_memory_mb'])
                cv2.putText(frame, gpu_text, (frame.shape[1] - 250, frame.shape[0] - 12),
                           cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)
        
        # ë§ˆì§€ë§‰ ìº¡ì…˜ í‘œì‹œ
        if last_caption and not is_processing:
            caption_y = 60
            max_width = frame.shape[1] - 20
            words = last_caption.split()
            line = ""
            line_num = 0
            
            for word in words:
                test_line = line + word + " "
                text_size = cv2.getTextSize(test_line, cv2.FONT_HERSHEY_SIMPLEX, 0.5, 1)[0]
                
                if text_size[0] > max_width:
                    text_size_actual = cv2.getTextSize(line, cv2.FONT_HERSHEY_SIMPLEX, 0.5, 1)[0]
                    cv2.rectangle(frame, (5, caption_y + line_num * 25 - 20), 
                                (15 + text_size_actual[0], caption_y + line_num * 25 + 5), 
                                (0, 0, 0), -1)
                    cv2.putText(frame, line, (10, caption_y + line_num * 25),
                               cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 200, 0), 2)
                    line = word + " "
                    line_num += 1
                else:
                    line = test_line
            
            if line:
                text_size_actual = cv2.getTextSize(line, cv2.FONT_HERSHEY_SIMPLEX, 0.5, 1)[0]
                cv2.rectangle(frame, (5, caption_y + line_num * 25 - 20), 
                            (15 + text_size_actual[0], caption_y + line_num * 25 + 5), 
                            (0, 0, 0), -1)
                cv2.putText(frame, line, (10, caption_y + line_num * 25),
                           cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 200, 0), 2)
        
        cv2.imshow('Image Captioning', frame)
        
        # í‚¤ ì…ë ¥ ì²˜ë¦¬
        key = cv2.waitKey(1) & 0xFF
        
        if key == ord('q'):
            print("\nì¢…ë£Œ")
            break
            
        elif key == ord('s') and not is_processing:
            is_processing = True
            print("\n" + "="*70)
            print("ìº¡ì…˜ ìƒì„± ì¤‘...")
            
            caption, inf_time = generate_caption_from_image(model, word_map, rev_word_map, frame)
            monitor.record_inference(inf_time)
            
            if caption:
                last_caption = caption
                print("\nìƒì„±ëœ ìº¡ì…˜: {}".format(caption))
                print("ì¶”ë¡  ì‹œê°„: {:.2f}ms".format(inf_time))
                
                # ìº¡ì…˜ ìŒì„± ì¶œë ¥
                speak_text_gtts(caption)
            else:
                print("ìº¡ì…˜ ìƒì„± ì‹¤íŒ¨")
            
            print("="*70 + "\n")
            is_processing = False
            
        elif key == ord('r') and last_caption:
            print("\nğŸ”Š ë§ˆì§€ë§‰ ìº¡ì…˜: \"{}\"".format(last_caption))
            speak_text_gtts(last_caption)
            
        elif key == ord('p'):
            monitor.print_stats()
            
    
    cap.release()
    cv2.destroyAllWindows()

if __name__ == "__main__":
    main()