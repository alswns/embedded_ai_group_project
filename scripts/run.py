import cv2
import torch
import numpy as np
import os
import threading
import tempfile
import time
from PIL import Image
from torchvision import transforms
from gtts import gTTS
import pygame
from src.muti_modal_model.model import MobileNetCaptioningModel

# ============================================================================
# í™˜ê²½ ì„¤ì •
# ============================================================================
# ë””ë°”ì´ìŠ¤ ì„ íƒ
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"ë””ë°”ì´ìŠ¤: {device}")

# ëª¨ë¸ ê²½ë¡œ ì„¤ì •
MODEL_PATH = "models/lightweight_captioning_model.pth"
if not os.path.exists(MODEL_PATH):
    MODEL_PATH = "lightweight_captioning_model.pth"

# ì´ë¯¸ì§€ ì „ì²˜ë¦¬
transform = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406], 
                       std=[0.229, 0.224, 0.225])
])

# ============================================================================
# ìŒì„± ì¶œë ¥ í•¨ìˆ˜
# ============================================================================
def speak_text_gtts(text):
    """TTS ìŒì„± ì¶œë ¥"""
    def _speak():
        temp_file = None
        try:
            pygame.mixer.init()
            tts = gTTS(text=text, lang='en', slow=False)
            temp_file = tempfile.NamedTemporaryFile(delete=False, suffix='.mp3')
            temp_filename = temp_file.name
            temp_file.close()
            
            tts.save(temp_filename)
            pygame.mixer.music.load(temp_filename)
            pygame.mixer.music.play()
            
            while pygame.mixer.music.get_busy():
                pygame.time.Clock().tick(10)
            
        except Exception as e:
            print(f"TTS Error: {e}")
        finally:
            try:
                if temp_file and os.path.exists(temp_filename):
                    pygame.mixer.music.unload()
                    os.remove(temp_filename)
            except:
                pass
    
    thread = threading.Thread(target=_speak)
    thread.daemon = True
    thread.start()

# ============================================================================
# ëª¨ë¸ ë¡œë“œ
# ============================================================================
def load_model():
    """í•™ìŠµëœ ìº¡ì…”ë‹ ëª¨ë¸ ë¡œë“œ"""
    if not os.path.exists(MODEL_PATH):
        print(f"âŒ ëª¨ë¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {MODEL_PATH}")
        return None, None, None
    
    try:
        print(f"ğŸ“‚ ëª¨ë¸ ë¡œë“œ ì¤‘: {MODEL_PATH}")
        checkpoint = torch.load(MODEL_PATH, map_location=device, weights_only=False)
        
        if isinstance(checkpoint, dict) and 'model_state_dict' in checkpoint:
            word_map = checkpoint.get('word_map')
            rev_word_map = checkpoint.get('rev_word_map')
            vocab_size = checkpoint.get('vocab_size')
            
            if word_map is None or rev_word_map is None:
                print("âŒ ë‹¨ì–´ì¥ ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤.")
                return None, None, None
            
            # ëª¨ë¸ ìƒì„±
            model = MobileNetCaptioningModel(vocab_size=vocab_size, embed_dim=300).to(device)
            model.load_state_dict(checkpoint['model_state_dict'])
            model.eval()
            
            print(f"âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ (ë‹¨ì–´ì¥ í¬ê¸°: {vocab_size})")
            return model, word_map, rev_word_map
        else:
            print("âŒ ì˜ëª»ëœ ëª¨ë¸ íŒŒì¼ í˜•ì‹ì…ë‹ˆë‹¤.")
            return None, None, None
            
    except Exception as e:
        print(f"âŒ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
        import traceback
        traceback.print_exc()
        return None, None, None

# ============================================================================
# ìº¡ì…˜ ìƒì„± í•¨ìˆ˜
# ============================================================================
def generate_caption_from_image(model, word_map, rev_word_map, frame):
    """ì´ë¯¸ì§€ë¡œë¶€í„° ìº¡ì…˜ ìƒì„±"""
    try:
        # OpenCV BGRì„ RGBë¡œ ë³€í™˜
        rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
        pil_image = Image.fromarray(rgb_frame)
        
        # ì „ì²˜ë¦¬
        image_tensor = transform(pil_image).unsqueeze(0).to(device)
        
        # ìº¡ì…˜ ìƒì„±
        start_time = time.time()
        with torch.no_grad():
            generated_words = model.generate(image_tensor, word_map, rev_word_map, max_len=50)
        
        inference_time = (time.time() - start_time) * 1000
        
        # í† í° ì œê±°í•˜ê³  ë¬¸ì¥ìœ¼ë¡œ ë³€í™˜
        caption = ' '.join([w for w in generated_words if w not in ['<start>', '<end>', '<pad>', '<unk>']])
        
        return caption, inference_time
    except Exception as e:
        print(f"ìº¡ì…˜ ìƒì„± ì˜¤ë¥˜: {e}")
        return None, 0.0

# ============================================================================
# ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜
# ============================================================================
def main():
    # ëª¨ë¸ ë¡œë“œ
    model, word_map, rev_word_map = load_model()
    if model is None:
        print("ëª¨ë¸ì„ ë¡œë“œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. í•™ìŠµì„ ë¨¼ì € ì‹¤í–‰í•˜ì„¸ìš”.")
        return
    
    # ì¹´ë©”ë¼ ì´ˆê¸°í™”
    cap = cv2.VideoCapture(0)
    if not cap.isOpened():
        print("âŒ ì¹´ë©”ë¼ë¥¼ ì—´ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    print("\n" + "="*70)
    print("=== ì´ë¯¸ì§€ ìº¡ì…”ë‹ ì‹¤ì‹œê°„ ì‹¤í–‰ ===")
    print("="*70)
    print("\ní‚¤ë³´ë“œ ëª…ë ¹ì–´:")
    print("  's' : í˜„ì¬ í”„ë ˆì„ì—ì„œ ìº¡ì…˜ ìƒì„± ë° ìŒì„± ì¶œë ¥")
    print("  'r' : ë§ˆì§€ë§‰ ìº¡ì…˜ ë‹¤ì‹œ ë“£ê¸°")
    print("  'q' : ì¢…ë£Œ\n")
    
    last_caption = None
    is_processing = False
    inference_times = []
    
    while True:
        ret, frame = cap.read()
        if not ret:
            print("ì¹´ë©”ë¼ ì½ê¸° ì‹¤íŒ¨")
            break
        
        # ì²˜ë¦¬ ì¤‘ í‘œì‹œ
        if is_processing:
            cv2.putText(frame, "Processing...", (10, 30),
                       cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 165, 255), 3)
        
        # ëª¨ë¸ ì •ë³´ í‘œì‹œ
        cv2.rectangle(frame, (5, frame.shape[0] - 35), (500, frame.shape[0] - 5), (50, 50, 50), -1)
        cv2.putText(frame, "Image Captioning Model", (10, frame.shape[0] - 12),
                   cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 255), 2)
        
        # í‰ê·  ì¶”ë¡  ì‹œê°„ í‘œì‹œ
        if inference_times:
            avg_inf_time = np.mean(inference_times[-30:])
            cv2.rectangle(frame, (frame.shape[1] - 200, frame.shape[0] - 35), 
                         (frame.shape[1] - 5, frame.shape[0] - 5), (50, 50, 50), -1)
            cv2.putText(frame, f"Inf: {avg_inf_time:.1f}ms", (frame.shape[1] - 190, frame.shape[0] - 12),
                       cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)
        
        # ë§ˆì§€ë§‰ ìº¡ì…˜ í‘œì‹œ
        if last_caption and not is_processing:
            caption_y = 60
            max_width = frame.shape[1] - 20
            words = last_caption.split()
            line = ""
            line_num = 0
            
            for word in words:
                test_line = line + word + " "
                text_size = cv2.getTextSize(test_line, cv2.FONT_HERSHEY_SIMPLEX, 0.5, 1)[0]
                
                if text_size[0] > max_width:
                    text_size_actual = cv2.getTextSize(line, cv2.FONT_HERSHEY_SIMPLEX, 0.5, 1)[0]
                    cv2.rectangle(frame, (5, caption_y + line_num * 25 - 20), 
                                (15 + text_size_actual[0], caption_y + line_num * 25 + 5), 
                                (0, 0, 0), -1)
                    cv2.putText(frame, line, (10, caption_y + line_num * 25),
                               cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 200, 0), 2)
                    line = word + " "
                    line_num += 1
                else:
                    line = test_line
            
            if line:
                text_size_actual = cv2.getTextSize(line, cv2.FONT_HERSHEY_SIMPLEX, 0.5, 1)[0]
                cv2.rectangle(frame, (5, caption_y + line_num * 25 - 20), 
                            (15 + text_size_actual[0], caption_y + line_num * 25 + 5), 
                            (0, 0, 0), -1)
                cv2.putText(frame, line, (10, caption_y + line_num * 25),
                           cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 200, 0), 2)
        
        cv2.imshow('Image Captioning', frame)
        
        # í‚¤ ì…ë ¥ ì²˜ë¦¬
        key = cv2.waitKey(1) & 0xFF
        
        if key == ord('q'):
            print("\nì¢…ë£Œ")
            break
            
        elif key == ord('s') and not is_processing:
            is_processing = True
            print("\n" + "="*70)
            print("ìº¡ì…˜ ìƒì„± ì¤‘...")
            
            caption, inf_time = generate_caption_from_image(model, word_map, rev_word_map, frame)
            
            if caption:
                last_caption = caption
                inference_times.append(inf_time)
                print(f"\nìƒì„±ëœ ìº¡ì…˜: {caption}")
                print(f"ì¶”ë¡  ì‹œê°„: {inf_time:.2f}ms")
                
                # ìº¡ì…˜ ìŒì„± ì¶œë ¥
                speak_text_gtts(caption)
            else:
                print("ìº¡ì…˜ ìƒì„± ì‹¤íŒ¨")
            
            print("="*70 + "\n")
            is_processing = False
            
        elif key == ord('r') and last_caption:
            print(f"\në§ˆì§€ë§‰ ìº¡ì…˜: \"{last_caption}\"")
            speak_text_gtts(last_caption)
    
    cap.release()
    cv2.destroyAllWindows()

if __name__ == "__main__":
    main()