"""
Pruning ë²¤ì¹˜ë§ˆí¬ ìŠ¤í¬ë¦½íŠ¸
ë‹¤ì–‘í•œ Pruning ê¸°ë²•ì„ ì ìš©í•˜ê³  ì„±ëŠ¥ì„ ë¹„êµí•©ë‹ˆë‹¤.
"""
import torch
import torch.nn as nn
import torch.nn.utils.prune as prune
import numpy as np
import os
import time
import psutil
import matplotlib.pyplot as plt
import matplotlib
from copy import deepcopy
import gc
from PIL import Image
from torchvision import transforms
import platform
import warnings

warnings.filterwarnings('ignore')

# -------------------------------------------------------------------------
# ëª¨ë¸ import
# -------------------------------------------------------------------------
try:
    from src.muti_modal_model.model import MobileNetCaptioningModel
except ImportError:
    print("âš ï¸ ëª¨ë¸ í´ë˜ìŠ¤ë¥¼ importí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê²½ë¡œë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.")
    class MobileNetCaptioningModel(nn.Module):
        def __init__(self, vocab_size, embed_dim):
            super().__init__()
            self.emb = nn.Embedding(vocab_size, embed_dim)
            self.gru = nn.GRU(embed_dim, 512)
            self.fc = nn.Linear(512, vocab_size)
        def generate(self, img, wm, rwm, max_len):
            return ["<start>", "a", "test", "caption", "<end>"]

# NLTK ë° METEOR ì„¤ì •
try:
    from nltk.translate.meteor_score import meteor_score
    from nltk.tokenize import word_tokenize
    import nltk
    nltk.download('punkt', quiet=True)
    nltk.download('wordnet', quiet=True)
    METEOR_AVAILABLE = True
except ImportError:
    print("âš ï¸ nltkê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. METEOR ì ìˆ˜ ê³„ì‚° ë¶ˆê°€.")
    METEOR_AVAILABLE = False

# ============================================================================
# ì„¤ì •
# ============================================================================
matplotlib.use('Agg')

# í•œê¸€ í°íŠ¸ ì„¤ì •
os_name = platform.system()
if os_name == 'Windows':
    plt.rcParams['font.family'] = 'Malgun Gothic'
    plt.rcParams['axes.unicode_minus'] = False
elif os_name == 'Darwin':
    plt.rcParams['font.family'] = 'AppleGothic'
    plt.rcParams['axes.unicode_minus'] = False
elif os_name == 'Linux':
    plt.rcParams['font.family'] = 'NanumGothic'
    plt.rcParams['axes.unicode_minus'] = False
else:
    plt.rcParams['axes.unicode_minus'] = False

MODEL_PATH = "models/lightweight_captioning_model.pth"
TEST_IMAGE_DIR = "assets/images"
CAPTIONS_FILE = "assets/captions.txt"
OUTPUT_DIR = "pruning_results"
NUM_RUNS = 50

# Pruning ì„¤ì •
PRUNING_RATES = [0.1, 0.3, 0.5, 0.7]  # 10%, 30%, 50%, 70% í”„ë£¨ë‹
PRUNING_METHODS = ['magnitude', 'structured']  # í”„ë£¨ë‹ ë°©ë²•

# ë””ë°”ì´ìŠ¤ ì„ íƒ
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
if hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():
    device = torch.device("mps")
print(f"ğŸš€ ì‹¤í–‰ ë””ë°”ì´ìŠ¤: {device}")

# ì´ë¯¸ì§€ ì „ì²˜ë¦¬
transform = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
])

# ============================================================================
# ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜
# ============================================================================
def count_parameters(model):
    """ëª¨ë¸ íŒŒë¼ë¯¸í„° ê°œìˆ˜ ê³„ì‚°"""
    total_params = sum(p.numel() for p in model.parameters())
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    return total_params, trainable_params

def count_nonzero_parameters(model):
    """0ì´ ì•„ë‹Œ íŒŒë¼ë¯¸í„° ê°œìˆ˜ ê³„ì‚° (í”„ë£¨ë‹ í›„)"""
    nonzero_params = 0
    total_params = 0
    for param in model.parameters():
        total_params += param.numel()
        nonzero_params += param.nonzero().size(0) if param.numel() > 0 else 0
    return nonzero_params, total_params

def get_model_size_mb(model, sparse=False):
    """ëª¨ë¸ íŒŒë¼ë¯¸í„° + ë²„í¼ í¬ê¸° ê³„ì‚° (MB)
    
    Args:
        model: ëª¨ë¸
        sparse: Trueë©´ ì‹¤ì œ 0ì´ ì•„ë‹Œ íŒŒë¼ë¯¸í„°ë§Œ ê³„ì‚° (Pruning í›„ ì‹¤ì œ í¬ê¸°)
    """
    param_size = 0
    buffer_size = 0
    
    if sparse:
        # Sparse format: ì‹¤ì œ 0ì´ ì•„ë‹Œ ê°’ë§Œ ê³„ì‚°
        for param in model.parameters():
            if param.is_sparse:
                # Sparse tensorì¸ ê²½ìš°
                param_size += param._values().numel() * param.element_size()
                # ì¸ë±ìŠ¤ë„ ì €ì¥í•´ì•¼ í•˜ë¯€ë¡œ ì¶”ê°€
                param_size += param._indices().numel() * param._indices().element_size()
            else:
                # Dense tensorì¸ ê²½ìš° 0ì´ ì•„ë‹Œ ê°’ë§Œ ê³„ì‚°
                nonzero = param.nonzero()
                if nonzero.numel() > 0:
                    # 0ì´ ì•„ë‹Œ ê°’ì˜ ê°œìˆ˜
                    nonzero_count = (param != 0).sum().item()
                    param_size += nonzero_count * param.element_size()
                    # ì¸ë±ìŠ¤ ì €ì¥ì„ ìœ„í•œ ì˜¤ë²„í—¤ë“œ (ê°„ë‹¨í•œ ì¶”ì •)
                    # ì‹¤ì œë¡œëŠ” ë” íš¨ìœ¨ì ì¸ ì¸ì½”ë”©ì´ ê°€ëŠ¥í•˜ì§€ë§Œ, ì—¬ê¸°ì„œëŠ” ë³´ìˆ˜ì ìœ¼ë¡œ ì¶”ì •
                    param_size += nonzero_count * 4  # ì¸ë±ìŠ¤ ì €ì¥ ì˜¤ë²„í—¤ë“œ (4 bytes per index)
    else:
        # Dense format: ëª¨ë“  íŒŒë¼ë¯¸í„° ê³„ì‚°
        for param in model.parameters():
            param_size += param.nelement() * param.element_size()
    
    for buffer in model.buffers():
        buffer_size += buffer.nelement() * buffer.element_size()
    
    return (param_size + buffer_size) / 1024 / 1024

def convert_to_sparse_model(model):
    """Pruningëœ ëª¨ë¸ì„ ì‹¤ì œë¡œ sparse formatìœ¼ë¡œ ë³€í™˜í•˜ì—¬ í¬ê¸° ê°ì†Œ"""
    # ì£¼ì˜: ì‹¤ì œë¡œ ëª¨ë¸ êµ¬ì¡°ë¥¼ ë³€ê²½í•˜ëŠ” ê²ƒì€ ë³µì¡í•˜ë¯€ë¡œ
    # ì—¬ê¸°ì„œëŠ” ê°€ì¤‘ì¹˜ë¥¼ sparse tensorë¡œ ë³€í™˜í•˜ëŠ” ëŒ€ì‹ 
    # ì‹¤ì œ 0ì´ ì•„ë‹Œ íŒŒë¼ë¯¸í„°ë§Œ ê³„ì‚°í•˜ëŠ” ë°©ì‹ ì‚¬ìš©
    # ì‹¤ì œ ë°°í¬ ì‹œì—ëŠ” sparse formatìœ¼ë¡œ ì €ì¥/ë¡œë“œí•˜ëŠ” ê²ƒì´ ì¢‹ìŠµë‹ˆë‹¤
    return model

def save_sparse_model(model, path):
    """ëª¨ë¸ì„ sparse formatìœ¼ë¡œ ì €ì¥ (ì‹¤ì œ í¬ê¸° ê°ì†Œ)"""
    state_dict = {}
    for name, param in model.named_parameters():
        if param.numel() > 0:
            # 0ì´ ì•„ë‹Œ ê°’ë§Œ ì €ì¥
            nonzero_mask = param != 0
            if nonzero_mask.any():
                # Sparse formatìœ¼ë¡œ ì €ì¥
                sparse_param = param[nonzero_mask]
                indices = nonzero_mask.nonzero(as_tuple=False)
                state_dict[name] = {
                    'values': sparse_param.cpu(),
                    'indices': indices.cpu(),
                    'shape': list(param.shape),
                    'dtype': str(param.dtype)
                }
            else:
                # ëª¨ë“  ê°’ì´ 0ì¸ ê²½ìš°
                state_dict[name] = {
                    'values': torch.tensor([], dtype=param.dtype),
                    'indices': torch.tensor([], dtype=torch.long),
                    'shape': list(param.shape),
                    'dtype': str(param.dtype)
                }
        else:
            state_dict[name] = param.cpu()
    
    # ë²„í¼ë„ ì €ì¥
    for name, buffer in model.named_buffers():
        state_dict[name] = buffer.cpu()
    
    torch.save(state_dict, path)
    print(f"   ğŸ’¾ Sparse ëª¨ë¸ ì €ì¥: {path}")

def get_sparse_model_size_mb(model):
    """Sparse formatìœ¼ë¡œ ì €ì¥í–ˆì„ ë•Œì˜ ì‹¤ì œ ëª¨ë¸ í¬ê¸° ê³„ì‚°"""
    total_size = 0
    
    for name, param in model.named_parameters():
        if param.numel() > 0:
            # 0ì´ ì•„ë‹Œ ê°’ì˜ ê°œìˆ˜
            nonzero_count = (param != 0).sum().item()
            total_params = param.numel()
            
            if nonzero_count > 0:
                # ê°’ ì €ì¥ (0ì´ ì•„ë‹Œ ê°’ë§Œ)
                total_size += nonzero_count * param.element_size()
                
                # ì¸ë±ìŠ¤ ì €ì¥ (COO format: Coordinate format)
                # ê° 0ì´ ì•„ë‹Œ ê°’ì˜ ìœ„ì¹˜ë¥¼ ì €ì¥
                if len(param.shape) == 1:
                    # 1D: ì¸ë±ìŠ¤ë§Œ
                    indices_size = nonzero_count * 4  # 4 bytes per index
                elif len(param.shape) == 2:
                    # 2D: (row, col) ìŒ
                    indices_size = nonzero_count * 2 * 4  # 2 indices per value
                else:
                    # ë‹¤ì°¨ì›: ëª¨ë“  ì°¨ì›ì˜ ì¸ë±ìŠ¤
                    indices_size = nonzero_count * len(param.shape) * 4
                
                total_size += indices_size
                
                # ë©”íƒ€ë°ì´í„° (shape, dtype, nonzero_count ë“±)
                total_size += 64  # ë©”íƒ€ë°ì´í„° ì˜¤ë²„í—¤ë“œ
            else:
                # ëª¨ë“  ê°’ì´ 0ì¸ ê²½ìš° ìµœì†Œ ë©”íƒ€ë°ì´í„°ë§Œ
                total_size += 32
    
    # ë²„í¼ í¬ê¸° (ë²„í¼ëŠ” ë³´í†µ ì‘ìœ¼ë¯€ë¡œ ê·¸ëŒ€ë¡œ ê³„ì‚°)
    for name, buffer in model.named_buffers():
        total_size += buffer.nelement() * buffer.element_size()
    
    return total_size / 1024 / 1024

def get_peak_memory_mb():
    process = psutil.Process(os.getpid())
    return process.memory_info().rss / 1024 / 1024

def calculate_meteor(generated_caption, reference_caption):
    """METEOR ì ìˆ˜ ê³„ì‚°"""
    if not METEOR_AVAILABLE:
        return None
    try:
        gen_words = [w for w in generated_caption if w not in ['<start>', '<end>', '<pad>', '<unk>']]
        ref_words = word_tokenize(reference_caption.lower())
        gen_words_str = ' '.join(gen_words)
        if not gen_words_str:
            return None
        gen_tokens = word_tokenize(gen_words_str.lower())
        score = meteor_score([ref_words], gen_tokens)
        return score
    except Exception:
        return None

# ============================================================================
# ë°ì´í„° ë¡œë“œ
# ============================================================================
def load_base_model():
    """í•™ìŠµëœ ëª¨ë¸ ë¡œë“œ"""
    print("ğŸ“‚ ëª¨ë¸ ë¡œë“œ ì¤‘...")
    if not os.path.exists(MODEL_PATH):
        raise FileNotFoundError(f"ëª¨ë¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {MODEL_PATH}")
    
    checkpoint = torch.load(MODEL_PATH, map_location=device)
    
    if isinstance(checkpoint, dict):
        if 'model_state_dict' in checkpoint:
            model_state = checkpoint['model_state_dict']
            vocab_size = checkpoint.get('vocab_size', 1000)
            word_map = checkpoint.get('word_map', {})
            rev_word_map = checkpoint.get('rev_word_map', {})
        else:
            model_state = checkpoint
            vocab_size = 1000
            word_map = {}
            rev_word_map = {}
    else:
        model_state = checkpoint
        vocab_size = 1000
        word_map = {}
        rev_word_map = {}
    
    embed_dim = 300
    model = MobileNetCaptioningModel(vocab_size=vocab_size, embed_dim=embed_dim)
    model.load_state_dict(model_state)
    model.eval()
    model.to(device)
    
    print(f"âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ (Vocab Size: {vocab_size})")
    return model, word_map, rev_word_map

def load_data():
    """í…ŒìŠ¤íŠ¸ ì´ë¯¸ì§€ì™€ ì°¸ì¡° ìº¡ì…˜ ë¡œë“œ"""
    img_tensor = None
    filename = None
    ref_caption = None
    
    if os.path.exists(TEST_IMAGE_DIR):
        files = [f for f in os.listdir(TEST_IMAGE_DIR) if f.lower().endswith(('.jpg', '.png', '.jpeg'))]
        if files:
            import random
            filename = random.choice(files)
            img_path = os.path.join(TEST_IMAGE_DIR, filename)
            try:
                img = Image.open(img_path).convert('RGB')
                img_tensor = transform(img).unsqueeze(0).to(device)
                print(f"ğŸ“¸ í…ŒìŠ¤íŠ¸ ì´ë¯¸ì§€: {filename}")
            except Exception as e:
                print(f"âš ï¸ ì´ë¯¸ì§€ ë¡œë“œ ì¤‘ ì—ëŸ¬: {e}")
    
    if img_tensor is None:
        print("âš ï¸ ì´ë¯¸ì§€ë¥¼ ì°¾ì„ ìˆ˜ ì—†ì–´ ë”ë¯¸ ë°ì´í„°ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")
        img_tensor = torch.randn(1, 3, 224, 224).to(device)
        filename = "dummy"
        ref_caption = "a test image"
    else:
        if os.path.exists(CAPTIONS_FILE) and filename != "dummy":
            with open(CAPTIONS_FILE, 'r', encoding='utf-8') as f:
                for line in f:
                    if filename in line:
                        if ',' in line:
                            parts = line.split(',', 1)
                            if len(parts) == 2:
                                ref_caption = parts[1].strip()
                                print(f"ğŸ“ ì°¸ì¡° ìº¡ì…˜: {ref_caption}")
                                break
    
    return img_tensor, ref_caption

# ============================================================================
# Pruning í•¨ìˆ˜
# ============================================================================
def apply_magnitude_pruning(model, pruning_rate):
    """Magnitude-based Pruning ì ìš© (ê°€ì¤‘ì¹˜ í¬ê¸° ê¸°ë°˜)"""
    pruned_model = deepcopy(model)
    pruned_model.eval()
    
    # í”„ë£¨ë‹í•  ë ˆì´ì–´ ì°¾ê¸° (Linear ë ˆì´ì–´ë§Œ)
    modules_to_prune = []
    for name, module in pruned_model.named_modules():
        if isinstance(module, nn.Linear):
            modules_to_prune.append((module, 'weight'))
    
    # Magnitude-based pruning ì ìš©
    for module, param_name in modules_to_prune:
        prune.l1_unstructured(module, name=param_name, amount=pruning_rate)
    
    # Pruning ì˜êµ¬ ì ìš© (0ìœ¼ë¡œ ë§Œë“¤ê¸°)
    for module, param_name in modules_to_prune:
        prune.remove(module, param_name)
    
    # ì‹¤ì œë¡œ 0ì¸ ê°€ì¤‘ì¹˜ë¥¼ ì œê±°í•˜ì—¬ ëª¨ë¸ í¬ê¸° ê°ì†Œ
    # ì£¼ì˜: ì´ëŠ” ë©”ëª¨ë¦¬ìƒì—ì„œë§Œ íš¨ê³¼ê°€ ìˆê³ , ì‹¤ì œ ëª¨ë¸ êµ¬ì¡°ëŠ” ë³€ê²½ë˜ì§€ ì•ŠìŒ
    # ì‹¤ì œ ë°°í¬ ì‹œì—ëŠ” sparse formatìœ¼ë¡œ ì €ì¥/ë¡œë“œí•´ì•¼ í•¨
    print(f"   âœ‚ï¸ Pruning ì™„ë£Œ: {pruning_rate*100:.0f}% ê°€ì¤‘ì¹˜ ì œê±°")
    
    return pruned_model

def apply_structured_pruning(model, pruning_rate):
    """Structured Pruning ì ìš© (ì±„ë„/í•„í„° ë‹¨ìœ„)"""
    pruned_model = deepcopy(model)
    pruned_model.eval()
    
    # Structured pruningì€ Linear ë ˆì´ì–´ì— ì ìš©
    modules_to_prune = []
    for name, module in pruned_model.named_modules():
        if isinstance(module, nn.Linear):
            modules_to_prune.append((module, 'weight'))
    
    # Structured pruning ì ìš© (ì±„ë„ ë‹¨ìœ„)
    for module, param_name in modules_to_prune:
        prune.ln_structured(module, name=param_name, amount=pruning_rate, n=2, dim=0)
    
    # Pruning ì˜êµ¬ ì ìš©
    for module, param_name in modules_to_prune:
        prune.remove(module, param_name)
    
    return pruned_model

def apply_global_pruning(model, pruning_rate):
    """Global Pruning ì ìš© (ì „ì²´ ëª¨ë¸ ê¸°ì¤€)"""
    pruned_model = deepcopy(model)
    pruned_model.eval()
    
    # í”„ë£¨ë‹í•  íŒŒë¼ë¯¸í„° ìˆ˜ì§‘
    parameters_to_prune = []
    for name, module in pruned_model.named_modules():
        if isinstance(module, nn.Linear):
            parameters_to_prune.append((module, 'weight'))
    
    # Global pruning ì ìš©
    prune.global_unstructured(
        parameters_to_prune,
        pruning_method=prune.L1Unstructured,
        amount=pruning_rate,
    )
    
    # Pruning ì˜êµ¬ ì ìš©
    for module, param_name in parameters_to_prune:
        prune.remove(module, param_name)
    
    return pruned_model

# ============================================================================
# ë²¤ì¹˜ë§ˆí¬ ì—”ì§„
# ============================================================================
def run_benchmark(model, img_tensor, wm, rwm, precision_name, ref_caption=None):
    """ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰"""
    print(f"\n[{precision_name}] ë²¤ì¹˜ë§ˆí¬ ì‹œì‘...")
    
    model_device = next(model.parameters()).device
    inp = img_tensor.clone().detach().to(model_device)
    
    # Warm-up
    with torch.no_grad():
        try:
            _ = model.generate(inp, wm, rwm, 20)
        except Exception as e:
            print(f"âš ï¸ Warm-up ì‹¤íŒ¨: {e}")
            return None
    
    # ì†ë„ ì¸¡ì •
    latencies = []
    start_mem = get_peak_memory_mb()
    peak_mem = start_mem
    
    for i in range(NUM_RUNS):
        gc.collect()
        if device.type == 'cuda': 
            torch.cuda.empty_cache()
            torch.cuda.synchronize()
        
        start = time.time()
        with torch.no_grad():
            gen_seq = model.generate(inp, wm, rwm, 20)
            
        if device.type == 'cuda': 
            torch.cuda.synchronize()
        
        latencies.append((time.time() - start) * 1000)
        
        current_mem = get_peak_memory_mb()
        peak_mem = max(peak_mem, current_mem)
        
        if (i + 1) % 10 == 0:
            print(f"   ì§„í–‰: {i+1}/{NUM_RUNS}")
    
    # METEOR ì ìˆ˜ ê³„ì‚°
    meteor_scores = []
    example_caption = "N/A"
    
    if ref_caption:
        for _ in range(5):
            with torch.no_grad():
                gen_seq = model.generate(inp, wm, rwm, 20)
            meteor = calculate_meteor(gen_seq, ref_caption)
            if meteor is not None:
                meteor_scores.append(meteor)
            if _ == 0:
                example_caption = ' '.join([w for w in gen_seq if w not in ['<start>', '<end>', '<pad>', '<unk>']])
    
    avg_meteor = np.mean(meteor_scores) if meteor_scores else None
    
    # ê²°ê³¼ ì •ë¦¬
    avg_time = np.mean(latencies)
    std_time = np.std(latencies)
    
    # Dense format í¬ê¸° (ë©”ëª¨ë¦¬ìƒ í¬ê¸°)
    size_mb_dense = get_model_size_mb(model, sparse=False)
    # Sparse format í¬ê¸° (ì‹¤ì œ ì €ì¥ í¬ê¸°)
    size_mb_sparse = get_sparse_model_size_mb(model)
    
    memory_usage = peak_mem - start_mem
    total_params, trainable_params = count_parameters(model)
    nonzero_params, _ = count_nonzero_parameters(model)
    sparsity = 1.0 - (nonzero_params / total_params) if total_params > 0 else 0.0
    
    print(f"   â±ï¸ í‰ê·  ì‹œê°„: {avg_time:.2f} Â± {std_time:.2f} ms")
    print(f"   ğŸ’¾ ëª¨ë¸ í¬ê¸° (Dense): {size_mb_dense:.2f} MB")
    print(f"   ğŸ’¾ ëª¨ë¸ í¬ê¸° (Sparse): {size_mb_sparse:.2f} MB")
    print(f"   ğŸ“‰ í¬ê¸° ê°ì†Œìœ¨: {(1 - size_mb_sparse/size_mb_dense)*100:.2f}%")
    print(f"   ğŸ“Š ì´ íŒŒë¼ë¯¸í„°: {total_params:,} (0ì´ ì•„ë‹Œ: {nonzero_params:,})")
    print(f"   âœ‚ï¸ Sparsity: {sparsity*100:.2f}%")
    print(f"   ğŸ§  ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: {memory_usage:.2f} MB")
    if avg_meteor is not None:
        print(f"   â­ METEOR: {avg_meteor:.4f}")
    print(f"   ğŸ“ ì˜ˆì‹œ ìº¡ì…˜: {example_caption}")
    
    return {
        'precision': precision_name,
        'mean_time_ms': avg_time,
        'std_time_ms': std_time,
        'min_time_ms': np.min(latencies),
        'max_time_ms': np.max(latencies),
        'model_size_mb': size_mb_sparse,  # Sparse format í¬ê¸° ì‚¬ìš©
        'model_size_mb_dense': size_mb_dense,  # Dense format í¬ê¸°ë„ ì €ì¥
        'memory_usage_mb': memory_usage,
        'meteor_score': avg_meteor,
        'inference_times': latencies,
        'example_caption': example_caption,
        'total_params': total_params,
        'nonzero_params': nonzero_params,
        'sparsity': sparsity,
        'trainable_params': trainable_params,
        'size_reduction': (1 - size_mb_sparse/size_mb_dense)*100 if size_mb_dense > 0 else 0
    }

# ============================================================================
# ì‹œê°í™”
# ============================================================================
def plot_pruning_comparison(results):
    """Pruning ê²°ê³¼ ë¹„êµ ê·¸ë˜í”„"""
    if not results:
        print("âŒ ê²°ê³¼ê°€ ì—†ì–´ plotì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    os.makedirs(OUTPUT_DIR, exist_ok=True)
    
    precisions = [r['precision'] for r in results]
    mean_times = [r['mean_time_ms'] for r in results]
    std_times = [r['std_time_ms'] for r in results]
    model_sizes = [r['model_size_mb'] for r in results]
    memory_usages = [r['memory_usage_mb'] for r in results]
    meteor_scores = [r.get('meteor_score', None) for r in results]
    sparsities = [r.get('sparsity', 0) * 100 for r in results]
    nonzero_params_list = [r.get('nonzero_params', 0) for r in results]
    
    valid_meteor_scores = [s for s in meteor_scores if s is not None]
    valid_meteor_precisions = [p for p, s in zip(precisions, meteor_scores) if s is not None]
    
    # ìƒ‰ìƒ ì„¤ì •
    colors = plt.cm.viridis(np.linspace(0, 1, len(precisions)))
    
    # ì¢…í•© ë¹„êµ ê·¸ë˜í”„
    if valid_meteor_scores:
        fig, axes = plt.subplots(3, 2, figsize=(14, 15))
        fig.suptitle('Pruning ì„±ëŠ¥ ë¹„êµ ì¢…í•©', fontsize=16, fontweight='bold')
        
        # 1. ì¶”ë¡  ì‹œê°„
        axes[0, 0].bar(precisions, mean_times, alpha=0.8, color=colors, yerr=std_times, capsize=5)
        axes[0, 0].set_ylabel('ì¶”ë¡  ì‹œê°„ (ms)', fontweight='bold')
        axes[0, 0].set_title('ì¶”ë¡  ì‹œê°„', fontweight='bold')
        axes[0, 0].grid(axis='y', alpha=0.3)
        axes[0, 0].tick_params(axis='x', rotation=45)
        for i, (p, m, s) in enumerate(zip(precisions, mean_times, std_times)):
            axes[0, 0].text(i, m + s + 1, f'{m:.1f}', ha='center', va='bottom', fontsize=9)
        
        # 2. ëª¨ë¸ í¬ê¸° (Sparse format - ì‹¤ì œ ì €ì¥ í¬ê¸°)
        axes[0, 1].bar(precisions, model_sizes, alpha=0.8, color=colors, label='Sparse (ì‹¤ì œ ì €ì¥ í¬ê¸°)')
        axes[0, 1].set_ylabel('ëª¨ë¸ í¬ê¸° (MB)', fontweight='bold')
        axes[0, 1].set_title('ëª¨ë¸ í¬ê¸° (Sparse Format)', fontweight='bold')
        axes[0, 1].grid(axis='y', alpha=0.3)
        axes[0, 1].tick_params(axis='x', rotation=45)
        axes[0, 1].legend()
        for i, (p, s) in enumerate(zip(precisions, model_sizes)):
            axes[0, 1].text(i, s + 0.5, f'{s:.1f}', ha='center', va='bottom', fontsize=9)
        
        # 3. Sparsity
        axes[1, 0].bar(precisions, sparsities, alpha=0.8, color=colors)
        axes[1, 0].set_ylabel('Sparsity (%)', fontweight='bold')
        axes[1, 0].set_title('Sparsity (í”„ë£¨ë‹ ë¹„ìœ¨)', fontweight='bold')
        axes[1, 0].grid(axis='y', alpha=0.3)
        axes[1, 0].tick_params(axis='x', rotation=45)
        for i, (p, s) in enumerate(zip(precisions, sparsities)):
            axes[1, 0].text(i, s + 1, f'{s:.1f}%', ha='center', va='bottom', fontsize=9)
        
        # 4. METEOR ì ìˆ˜
        axes[1, 1].bar(valid_meteor_precisions, valid_meteor_scores, alpha=0.8, 
                     color=colors[:len(valid_meteor_scores)])
        axes[1, 1].set_ylabel('METEOR ì ìˆ˜', fontweight='bold')
        axes[1, 1].set_title('METEOR ì ìˆ˜ (ìº¡ì…˜ í’ˆì§ˆ)', fontweight='bold')
        axes[1, 1].set_ylim(0, 1.0)
        axes[1, 1].grid(axis='y', alpha=0.3)
        axes[1, 1].tick_params(axis='x', rotation=45)
        for i, (p, s) in enumerate(zip(valid_meteor_precisions, valid_meteor_scores)):
            axes[1, 1].text(i, s + 0.01, f'{s:.2f}', ha='center', va='bottom', fontsize=9)
        
        # 5. ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰
        axes[2, 0].bar(precisions, memory_usages, alpha=0.8, color=colors)
        axes[2, 0].set_ylabel('ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ (MB)', fontweight='bold')
        axes[2, 0].set_title('ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰', fontweight='bold')
        axes[2, 0].grid(axis='y', alpha=0.3)
        axes[2, 0].tick_params(axis='x', rotation=45)
        for i, (p, m) in enumerate(zip(precisions, memory_usages)):
            axes[2, 0].text(i, m + max(memory_usages) * 0.02, f'{m:.1f}', ha='center', va='bottom', fontsize=9)
        
        # 6. 0ì´ ì•„ë‹Œ íŒŒë¼ë¯¸í„° ê°œìˆ˜
        nonzero_params_m = [p / 1e6 for p in nonzero_params_list]
        axes[2, 1].bar(precisions, nonzero_params_m, alpha=0.8, color=colors)
        axes[2, 1].set_ylabel('0ì´ ì•„ë‹Œ íŒŒë¼ë¯¸í„° (M)', fontweight='bold')
        axes[2, 1].set_title('0ì´ ì•„ë‹Œ íŒŒë¼ë¯¸í„° ê°œìˆ˜', fontweight='bold')
        axes[2, 1].grid(axis='y', alpha=0.3)
        axes[2, 1].tick_params(axis='x', rotation=45)
        for i, (p, np_m) in enumerate(zip(precisions, nonzero_params_m)):
            axes[2, 1].text(i, np_m + max(nonzero_params_m) * 0.02, f'{np_m:.2f}M', ha='center', va='bottom', fontsize=9)
    
    plt.tight_layout()
    plt.savefig(os.path.join(OUTPUT_DIR, 'pruning_comparison_comprehensive.png'), 
                dpi=300, bbox_inches='tight')
    print(f"âœ… Plot ì €ì¥: {os.path.join(OUTPUT_DIR, 'pruning_comparison_comprehensive.png')}")
    plt.close()

# ============================================================================
# Main
# ============================================================================
def main():
    os.makedirs(OUTPUT_DIR, exist_ok=True)
    
    print("="*70)
    print("=== Pruning ë²¤ì¹˜ë§ˆí¬ ===")
    print("="*70)
    
    # 1. ëª¨ë¸ ë° ë°ì´í„° ë¡œë“œ
    base_model, wm, rwm = load_base_model()
    img_tensor, ref_caption = load_data()
    
    results = []
    
    # 2. ì›ë³¸ ëª¨ë¸ ë²¤ì¹˜ë§ˆí¬ (Baseline)
    print("\n" + "="*70)
    print("=== [Baseline] ì›ë³¸ ëª¨ë¸ ===")
    print("="*70)
    result_baseline = run_benchmark(base_model, img_tensor, wm, rwm, "Original (Baseline)", ref_caption)
    if result_baseline:
        results.append(result_baseline)
    
    # 3. ë‹¤ì–‘í•œ Pruning Rateë¡œ í…ŒìŠ¤íŠ¸
    for pruning_rate in PRUNING_RATES:
        # Magnitude-based Pruning
        print("\n" + "="*70)
        print(f"=== Magnitude Pruning ({pruning_rate*100:.0f}%) ===")
        print("="*70)
        try:
            pruned_model = apply_magnitude_pruning(base_model, pruning_rate)
            pruned_model.to(device)
            result = run_benchmark(
                pruned_model, img_tensor, wm, rwm, 
                f"Magnitude-{pruning_rate*100:.0f}%", ref_caption
            )
            if result:
                results.append(result)
            del pruned_model
            gc.collect()
        except Exception as e:
            print(f"âš ï¸ Magnitude Pruning ({pruning_rate*100:.0f}%) ì‹¤íŒ¨: {e}")
        
        # Structured Pruning
        print("\n" + "="*70)
        print(f"=== Structured Pruning ({pruning_rate*100:.0f}%) ===")
        print("="*70)
        try:
            pruned_model = apply_structured_pruning(base_model, pruning_rate)
            pruned_model.to(device)
            result = run_benchmark(
                pruned_model, img_tensor, wm, rwm, 
                f"Structured-{pruning_rate*100:.0f}%", ref_caption
            )
            if result:
                results.append(result)
            del pruned_model
            gc.collect()
        except Exception as e:
            print(f"âš ï¸ Structured Pruning ({pruning_rate*100:.0f}%) ì‹¤íŒ¨: {e}")
    
    # 4. Global Pruning í…ŒìŠ¤íŠ¸
    print("\n" + "="*70)
    print("=== Global Pruning (50%) ===")
    print("="*70)
    try:
        pruned_model = apply_global_pruning(base_model, 0.5)
        pruned_model.to(device)
        result = run_benchmark(pruned_model, img_tensor, wm, rwm, "Global-50%", ref_caption)
        if result:
            results.append(result)
        del pruned_model
        gc.collect()
    except Exception as e:
        print(f"âš ï¸ Global Pruning ì‹¤íŒ¨: {e}")
    
    # 5. ê²°ê³¼ ìš”ì•½ ì¶œë ¥
    print("\n" + "="*70)
    print("=== ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼ ìš”ì•½ ===")
    print("="*70)
    if any(r.get('meteor_score') is not None for r in results):
        print(f"{'Method':<25} {'ì¶”ë¡ ì‹œê°„(ms)':<15} {'ëª¨ë¸í¬ê¸°(MB)':<15} {'í¬ê¸°ê°ì†Œ(%)':<15} {'Sparsity(%)':<15} {'METEOR':<10}")
        print("-"*100)
        for result in results:
            meteor_str = f"{result.get('meteor_score', 0):.4f}" if result.get('meteor_score') is not None else "N/A"
            sparsity = result.get('sparsity', 0) * 100
            size_reduction = result.get('size_reduction', 0)
            print(f"{result['precision']:<25} "
                  f"{result['mean_time_ms']:.2f}Â±{result['std_time_ms']:.2f}    "
                  f"{result['model_size_mb']:.2f}          "
                  f"{size_reduction:<15.2f} "
                  f"{sparsity:<15.2f} "
                  f"{meteor_str}")
    else:
        print(f"{'Method':<25} {'ì¶”ë¡ ì‹œê°„(ms)':<15} {'ëª¨ë¸í¬ê¸°(MB)':<15} {'í¬ê¸°ê°ì†Œ(%)':<15} {'Sparsity(%)':<15}")
        print("-"*85)
        for result in results:
            sparsity = result.get('sparsity', 0) * 100
            size_reduction = result.get('size_reduction', 0)
            print(f"{result['precision']:<25} "
                  f"{result['mean_time_ms']:.2f}Â±{result['std_time_ms']:.2f}    "
                  f"{result['model_size_mb']:.2f}          "
                  f"{size_reduction:<15.2f} "
                  f"{sparsity:<15.2f}")
    
    # 6. ì‹œê°í™”
    print("\n" + "="*70)
    print("Plot ìƒì„± ì¤‘...")
    print("="*70)
    plot_pruning_comparison(results)
    
    print("\n" + "="*70)
    print("=== ë²¤ì¹˜ë§ˆí¬ ì™„ë£Œ ===")
    print(f"ê²°ê³¼ ì €ì¥ ìœ„ì¹˜: {OUTPUT_DIR}")
    print("="*70)

if __name__ == "__main__":
    main()

