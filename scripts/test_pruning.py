"""
Pruning ë²¤ì¹˜ë§ˆí¬ ìŠ¤í¬ë¦½íŠ¸
ë‹¤ì–‘í•œ Pruning ê¸°ë²•ì„ ì ìš©í•˜ê³  ì„±ëŠ¥ì„ ë¹„êµí•©ë‹ˆë‹¤.
"""
import torch
import torch.nn as nn
import torch.nn.utils.prune as prune
import numpy as np
import os
import time
import matplotlib.pyplot as plt
from copy import deepcopy
import gc
import warnings
from PIL import Image

warnings.filterwarnings('ignore')

# ê³µí†µ ìœ í‹¸ë¦¬í‹° import
from src.utils import (
    setup_device,
    setup_matplotlib,
    get_image_transform,
    count_parameters,
    get_model_size_mb,
    get_peak_memory_mb,
    calculate_meteor,
    load_test_data,
    load_base_model,
    TEST_IMAGE_DIR,
    CAPTIONS_FILE,
)

# ============================================================================
# ì„¤ì •
# ============================================================================
setup_matplotlib()

OUTPUT_DIR = "pruning_results"
NUM_RUNS = 50

# Pruning ì„¤ì •
PRUNING_RATES = [0.1, 0.3, 0.5, 0.7]  # 10%, 30%, 50%, 70% í”„ë£¨ë‹
PRUNING_METHODS = ['magnitude', 'structured']  # í”„ë£¨ë‹ ë°©ë²•

# ë””ë°”ì´ìŠ¤ ì„ íƒ
device = setup_device()

# ì´ë¯¸ì§€ ì „ì²˜ë¦¬
transform = get_image_transform()

# ============================================================================
# Pruning ì „ìš© ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜
# ============================================================================
def count_nonzero_parameters(model):
    """0ì´ ì•„ë‹Œ íŒŒë¼ë¯¸í„° ê°œìˆ˜ ê³„ì‚° (í”„ë£¨ë‹ í›„)"""
    nonzero_params = 0
    total_params = 0
    for param in model.parameters():
        total_params += param.numel()
        nonzero_params += param.nonzero().size(0) if param.numel() > 0 else 0
    return nonzero_params, total_params

def convert_to_sparse_model(model):
    """Pruningëœ ëª¨ë¸ì„ ì‹¤ì œë¡œ sparse formatìœ¼ë¡œ ë³€í™˜í•˜ì—¬ í¬ê¸° ê°ì†Œ"""
    # ì£¼ì˜: ì‹¤ì œë¡œ ëª¨ë¸ êµ¬ì¡°ë¥¼ ë³€ê²½í•˜ëŠ” ê²ƒì€ ë³µì¡í•˜ë¯€ë¡œ
    # ì—¬ê¸°ì„œëŠ” ê°€ì¤‘ì¹˜ë¥¼ sparse tensorë¡œ ë³€í™˜í•˜ëŠ” ëŒ€ì‹ 
    # ì‹¤ì œ 0ì´ ì•„ë‹Œ íŒŒë¼ë¯¸í„°ë§Œ ê³„ì‚°í•˜ëŠ” ë°©ì‹ ì‚¬ìš©
    # ì‹¤ì œ ë°°í¬ ì‹œì—ëŠ” sparse formatìœ¼ë¡œ ì €ì¥/ë¡œë“œí•˜ëŠ” ê²ƒì´ ì¢‹ìŠµë‹ˆë‹¤
    return model

def save_sparse_model(model, path):
    """ëª¨ë¸ì„ sparse formatìœ¼ë¡œ ì €ì¥ (ì‹¤ì œ í¬ê¸° ê°ì†Œ)"""
    state_dict = {}
    for name, param in model.named_parameters():
        if param.numel() > 0:
            # 0ì´ ì•„ë‹Œ ê°’ë§Œ ì €ì¥
            nonzero_mask = param != 0
            if nonzero_mask.any():
                # Sparse formatìœ¼ë¡œ ì €ì¥
                sparse_param = param[nonzero_mask]
                indices = nonzero_mask.nonzero(as_tuple=False)
                state_dict[name] = {
                    'values': sparse_param.cpu(),
                    'indices': indices.cpu(),
                    'shape': list(param.shape),
                    'dtype': str(param.dtype)
                }
            else:
                # ëª¨ë“  ê°’ì´ 0ì¸ ê²½ìš°
                state_dict[name] = {
                    'values': torch.tensor([], dtype=param.dtype),
                    'indices': torch.tensor([], dtype=torch.long),
                    'shape': list(param.shape),
                    'dtype': str(param.dtype)
                }
        else:
            state_dict[name] = param.cpu()
    
    # ë²„í¼ë„ ì €ì¥
    for name, buffer in model.named_buffers():
        state_dict[name] = buffer.cpu()
    
    torch.save(state_dict, path)
    print(f"   ğŸ’¾ Sparse ëª¨ë¸ ì €ì¥: {path}")

def get_sparse_model_size_mb(model):
    """Sparse formatìœ¼ë¡œ ì €ì¥í–ˆì„ ë•Œì˜ ì‹¤ì œ ëª¨ë¸ í¬ê¸° ê³„ì‚°"""
    total_size = 0
    
    for name, param in model.named_parameters():
        if param.numel() > 0:
            # 0ì´ ì•„ë‹Œ ê°’ì˜ ê°œìˆ˜
            nonzero_count = (param != 0).sum().item()
            total_params = param.numel()
            
            if nonzero_count > 0:
                # ê°’ ì €ì¥ (0ì´ ì•„ë‹Œ ê°’ë§Œ)
                total_size += nonzero_count * param.element_size()
                
                # ì¸ë±ìŠ¤ ì €ì¥ (COO format: Coordinate format)
                # ê° 0ì´ ì•„ë‹Œ ê°’ì˜ ìœ„ì¹˜ë¥¼ ì €ì¥
                if len(param.shape) == 1:
                    # 1D: ì¸ë±ìŠ¤ë§Œ
                    indices_size = nonzero_count * 4  # 4 bytes per index
                elif len(param.shape) == 2:
                    # 2D: (row, col) ìŒ
                    indices_size = nonzero_count * 2 * 4  # 2 indices per value
                else:
                    # ë‹¤ì°¨ì›: ëª¨ë“  ì°¨ì›ì˜ ì¸ë±ìŠ¤
                    indices_size = nonzero_count * len(param.shape) * 4
                
                total_size += indices_size
                
                # ë©”íƒ€ë°ì´í„° (shape, dtype, nonzero_count ë“±)
                total_size += 64  # ë©”íƒ€ë°ì´í„° ì˜¤ë²„í—¤ë“œ
            else:
                # ëª¨ë“  ê°’ì´ 0ì¸ ê²½ìš° ìµœì†Œ ë©”íƒ€ë°ì´í„°ë§Œ
                total_size += 32
    
    # ë²„í¼ í¬ê¸° (ë²„í¼ëŠ” ë³´í†µ ì‘ìœ¼ë¯€ë¡œ ê·¸ëŒ€ë¡œ ê³„ì‚°)
    for name, buffer in model.named_buffers():
        total_size += buffer.nelement() * buffer.element_size()
    
    return total_size / 1024 / 1024

# ============================================================================
# ë°ì´í„° ë¡œë“œ (ê³µí†µ ëª¨ë“ˆ ì‚¬ìš©)
# ============================================================================
# load_base_model, load_test_dataëŠ” utilsì—ì„œ import

# ============================================================================
# Pruning í•¨ìˆ˜ (ë¬¼ë¦¬ì  êµ¬ì¡° ìˆ˜ì •)
# ============================================================================
def get_pruning_mask(weight, pruning_rate, dim=0, use_l2=False):
    """í”„ë£¨ë‹ ë§ˆìŠ¤í¬ ìƒì„± (ì œê±°í•  ì±„ë„/ë‰´ëŸ° ì‹ë³„)
    
    Args:
        weight: ê°€ì¤‘ì¹˜ í…ì„œ
        pruning_rate: í”„ë£¨ë‹ ë¹„ìœ¨
        dim: í”„ë£¨ë‹í•  ì°¨ì› (0: ì¶œë ¥, 1: ì…ë ¥)
        use_l2: Trueë©´ L2 norm ì‚¬ìš© (Structured), Falseë©´ L1 norm ì‚¬ìš© (Magnitude)
    """
    if dim == 0:  # ì¶œë ¥ ì°¨ì› í”„ë£¨ë‹
        if use_l2:
            # Structured: L2 norm ì‚¬ìš© (ì±„ë„ ë‹¨ìœ„ ì¤‘ìš”ë„)
            importance = torch.norm(weight, p=2, dim=1)  # [out_features] - L2 norm
        else:
            # Magnitude: L1 norm ì‚¬ìš©
            importance = torch.abs(weight).sum(dim=1)  # [out_features] - L1 norm
    else:  # ì…ë ¥ ì°¨ì› í”„ë£¨ë‹
        if use_l2:
            # Structured: L2 norm ì‚¬ìš©
            importance = torch.norm(weight, p=2, dim=0)  # [in_features] - L2 norm
        else:
            # Magnitude: L1 norm ì‚¬ìš©
            importance = torch.abs(weight).sum(dim=0)  # [in_features] - L1 norm
    
    # ì¤‘ìš”ë„ê°€ ë‚®ì€ ìˆœì„œë¡œ ì •ë ¬
    num_to_prune = int(pruning_rate * importance.numel())
    if num_to_prune == 0:
        return torch.ones(importance.numel(), dtype=torch.bool, device=weight.device)
    
    _, indices = torch.sort(importance)
    mask = torch.ones(importance.numel(), dtype=torch.bool, device=weight.device)
    mask[indices[:num_to_prune]] = False
    
    return mask

def apply_structured_pruning_physical(model, pruning_rate):
    """Structured Pruning ì ìš© (ë¬¼ë¦¬ì  êµ¬ì¡° ë³€ê²½ - ì‹¤ì œ ì±„ë„/ë‰´ëŸ° ì œê±°)"""
    from src.gru_model.model import LightweightCaptionDecoder
    
    pruned_model = deepcopy(model)
    pruned_model.eval()
    
    decoder = pruned_model.decoder
    
    # 1. decoder_dim ì°¨ì› í”„ë£¨ë‹ (ê°€ì¥ ì˜í–¥ì´ í° ì°¨ì›)
    # decoder_dimì€ decoder_att, init_h, fc, decode_stepì—ì„œ ì‚¬ìš©ë¨
    
    # decoder_attì˜ ì…ë ¥ ì°¨ì›(decoder_dim)ì„ ê¸°ì¤€ìœ¼ë¡œ í”„ë£¨ë‹
    if hasattr(decoder, 'decoder_att'):
        old_decoder_att = decoder.decoder_att
        weight = old_decoder_att.weight.data  # [attention_dim, decoder_dim]
        
        # decoder_dim ì°¨ì›ì—ì„œ í”„ë£¨ë‹ (ì…ë ¥ ì°¨ì›)
        # Structured pruning: L2 norm ì‚¬ìš©
        mask_decoder_dim = get_pruning_mask(weight, pruning_rate, dim=1, use_l2=True)
        new_decoder_dim = mask_decoder_dim.sum().item()
        
        # decoder_att ë ˆì´ì–´ ì¬ìƒì„±
        new_decoder_att = nn.Linear(new_decoder_dim, decoder.attention_dim)
        new_decoder_att.weight.data = weight[:, mask_decoder_dim]  # [attention_dim, new_decoder_dim]
        if old_decoder_att.bias is not None:
            new_decoder_att.bias.data = old_decoder_att.bias.data.clone()
        decoder.decoder_att = new_decoder_att
        
        # decoder_dim ì—…ë°ì´íŠ¸
        decoder.decoder_dim = new_decoder_dim
        
        # init_h ë ˆì´ì–´ ì¡°ì • (ì¶œë ¥ ì°¨ì›ì´ decoder_dim)
        if hasattr(decoder, 'init_h'):
            old_init_h = decoder.init_h
            old_weight = old_init_h.weight.data  # [decoder_dim, encoder_dim]
            new_init_h = nn.Linear(decoder.encoder_dim, new_decoder_dim)
            new_init_h.weight.data = old_weight[mask_decoder_dim, :]  # [new_decoder_dim, encoder_dim]
            if old_init_h.bias is not None:
                new_init_h.bias.data = old_init_h.bias.data[mask_decoder_dim]
            decoder.init_h = new_init_h
        
        # fc ë ˆì´ì–´ ì¡°ì • (ì…ë ¥ ì°¨ì›ì´ decoder_dim)
        if hasattr(decoder, 'fc'):
            old_fc = decoder.fc
            old_weight = old_fc.weight.data  # [vocab_size, decoder_dim]
            new_fc = nn.Linear(new_decoder_dim, decoder.vocab_size)
            new_fc.weight.data = old_weight[:, mask_decoder_dim]  # [vocab_size, new_decoder_dim]
            if old_fc.bias is not None:
                new_fc.bias.data = old_fc.bias.data.clone()
            decoder.fc = new_fc
        
        # decode_step (GRUCell) ì¡°ì •
        # GRUCellì˜ hidden_sizeê°€ decoder_dimì´ë¯€ë¡œ ì¬ìƒì„± í•„ìš”
        if hasattr(decoder, 'decode_step'):
            from src.gru_model.model import LightweightCaptionDecoder
            # GRUCell: input_size = embed_dim + encoder_dim, hidden_size = decoder_dim
            old_decode_step = decoder.decode_step
            input_size = old_decode_step.input_size
            new_decode_step = nn.GRUCell(input_size, new_decoder_dim)
            
            # ê°€ì¤‘ì¹˜ ë³µì‚¬ (ê°€ëŠ¥í•œ ë¶€ë¶„ë§Œ)
            old_hidden_size = old_decode_step.hidden_size
            if new_decoder_dim <= old_hidden_size:
                # weight_ih: [3 * hidden_size, input_size]
                # weight_hh: [3 * hidden_size, hidden_size]
                old_weight_ih = old_decode_step.weight_ih.data  # [3 * old_hidden_size, input_size]
                old_weight_hh = old_decode_step.weight_hh.data  # [3 * old_hidden_size, old_hidden_size]
                
                # ê° ê²Œì´íŠ¸ë³„ë¡œ ë§ˆìŠ¤í¬ ì ìš©
                gate_size = old_hidden_size
                new_gate_size = new_decoder_dim
                mask_gates = mask_decoder_dim.repeat(3)  # [3 * decoder_dim]
                
                new_weight_ih = old_weight_ih[mask_gates, :]  # [3 * new_decoder_dim, input_size]
                new_weight_hh = old_weight_hh[mask_gates, :][:, mask_decoder_dim]  # [3 * new_decoder_dim, new_decoder_dim]
                
                new_decode_step.weight_ih.data = new_weight_ih
                new_decode_step.weight_hh.data = new_weight_hh
                
                if old_decode_step.bias_ih is not None:
                    old_bias_ih = old_decode_step.bias_ih.data
                    new_decode_step.bias_ih.data = old_bias_ih[mask_gates]
                if old_decode_step.bias_hh is not None:
                    old_bias_hh = old_decode_step.bias_hh.data
                    new_decode_step.bias_hh.data = old_bias_hh[mask_gates]
            
            decoder.decode_step = new_decode_step
    
    # 2. attention_dim ì°¨ì› í”„ë£¨ë‹ (ì„ íƒì )
    if hasattr(decoder, 'encoder_att') and hasattr(decoder, 'full_att'):
        # encoder_attì˜ ì¶œë ¥ ì°¨ì›(attention_dim) í”„ë£¨ë‹
        old_encoder_att = decoder.encoder_att
        weight = old_encoder_att.weight.data  # [attention_dim, encoder_dim]
        
        # Structured pruning: L2 norm ì‚¬ìš©
        mask_attention_dim = get_pruning_mask(weight, pruning_rate, dim=0, use_l2=True)
        new_attention_dim = mask_attention_dim.sum().item()
        
        # encoder_att ë ˆì´ì–´ ì¬ìƒì„±
        new_encoder_att = nn.Linear(decoder.encoder_dim, new_attention_dim)
        new_encoder_att.weight.data = weight[mask_attention_dim, :]  # [new_attention_dim, encoder_dim]
        if old_encoder_att.bias is not None:
            new_encoder_att.bias.data = old_encoder_att.bias.data[mask_attention_dim]
        decoder.encoder_att = new_encoder_att
        
        # attention_dim ì—…ë°ì´íŠ¸
        decoder.attention_dim = new_attention_dim
        
        # decoder_attì˜ ì¶œë ¥ ì°¨ì›ë„ ì¡°ì •
        if hasattr(decoder, 'decoder_att'):
            old_decoder_att = decoder.decoder_att
            old_weight = old_decoder_att.weight.data  # [old_attention_dim, decoder_dim]
            new_decoder_att = nn.Linear(decoder.decoder_dim, new_attention_dim)
            new_decoder_att.weight.data = old_weight[mask_attention_dim, :]  # [new_attention_dim, decoder_dim]
            if old_decoder_att.bias is not None:
                new_decoder_att.bias.data = old_decoder_att.bias.data[mask_attention_dim]
            decoder.decoder_att = new_decoder_att
        
        # full_attì˜ ì…ë ¥ ì°¨ì› ì¡°ì •
        if hasattr(decoder, 'full_att'):
            old_full_att = decoder.full_att
            old_weight = old_full_att.weight.data  # [1, old_attention_dim]
            new_full_att = nn.Linear(new_attention_dim, 1)
            new_full_att.weight.data = old_weight[:, mask_attention_dim]  # [1, new_attention_dim]
            if old_full_att.bias is not None:
                new_full_att.bias.data = old_full_att.bias.data.clone()
            decoder.full_att = new_full_att
    
    pruned_model.decoder = decoder
    pruned_model.eval()
    
    # íŒŒë¼ë¯¸í„° ê°œìˆ˜ í™•ì¸
    old_params = sum(p.numel() for p in model.parameters())
    new_params = sum(p.numel() for p in pruned_model.parameters())
    reduction = (1 - new_params / old_params) * 100
    
    print(f"   âœ‚ï¸ ë¬¼ë¦¬ì  êµ¬ì¡° Pruning ì™„ë£Œ: {pruning_rate*100:.0f}% ì±„ë„ ì œê±°")
    print(f"   ğŸ“Š íŒŒë¼ë¯¸í„° ê°ì†Œ: {old_params:,} â†’ {new_params:,} ({reduction:.1f}% ê°ì†Œ)")
    
    return pruned_model

def apply_magnitude_pruning(model, pruning_rate):
    """Magnitude-based Pruning ì ìš© (ê°€ì¤‘ì¹˜ í¬ê¸° ê¸°ë°˜, ë¬¼ë¦¬ì  êµ¬ì¡° ë³€ê²½)
    
    Magnitude pruningì€ ê° ë ˆì´ì–´ì˜ ê°€ì¤‘ì¹˜ ì ˆëŒ“ê°’(magnitude)ì„ ê¸°ì¤€ìœ¼ë¡œ
    ì¤‘ìš”ë„ê°€ ë‚®ì€ ì±„ë„/ë‰´ëŸ°ì„ ì œê±°í•©ë‹ˆë‹¤.
    """
    from src.gru_model.model import LightweightCaptionDecoder
    
    pruned_model = deepcopy(model)
    pruned_model.eval()
    
    decoder = pruned_model.decoder
    
    # Magnitude-based pruning: ê° ë ˆì´ì–´ì˜ ê°€ì¤‘ì¹˜ ì ˆëŒ“ê°’ì„ ê¸°ì¤€ìœ¼ë¡œ ì¤‘ìš”ë„ ê³„ì‚°
    # ëª¨ë“  ë ˆì´ì–´ì— ì¼ê´€ë˜ê²Œ ì ìš©
    
    # 1. decoder_dim ì°¨ì› í”„ë£¨ë‹ (Magnitude ê¸°ë°˜)
    # decoder_attì˜ ê°€ì¤‘ì¹˜ë¥¼ magnitude ê¸°ì¤€ìœ¼ë¡œ í‰ê°€
    if hasattr(decoder, 'decoder_att'):
        old_decoder_att = decoder.decoder_att
        weight = old_decoder_att.weight.data  # [attention_dim, decoder_dim]
        
        # Magnitude ê¸°ë°˜ ì¤‘ìš”ë„ ê³„ì‚°: ê° decoder_dim ì±„ë„ì˜ L1 norm
        # ê° ì…ë ¥ ì±„ë„(decoder_dim)ì˜ ëª¨ë“  ì¶œë ¥ì— ëŒ€í•œ ê°€ì¤‘ì¹˜ í•©
        importance = torch.abs(weight).sum(dim=0)  # [decoder_dim] - ê° ì…ë ¥ ì±„ë„ì˜ ì¤‘ìš”ë„
        
        # ì¤‘ìš”ë„ê°€ ë‚®ì€ ìˆœì„œë¡œ ì •ë ¬
        num_to_prune = int(pruning_rate * importance.numel())
        if num_to_prune > 0 and num_to_prune < importance.numel():
            _, indices = torch.sort(importance)
            mask_decoder_dim = torch.ones(importance.numel(), dtype=torch.bool, device=weight.device)
            mask_decoder_dim[indices[:num_to_prune]] = False
        else:
            mask_decoder_dim = torch.ones(importance.numel(), dtype=torch.bool, device=weight.device)
        
        new_decoder_dim = mask_decoder_dim.sum().item()
        
        # decoder_att ë ˆì´ì–´ ì¬ìƒì„±
        new_decoder_att = nn.Linear(new_decoder_dim, decoder.attention_dim)
        new_decoder_att.weight.data = weight[:, mask_decoder_dim]  # [attention_dim, new_decoder_dim]
        if old_decoder_att.bias is not None:
            new_decoder_att.bias.data = old_decoder_att.bias.data.clone()
        decoder.decoder_att = new_decoder_att
        
        # decoder_dim ì—…ë°ì´íŠ¸
        decoder.decoder_dim = new_decoder_dim
        
        # init_h ë ˆì´ì–´ ì¡°ì • (ì¶œë ¥ ì°¨ì›ì´ decoder_dim)
        if hasattr(decoder, 'init_h'):
            old_init_h = decoder.init_h
            old_weight = old_init_h.weight.data  # [decoder_dim, encoder_dim]
            new_init_h = nn.Linear(decoder.encoder_dim, new_decoder_dim)
            new_init_h.weight.data = old_weight[mask_decoder_dim, :]  # [new_decoder_dim, encoder_dim]
            if old_init_h.bias is not None:
                new_init_h.bias.data = old_init_h.bias.data[mask_decoder_dim]
            decoder.init_h = new_init_h
        
        # fc ë ˆì´ì–´ ì¡°ì • (ì…ë ¥ ì°¨ì›ì´ decoder_dim)
        if hasattr(decoder, 'fc'):
            old_fc = decoder.fc
            old_weight = old_fc.weight.data  # [vocab_size, decoder_dim]
            new_fc = nn.Linear(new_decoder_dim, decoder.vocab_size)
            new_fc.weight.data = old_weight[:, mask_decoder_dim]  # [vocab_size, new_decoder_dim]
            if old_fc.bias is not None:
                new_fc.bias.data = old_fc.bias.data.clone()
            decoder.fc = new_fc
        
        # decode_step (GRUCell) ì¡°ì •
        if hasattr(decoder, 'decode_step'):
            old_decode_step = decoder.decode_step
            input_size = old_decode_step.input_size
            new_decode_step = nn.GRUCell(input_size, new_decoder_dim)
            
            old_hidden_size = old_decode_step.hidden_size
            if new_decoder_dim <= old_hidden_size:
                old_weight_ih = old_decode_step.weight_ih.data  # [3 * old_hidden_size, input_size]
                old_weight_hh = old_decode_step.weight_hh.data  # [3 * old_hidden_size, old_hidden_size]
                
                # ê° ê²Œì´íŠ¸ë³„ë¡œ ë§ˆìŠ¤í¬ ì ìš©
                mask_gates = mask_decoder_dim.repeat(3)  # [3 * decoder_dim]
                
                new_weight_ih = old_weight_ih[mask_gates, :]  # [3 * new_decoder_dim, input_size]
                new_weight_hh = old_weight_hh[mask_gates, :][:, mask_decoder_dim]  # [3 * new_decoder_dim, new_decoder_dim]
                
                new_decode_step.weight_ih.data = new_weight_ih
                new_decode_step.weight_hh.data = new_weight_hh
                
                if old_decode_step.bias_ih is not None:
                    old_bias_ih = old_decode_step.bias_ih.data
                    new_decode_step.bias_ih.data = old_bias_ih[mask_gates]
                if old_decode_step.bias_hh is not None:
                    old_bias_hh = old_decode_step.bias_hh.data
                    new_decode_step.bias_hh.data = old_bias_hh[mask_gates]
            
            decoder.decode_step = new_decode_step
    
    # 2. attention_dim ì°¨ì› í”„ë£¨ë‹ (Magnitude ê¸°ë°˜)
    if hasattr(decoder, 'encoder_att') and hasattr(decoder, 'full_att'):
        old_encoder_att = decoder.encoder_att
        weight = old_encoder_att.weight.data  # [attention_dim, encoder_dim]
        
        # Magnitude ê¸°ë°˜ ì¤‘ìš”ë„ ê³„ì‚°: ê° attention_dim ì¶œë ¥ ì±„ë„ì˜ L1 norm
        importance = torch.abs(weight).sum(dim=1)  # [attention_dim] - ê° ì¶œë ¥ ì±„ë„ì˜ ì¤‘ìš”ë„
        
        # ì¤‘ìš”ë„ê°€ ë‚®ì€ ìˆœì„œë¡œ ì •ë ¬
        num_to_prune = int(pruning_rate * importance.numel())
        if num_to_prune > 0 and num_to_prune < importance.numel():
            _, indices = torch.sort(importance)
            mask_attention_dim = torch.ones(importance.numel(), dtype=torch.bool, device=weight.device)
            mask_attention_dim[indices[:num_to_prune]] = False
        else:
            mask_attention_dim = torch.ones(importance.numel(), dtype=torch.bool, device=weight.device)
        
        new_attention_dim = mask_attention_dim.sum().item()
        
        # encoder_att ë ˆì´ì–´ ì¬ìƒì„±
        new_encoder_att = nn.Linear(decoder.encoder_dim, new_attention_dim)
        new_encoder_att.weight.data = weight[mask_attention_dim, :]  # [new_attention_dim, encoder_dim]
        if old_encoder_att.bias is not None:
            new_encoder_att.bias.data = old_encoder_att.bias.data[mask_attention_dim]
        decoder.encoder_att = new_encoder_att
        
        # attention_dim ì—…ë°ì´íŠ¸
        decoder.attention_dim = new_attention_dim
        
        # decoder_attì˜ ì¶œë ¥ ì°¨ì›ë„ ì¡°ì •
        if hasattr(decoder, 'decoder_att'):
            old_decoder_att = decoder.decoder_att
            old_weight = old_decoder_att.weight.data  # [old_attention_dim, decoder_dim]
            new_decoder_att = nn.Linear(decoder.decoder_dim, new_attention_dim)
            new_decoder_att.weight.data = old_weight[mask_attention_dim, :]  # [new_attention_dim, decoder_dim]
            if old_decoder_att.bias is not None:
                new_decoder_att.bias.data = old_decoder_att.bias.data[mask_attention_dim]
            decoder.decoder_att = new_decoder_att
        
        # full_attì˜ ì…ë ¥ ì°¨ì› ì¡°ì •
        if hasattr(decoder, 'full_att'):
            old_full_att = decoder.full_att
            old_weight = old_full_att.weight.data  # [1, old_attention_dim]
            new_full_att = nn.Linear(new_attention_dim, 1)
            new_full_att.weight.data = old_weight[:, mask_attention_dim]  # [1, new_attention_dim]
            if old_full_att.bias is not None:
                new_full_att.bias.data = old_full_att.bias.data.clone()
            decoder.full_att = new_full_att
    
    pruned_model.decoder = decoder
    pruned_model.eval()
    
    # íŒŒë¼ë¯¸í„° ê°œìˆ˜ í™•ì¸
    old_params = sum(p.numel() for p in model.parameters())
    new_params = sum(p.numel() for p in pruned_model.parameters())
    reduction = (1 - new_params / old_params) * 100
    
    print(f"   âœ‚ï¸ Magnitude-based Pruning ì™„ë£Œ: {pruning_rate*100:.0f}% ì±„ë„ ì œê±°")
    print(f"   ğŸ“Š íŒŒë¼ë¯¸í„° ê°ì†Œ: {old_params:,} â†’ {new_params:,} ({reduction:.1f}% ê°ì†Œ)")
    
    return pruned_model

def apply_structured_pruning(model, pruning_rate):
    """Structured Pruning ì ìš© (ì±„ë„/í•„í„° ë‹¨ìœ„, ë¬¼ë¦¬ì  êµ¬ì¡° ë³€ê²½)"""
    return apply_structured_pruning_physical(model, pruning_rate)

def apply_global_pruning(model, pruning_rate):
    """Global Pruning ì ìš© (ì „ì²´ ëª¨ë¸ ê¸°ì¤€, ë¬¼ë¦¬ì  êµ¬ì¡° ë³€ê²½)"""
    # Global pruningë„ structured ë°©ì‹ìœ¼ë¡œ ì ìš©
    return apply_structured_pruning_physical(model, pruning_rate)


# ============================================================================
# ë²¤ì¹˜ë§ˆí¬ ì—”ì§„
# ============================================================================
def run_benchmark(model, img_tensor, wm, rwm, precision_name, ref_caption=None, baseline_params=None):
    """ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰
    
    Args:
        model: ëª¨ë¸
        img_tensor: ì…ë ¥ ì´ë¯¸ì§€
        wm: word_map
        rwm: rev_word_map
        precision_name: ì •ë°€ë„ ì´ë¦„
        ref_caption: ì°¸ì¡° ìº¡ì…˜
        baseline_params: Baseline ëª¨ë¸ì˜ íŒŒë¼ë¯¸í„° ê°œìˆ˜ (Sparsity ê³„ì‚°ìš©)
    """
    print(f"\n[{precision_name}] ë²¤ì¹˜ë§ˆí¬ ì‹œì‘...")
    
    model_device = next(model.parameters()).device
    inp = img_tensor.clone().detach().to(model_device)
    
    # Warm-up
    with torch.no_grad():
        try:
            _ = model.generate(inp, wm, rwm, 20)
        except Exception as e:
            print(f"âš ï¸ Warm-up ì‹¤íŒ¨: {e}")
            return None
    
    # ì†ë„ ë° ë©”ëª¨ë¦¬ ì¸¡ì • (ì¶”ë¡  ê³¼ì •ë§Œ)
    latencies = []
    time_per_tokens = []  # í† í°ë‹¹ ì¶”ë¡  ì‹œê°„
    memory_usages = []  # ê° ì¶”ë¡ ì˜ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰
    
    # CUDA ë©”ëª¨ë¦¬ ì¸¡ì • ì¤€ë¹„
    if device.type == 'cuda':
        torch.cuda.reset_peak_memory_stats()
        torch.cuda.empty_cache()
    
    for i in range(NUM_RUNS):
        gc.collect()
        
        # ë©”ëª¨ë¦¬ ì¸¡ì • ì¤€ë¹„ (ì‹œê°„ ì¸¡ì • ì „)
        if device.type == 'cuda': 
            torch.cuda.empty_cache()
            torch.cuda.synchronize()  # ì´ì „ ì‘ì—… ì™„ë£Œ ëŒ€ê¸°
            torch.cuda.reset_peak_memory_stats()  # í”¼í¬ ë©”ëª¨ë¦¬ í†µê³„ ì´ˆê¸°í™”
            mem_before = torch.cuda.memory_allocated() / 1024 / 1024  # MB
        else:
            mem_before = get_peak_memory_mb()
        
        # ì¶”ë¡  ì‹œê°„ ì¸¡ì • ì‹œì‘
        if device.type == 'cuda':
            torch.cuda.synchronize()  # ì¶”ë¡  ì „ ë™ê¸°í™” (ì´ì „ ì‘ì—… ì™„ë£Œ ë³´ì¥)
        
        start = time.time()
        
        # ì¶”ë¡  ì‹¤í–‰
        with torch.no_grad():
            gen_seq = model.generate(inp, wm, rwm, 20)
        
        # CUDAì˜ ê²½ìš° ë¹„ë™ê¸° ì‹¤í–‰ ì™„ë£Œ ëŒ€ê¸° (ì¶”ë¡  ì‹œê°„ì— í¬í•¨)
        if device.type == 'cuda': 
            torch.cuda.synchronize()
        
        # ì¶”ë¡  ì‹œê°„ ì¸¡ì • ì¢…ë£Œ
        inference_time = (time.time() - start) * 1000  # ms
        
        # ìƒì„±ëœ í† í° ê¸¸ì´ ê³„ì‚° (ì´ë¯¸ gen_seqê°€ ìƒì„±ë¨)
        token_length = len([w for w in gen_seq if w not in ['<start>', '<end>', '<pad>', '<unk>']])
        if token_length == 0:
            token_length = 1  # 0ìœ¼ë¡œ ë‚˜ëˆ„ê¸° ë°©ì§€
        
        # í† í°ë‹¹ í‰ê·  ì¶”ë¡  ì‹œê°„ ê³„ì‚°
        time_per_token = inference_time / token_length
        
        latencies.append(inference_time)
        time_per_tokens.append(time_per_token)
        
        # ë©”ëª¨ë¦¬ ì¸¡ì • (ì‹œê°„ ì¸¡ì • í›„)
        if device.type == 'cuda': 
            # ì‹¤ì œ ì‚¬ìš©ëœ ë©”ëª¨ë¦¬ (í”¼í¬ ë©”ëª¨ë¦¬ ì‚¬ìš©)
            mem_used = torch.cuda.max_memory_allocated() / 1024 / 1024  # MB
        else:
            # CPU/MPS: ì¶”ë¡  í›„ ë©”ëª¨ë¦¬
            mem_after = get_peak_memory_mb()
            mem_used = max(0, mem_after - mem_before)  # ì°¨ì´ë§Œ ê³„ì‚°
        
        memory_usages.append(mem_used)
        
        if (i + 1) % 10 == 0:
            print(f"   ì§„í–‰: {i+1}/{NUM_RUNS}")
    
    # METEOR ì ìˆ˜ ê³„ì‚° (10ê°œ ì´ë¯¸ì§€ë¡œ ì¸¡ì •)
    meteor_scores = []
    example_caption = "N/A"
    
    # 10ê°œì˜ ì´ë¯¸ì§€ë¡œ METEOR ì ìˆ˜ ì¸¡ì •
    test_images_meteor = []
    test_captions_meteor = []
    
    # í…ŒìŠ¤íŠ¸ ì´ë¯¸ì§€ ë””ë ‰í† ë¦¬ì—ì„œ 10ê°œ ì´ë¯¸ì§€ ë¡œë“œ
    if os.path.exists(TEST_IMAGE_DIR):
        image_files = [f for f in os.listdir(TEST_IMAGE_DIR) 
                      if f.lower().endswith(('.jpg', '.png', '.jpeg'))]
        if image_files:
            import random
            # ìµœëŒ€ 10ê°œ ì´ë¯¸ì§€ ì„ íƒ
            selected_files = random.sample(image_files, min(10, len(image_files)))
            
            # ê° ì´ë¯¸ì§€ì™€ ìº¡ì…˜ ë¡œë“œ
            for filename in selected_files:
                try:
                    img_path = os.path.join(TEST_IMAGE_DIR, filename)
                    img = Image.open(img_path).convert('RGB')
                    img_tensor_meteor = transform(img).unsqueeze(0).to(model_device)
                    test_images_meteor.append(img_tensor_meteor)
                    
                    # ì°¸ì¡° ìº¡ì…˜ ë¡œë“œ
                    ref_cap = None
                    if os.path.exists(CAPTIONS_FILE):
                        with open(CAPTIONS_FILE, 'r', encoding='utf-8') as f:
                            lines = f.readlines()
                            for line in lines:
                                if ',' in line:
                                    parts = line.split(',', 1)
                                    if len(parts) == 2 and parts[0].strip() == filename:
                                        ref_cap = parts[1].strip()
                                        break
                    
                    if ref_cap:
                        test_captions_meteor.append(ref_cap)
                    else:
                        test_captions_meteor.append(None)
                except Exception as e:
                    print(f"   âš ï¸ ì´ë¯¸ì§€ ë¡œë“œ ì‹¤íŒ¨ ({filename}): {e}")
                    continue
    
    # ì´ë¯¸ì§€ê°€ ë¶€ì¡±í•˜ë©´ ë”ë¯¸ ë°ì´í„°ë¡œ ì±„ì›€
    while len(test_images_meteor) < 10:
        dummy_img = torch.randn(1, 3, 224, 224).to(model_device)
        test_images_meteor.append(dummy_img)
        test_captions_meteor.append("a test image")
    
    # 10ê°œ ì´ë¯¸ì§€ì— ëŒ€í•´ METEOR ì ìˆ˜ ê³„ì‚°
    if test_images_meteor and any(test_captions_meteor):
        print(f"   ğŸ“Š METEOR ì ìˆ˜ ì¸¡ì • ì¤‘: {len([c for c in test_captions_meteor if c])}ê°œ ì´ë¯¸ì§€")
        for idx, (test_img, ref_cap) in enumerate(zip(test_images_meteor[:10], test_captions_meteor[:10])):
            if ref_cap:
                with torch.no_grad():
                    gen_seq = model.generate(test_img, wm, rwm, 20)
                meteor = calculate_meteor(gen_seq, ref_cap)
                if meteor is not None:
                    meteor_scores.append(meteor)
                if idx == 0:
                    example_caption = ' '.join([w for w in gen_seq if w not in ['<start>', '<end>', '<pad>', '<unk>']])
    
    avg_meteor = np.mean(meteor_scores) if meteor_scores else None
    
    # ê²°ê³¼ ì •ë¦¬
    avg_time = np.mean(latencies)
    std_time = np.std(latencies)
    avg_time_per_token = np.mean(time_per_tokens)  # í† í°ë‹¹ í‰ê·  ì¶”ë¡  ì‹œê°„
    
    # Dense format í¬ê¸° (ë©”ëª¨ë¦¬ìƒ í¬ê¸°)
    size_mb_dense = get_model_size_mb(model, sparse=False)
    # Sparse format í¬ê¸° (ì‹¤ì œ ì €ì¥ í¬ê¸°)
    size_mb_sparse = get_sparse_model_size_mb(model)
    
    # ì¶”ë¡  ê³¼ì •ì—ì„œì˜ í‰ê·  ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰
    memory_usage = np.mean(memory_usages) if memory_usages else 0.0
    total_params, trainable_params = count_parameters(model)
    nonzero_params, _ = count_nonzero_parameters(model)
    
    # Sparsity ê³„ì‚°: ë¬¼ë¦¬ì  êµ¬ì¡° ë³€ê²½ í›„ì—ëŠ” baselineê³¼ ë¹„êµí•œ ì‹¤ì œ ê°ì†Œìœ¨
    if baseline_params is not None and baseline_params > 0:
        # Baseline ëŒ€ë¹„ ì‹¤ì œ íŒŒë¼ë¯¸í„° ê°ì†Œìœ¨
        sparsity = 1.0 - (total_params / baseline_params)
    else:
        # Baselineì´ ì—†ìœ¼ë©´ 0ì´ ì•„ë‹Œ íŒŒë¼ë¯¸í„° ê¸°ë°˜ ê³„ì‚° (ê¸°ì¡´ ë°©ì‹)
        sparsity = 1.0 - (nonzero_params / total_params) if total_params > 0 else 0.0
    
    print(f"   â±ï¸ í‰ê·  ì‹œê°„: {avg_time:.2f} Â± {std_time:.2f} ms")
    print(f"   ğŸ’¾ ëª¨ë¸ í¬ê¸° (Dense): {size_mb_dense:.2f} MB")
    print(f"   ğŸ’¾ ëª¨ë¸ í¬ê¸° (Sparse): {size_mb_sparse:.2f} MB")
    print(f"   ğŸ“‰ í¬ê¸° ê°ì†Œìœ¨: {(1 - size_mb_sparse/size_mb_dense)*100:.2f}%")
    print(f"   ğŸ“Š ì´ íŒŒë¼ë¯¸í„°: {total_params:,} (0ì´ ì•„ë‹Œ: {nonzero_params:,})")
    print(f"   âœ‚ï¸ Sparsity: {sparsity*100:.2f}%")
    print(f"   ğŸ§  ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: {memory_usage:.2f} MB")
    if avg_meteor is not None:
        print(f"   â­ METEOR: {avg_meteor:.4f}")
    print(f"   ğŸ“ ì˜ˆì‹œ ìº¡ì…˜: {example_caption}")
    
    return {
        'precision': precision_name,
        'mean_time_ms': avg_time,
        'std_time_ms': std_time,
        'min_time_ms': np.min(latencies),
        'max_time_ms': np.max(latencies),
        'mean_time_per_token_ms': avg_time_per_token,  # í† í°ë‹¹ í‰ê·  ì¶”ë¡  ì‹œê°„
        'model_size_mb': size_mb_sparse,  # Sparse format í¬ê¸° ì‚¬ìš©
        'model_size_mb_dense': size_mb_dense,  # Dense format í¬ê¸°ë„ ì €ì¥
        'memory_usage_mb': memory_usage,
        'meteor_score': avg_meteor,
        'inference_times': latencies,
        'example_caption': example_caption,
        'total_params': total_params,
        'nonzero_params': nonzero_params,
        'sparsity': sparsity,
        'trainable_params': trainable_params,
        'size_reduction': (1 - size_mb_sparse/size_mb_dense)*100 if size_mb_dense > 0 else 0
    }

# ============================================================================
# ì‹œê°í™”
# ============================================================================
def plot_pruning_comparison(results):
    """Pruning ê²°ê³¼ ë¹„êµ ê·¸ë˜í”„ (íŒŒì¸ íŠœë‹ ì œì™¸)"""
    if not results:
        print("âŒ ê²°ê³¼ê°€ ì—†ì–´ plotì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    os.makedirs(OUTPUT_DIR, exist_ok=True)
    
    # íŒŒì¸ íŠœë‹ëœ ê²°ê³¼ ì œì™¸
    filtered_results = [r for r in results if '(Fine-tuned)' not in r['precision']]
    
    if not filtered_results:
        print("âŒ íŒŒì¸ íŠœë‹ ì œì™¸ í›„ ê²°ê³¼ê°€ ì—†ì–´ plotì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    precisions = [r['precision'] for r in filtered_results]
    mean_times = [r['mean_time_ms'] for r in filtered_results]
    std_times = [r['std_time_ms'] for r in filtered_results]
    model_sizes = [r['model_size_mb'] for r in filtered_results]
    memory_usages = [r['memory_usage_mb'] for r in filtered_results]
    meteor_scores = [r.get('meteor_score', None) for r in filtered_results]
    sparsities = [r.get('sparsity', 0) * 100 for r in filtered_results]
    nonzero_params_list = [r.get('nonzero_params', 0) for r in filtered_results]
    
    valid_meteor_scores = [s for s in meteor_scores if s is not None]
    valid_meteor_precisions = [p for p, s in zip(precisions, meteor_scores) if s is not None]
    
    # ìƒ‰ìƒ ì„¤ì •
    colors = plt.cm.viridis(np.linspace(0, 1, len(precisions)))
    
    # ì¢…í•© ë¹„êµ ê·¸ë˜í”„
    if valid_meteor_scores:
        fig, axes = plt.subplots(3, 2, figsize=(14, 15))
        fig.suptitle('Pruning ì„±ëŠ¥ ë¹„êµ ì¢…í•©', fontsize=16, fontweight='bold')
        
        # 1. ì¶”ë¡  ì‹œê°„
        axes[0, 0].bar(precisions, mean_times, alpha=0.8, color=colors, yerr=std_times, capsize=5)
        axes[0, 0].set_ylabel('ì¶”ë¡  ì‹œê°„ (ms)', fontweight='bold')
        axes[0, 0].set_title('ì¶”ë¡  ì‹œê°„', fontweight='bold')
        axes[0, 0].grid(axis='y', alpha=0.3)
        axes[0, 0].tick_params(axis='x', rotation=45)
        for i, (p, m, s) in enumerate(zip(precisions, mean_times, std_times)):
            axes[0, 0].text(i, m + s + 1, f'{m:.1f}', ha='center', va='bottom', fontsize=9)
        
        # 2. ëª¨ë¸ í¬ê¸° (Sparse format - ì‹¤ì œ ì €ì¥ í¬ê¸°)
        axes[0, 1].bar(precisions, model_sizes, alpha=0.8, color=colors, label='Sparse (ì‹¤ì œ ì €ì¥ í¬ê¸°)')
        axes[0, 1].set_ylabel('ëª¨ë¸ í¬ê¸° (MB)', fontweight='bold')
        axes[0, 1].set_title('ëª¨ë¸ í¬ê¸° (Sparse Format)', fontweight='bold')
        axes[0, 1].grid(axis='y', alpha=0.3)
        axes[0, 1].tick_params(axis='x', rotation=45)
        axes[0, 1].legend()
        for i, (p, s) in enumerate(zip(precisions, model_sizes)):
            axes[0, 1].text(i, s + 0.5, f'{s:.1f}', ha='center', va='bottom', fontsize=9)
        
        # 3. Sparsity (íŒŒë¼ë¯¸í„° ê°ì†Œìœ¨)
        axes[1, 0].bar(precisions, sparsities, alpha=0.8, color=colors)
        axes[1, 0].set_ylabel('íŒŒë¼ë¯¸í„° ê°ì†Œìœ¨ (%)', fontweight='bold')
        axes[1, 0].set_title('íŒŒë¼ë¯¸í„° ê°ì†Œìœ¨ (Baseline ëŒ€ë¹„)', fontweight='bold')
        axes[1, 0].grid(axis='y', alpha=0.3)
        axes[1, 0].tick_params(axis='x', rotation=45)
        # Yì¶• ë²”ìœ„ ì„¤ì • (0-100%)
        axes[1, 0].set_ylim(0, max(sparsities) * 1.2 if sparsities else 100)
        for i, (p, s) in enumerate(zip(precisions, sparsities)):
            axes[1, 0].text(i, s + max(sparsities) * 0.02 if sparsities else 1, 
                          f'{s:.1f}%', ha='center', va='bottom', fontsize=9)
        
        # 4. METEOR ì ìˆ˜
        axes[1, 1].bar(valid_meteor_precisions, valid_meteor_scores, alpha=0.8, 
                     color=colors[:len(valid_meteor_scores)])
        axes[1, 1].set_ylabel('METEOR ì ìˆ˜', fontweight='bold')
        axes[1, 1].set_title('METEOR ì ìˆ˜ (ìº¡ì…˜ í’ˆì§ˆ)', fontweight='bold')
        axes[1, 1].set_ylim(0, 1.0)
        axes[1, 1].grid(axis='y', alpha=0.3)
        axes[1, 1].tick_params(axis='x', rotation=45)
        for i, (p, s) in enumerate(zip(valid_meteor_precisions, valid_meteor_scores)):
            axes[1, 1].text(i, s + 0.01, f'{s:.2f}', ha='center', va='bottom', fontsize=9)
        
        # 5. ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰
        axes[2, 0].bar(precisions, memory_usages, alpha=0.8, color=colors)
        axes[2, 0].set_ylabel('ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ (MB)', fontweight='bold')
        axes[2, 0].set_title('ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰', fontweight='bold')
        axes[2, 0].grid(axis='y', alpha=0.3)
        axes[2, 0].tick_params(axis='x', rotation=45)
        for i, (p, m) in enumerate(zip(precisions, memory_usages)):
            axes[2, 0].text(i, m + max(memory_usages) * 0.02, f'{m:.1f}', ha='center', va='bottom', fontsize=9)
        
        # 6. ì´ íŒŒë¼ë¯¸í„° ê°œìˆ˜ (ë¬¼ë¦¬ì  êµ¬ì¡° ë³€ê²½ í›„ ì‹¤ì œ íŒŒë¼ë¯¸í„° ìˆ˜)
        total_params_list = [r.get('total_params', 0) for r in filtered_results]
        total_params_m = [p / 1e6 for p in total_params_list]
        axes[2, 1].bar(precisions, total_params_m, alpha=0.8, color=colors)
        axes[2, 1].set_ylabel('ì´ íŒŒë¼ë¯¸í„° (M)', fontweight='bold')
        axes[2, 1].set_title('ì´ íŒŒë¼ë¯¸í„° ê°œìˆ˜ (ë¬¼ë¦¬ì  êµ¬ì¡° ë³€ê²½ í›„)', fontweight='bold')
        axes[2, 1].grid(axis='y', alpha=0.3)
        axes[2, 1].tick_params(axis='x', rotation=45)
        for i, (p, tp_m) in enumerate(zip(precisions, total_params_m)):
            axes[2, 1].text(i, tp_m + max(total_params_m) * 0.02 if total_params_m else 0.1, 
                          f'{tp_m:.2f}M', ha='center', va='bottom', fontsize=9)
    
    plt.tight_layout()
    plt.savefig(os.path.join(OUTPUT_DIR, 'pruning_comparison_comprehensive.png'), 
                dpi=300, bbox_inches='tight')
    print(f"âœ… Plot ì €ì¥: {os.path.join(OUTPUT_DIR, 'pruning_comparison_comprehensive.png')}")
    plt.close()

def plot_finetune_comparison(results, baseline_result):
    """íŒŒì¸ íŠœë‹ ê²°ê³¼ ë¹„êµ ê·¸ë˜í”„ (Baseline ëŒ€ë¹„)"""
    if not results or not baseline_result:
        print("âŒ ê²°ê³¼ê°€ ì—†ì–´ plotì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    os.makedirs(OUTPUT_DIR, exist_ok=True)
    
    # íŒŒì¸ íŠœë‹ëœ ê²°ê³¼ë§Œ í•„í„°ë§
    finetuned_results = [r for r in results if '(Fine-tuned)' in r['precision']]
    
    if not finetuned_results:
        print("âŒ íŒŒì¸ íŠœë‹ ê²°ê³¼ê°€ ì—†ì–´ plotì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    # Baseline ì •ë³´
    baseline_time_per_token = baseline_result.get('mean_time_per_token_ms', baseline_result['mean_time_ms'] / 10)  # ê¸°ë³¸ê°’
    baseline_meteor = baseline_result.get('meteor_score', 0)
    baseline_time = baseline_result['mean_time_ms']
    
    # ëª¨ë¸ë³„ë¡œ ê·¸ë£¹í™” (Magnitude, Structured, Global)
    model_groups = {}
    for result in finetuned_results:
        precision = result['precision']
        # "Magnitude-10% (Fine-tuned)" -> "Magnitude-10%"
        base_name = precision.replace(' (Fine-tuned)', '')
        
        if base_name not in model_groups:
            model_groups[base_name] = []
        model_groups[base_name].append(result)
    
    # ê·¸ë˜í”„ ìƒì„±
    fig, axes = plt.subplots(2, 2, figsize=(14, 12))
    fig.suptitle('íŒŒì¸ íŠœë‹ íš¨ê³¼ ë¹„êµ (Baseline ëŒ€ë¹„)', fontsize=16, fontweight='bold')
    
    # ë°ì´í„° ì¤€ë¹„
    model_names = []
    time_improvements = []  # Baseline ëŒ€ë¹„ ì¶”ë¡  ì‹œê°„ ê°œì„ ìœ¨
    time_per_token_improvements = []  # Baseline ëŒ€ë¹„ í† í°ë‹¹ ì‹œê°„ ê°œì„ ìœ¨
    meteor_improvements = []  # Baseline ëŒ€ë¹„ METEOR ì ìˆ˜ ê°œì„ ìœ¨
    model_sizes = []
    memory_usages = []
    
    for model_name, group_results in sorted(model_groups.items()):
        # ê° ê·¸ë£¹ì˜ í‰ê·  ê³„ì‚° (ê°™ì€ ëª¨ë¸ì´ ì—¬ëŸ¬ ê°œì¼ ìˆ˜ ìˆìŒ)
        avg_time = np.mean([r['mean_time_ms'] for r in group_results])
        avg_time_per_token = np.mean([r.get('mean_time_per_token_ms', r['mean_time_ms'] / 10) for r in group_results])
        avg_meteor = np.mean([r.get('meteor_score', 0) for r in group_results if r.get('meteor_score') is not None]) if any(r.get('meteor_score') for r in group_results) else None
        avg_size = np.mean([r['model_size_mb'] for r in group_results])
        avg_memory = np.mean([r['memory_usage_mb'] for r in group_results])
        
        model_names.append(model_name)
        
        # Baseline ëŒ€ë¹„ ê°œì„ ìœ¨ ê³„ì‚°
        time_improvement = ((baseline_time - avg_time) / baseline_time) * 100 if baseline_time > 0 else 0
        time_improvements.append(time_improvement)
        
        time_per_token_improvement = ((baseline_time_per_token - avg_time_per_token) / baseline_time_per_token) * 100 if baseline_time_per_token > 0 else 0
        time_per_token_improvements.append(time_per_token_improvement)
        
        if avg_meteor is not None and baseline_meteor > 0:
            meteor_improvement = ((avg_meteor - baseline_meteor) / baseline_meteor) * 100
        else:
            meteor_improvement = 0
        meteor_improvements.append(meteor_improvement)
        
        model_sizes.append(avg_size)
        memory_usages.append(avg_memory)
    
    # ìƒ‰ìƒ ì„¤ì •
    colors = plt.cm.viridis(np.linspace(0, 1, len(model_names)))
    
    # 1. ì¶”ë¡  ì‹œê°„ ê°œì„ ìœ¨ (Baseline ëŒ€ë¹„)
    axes[0, 0].bar(model_names, time_improvements, alpha=0.8, color=colors)
    axes[0, 0].axhline(y=0, color='black', linestyle='-', linewidth=0.5)
    axes[0, 0].set_ylabel('ê°œì„ ìœ¨ (%)', fontweight='bold')
    axes[0, 0].set_title('ì¶”ë¡  ì‹œê°„ ê°œì„ ìœ¨ (Baseline ëŒ€ë¹„)', fontweight='bold')
    axes[0, 0].grid(axis='y', alpha=0.3)
    axes[0, 0].tick_params(axis='x', rotation=45)
    for i, (name, imp) in enumerate(zip(model_names, time_improvements)):
        axes[0, 0].text(i, imp + (max(time_improvements) - min(time_improvements)) * 0.02 if time_improvements else 1,
                       f'{imp:+.1f}%', ha='center', va='bottom' if imp > 0 else 'top', fontsize=9)
    
    # 2. í† í°ë‹¹ ì¶”ë¡  ì‹œê°„ ê°œì„ ìœ¨ (Baseline ëŒ€ë¹„)
    axes[0, 1].bar(model_names, time_per_token_improvements, alpha=0.8, color=colors)
    axes[0, 1].axhline(y=0, color='black', linestyle='-', linewidth=0.5)
    axes[0, 1].set_ylabel('ê°œì„ ìœ¨ (%)', fontweight='bold')
    axes[0, 1].set_title('í† í°ë‹¹ ì¶”ë¡  ì‹œê°„ ê°œì„ ìœ¨ (Baseline ëŒ€ë¹„)', fontweight='bold')
    axes[0, 1].grid(axis='y', alpha=0.3)
    axes[0, 1].tick_params(axis='x', rotation=45)
    for i, (name, imp) in enumerate(zip(model_names, time_per_token_improvements)):
        axes[0, 1].text(i, imp + (max(time_per_token_improvements) - min(time_per_token_improvements)) * 0.02 if time_per_token_improvements else 1,
                       f'{imp:+.1f}%', ha='center', va='bottom' if imp > 0 else 'top', fontsize=9)
    
    # 3. METEOR ì ìˆ˜ ê°œì„ ìœ¨ (Baseline ëŒ€ë¹„)
    if any(m != 0 for m in meteor_improvements):
        axes[1, 0].bar(model_names, meteor_improvements, alpha=0.8, color=colors)
        axes[1, 0].axhline(y=0, color='black', linestyle='-', linewidth=0.5)
        axes[1, 0].set_ylabel('ê°œì„ ìœ¨ (%)', fontweight='bold')
        axes[1, 0].set_title('METEOR ì ìˆ˜ ê°œì„ ìœ¨ (Baseline ëŒ€ë¹„)', fontweight='bold')
        axes[1, 0].grid(axis='y', alpha=0.3)
        axes[1, 0].tick_params(axis='x', rotation=45)
        for i, (name, imp) in enumerate(zip(model_names, meteor_improvements)):
            axes[1, 0].text(i, imp + (max(meteor_improvements) - min(meteor_improvements)) * 0.02 if meteor_improvements else 1,
                           f'{imp:+.1f}%', ha='center', va='bottom' if imp > 0 else 'top', fontsize=9)
    else:
        axes[1, 0].text(0.5, 0.5, 'METEOR ì ìˆ˜ ë°ì´í„° ì—†ìŒ', 
                        ha='center', va='center', transform=axes[1, 0].transAxes)
        axes[1, 0].set_title('METEOR ì ìˆ˜ ê°œì„ ìœ¨ (Baseline ëŒ€ë¹„)', fontweight='bold')
    
    # 4. ëª¨ë¸ í¬ê¸° ë¹„êµ
    baseline_size = baseline_result['model_size_mb']
    size_reductions = [((baseline_size - size) / baseline_size) * 100 for size in model_sizes]
    axes[1, 1].bar(model_names, size_reductions, alpha=0.8, color=colors)
    axes[1, 1].set_ylabel('í¬ê¸° ê°ì†Œìœ¨ (%)', fontweight='bold')
    axes[1, 1].set_title('ëª¨ë¸ í¬ê¸° ê°ì†Œìœ¨ (Baseline ëŒ€ë¹„)', fontweight='bold')
    axes[1, 1].grid(axis='y', alpha=0.3)
    axes[1, 1].tick_params(axis='x', rotation=45)
    for i, (name, red) in enumerate(zip(model_names, size_reductions)):
        axes[1, 1].text(i, red + max(size_reductions) * 0.02 if size_reductions else 1,
                       f'{red:.1f}%', ha='center', va='bottom', fontsize=9)
    
    plt.tight_layout()
    plt.savefig(os.path.join(OUTPUT_DIR, 'pruning_comparison_finetune.png'), 
                dpi=300, bbox_inches='tight')
    print(f"âœ… íŒŒì¸ íŠœë‹ ë¹„êµ Plot ì €ì¥: {os.path.join(OUTPUT_DIR, 'pruning_comparison_finetune.png')}")
    plt.close()

# ============================================================================
# íŒŒì¸ íŠœë‹ í•¨ìˆ˜
# ============================================================================
def fine_tune_pruned_model(model, word_map, epochs=1):
    """í”„ë£¨ë‹ëœ ëª¨ë¸ì„ 1 epoch íŒŒì¸ íŠœë‹"""
    from torch.utils.data import DataLoader
    from src.utils import CaptionDataset
    
    print(f"\n   ğŸ”„ íŒŒì¸ íŠœë‹ ì‹œì‘ ({epochs} epoch)...")
    
    # í•™ìŠµ ë°ì´í„°ì…‹ ì¤€ë¹„
    try:
        dataset = CaptionDataset(
            images_dir=TEST_IMAGE_DIR,
            captions_file=CAPTIONS_FILE,
            transform=transform,
            word_map=word_map,
            max_len=50
        )
        
        if len(dataset) == 0:
            print("   âš ï¸ í•™ìŠµ ë°ì´í„°ê°€ ì—†ì–´ íŒŒì¸ íŠœë‹ì„ ê±´ë„ˆëœë‹ˆë‹¤.")
            return model
        
        dataloader = DataLoader(
            dataset,
            batch_size=4,
            shuffle=True,
            num_workers=0,
            pin_memory=False
        )
        
        print(f"   ğŸ“š í•™ìŠµ ë°ì´í„°: {len(dataset)}ê°œ ìƒ˜í”Œ")
        
        # ëª¨ë¸ì„ í•™ìŠµ ëª¨ë“œë¡œ ì „í™˜
        model.train()
        model.to(device)
        
        # Optimizer ë° Loss ì„¤ì •
        criterion = nn.CrossEntropyLoss(ignore_index=0)
        optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)
        vocab_size = len(word_map)
        
        # 1 epoch í•™ìŠµ
        total_loss = 0
        num_batches = 0
        
        for batch_idx, (imgs, caps) in enumerate(dataloader):
            if batch_idx >= 50:  # ìµœëŒ€ 50 ë°°ì¹˜ë§Œ í•™ìŠµ (ë¹ ë¥¸ íŒŒì¸ íŠœë‹)
                break
            
            imgs = imgs.to(device)
            caps = caps.to(device)
            
            optimizer.zero_grad()
            
            try:
                outputs, alphas = model(imgs, caps)
                targets = caps[:, 1:]
                outputs = outputs[:, :targets.shape[1], :]
                loss = criterion(outputs.reshape(-1, vocab_size), targets.reshape(-1))
                loss.backward()
                optimizer.step()
                
                total_loss += loss.item()
                num_batches += 1
            except Exception as e:
                print(f"   âš ï¸ ë°°ì¹˜ {batch_idx} í•™ìŠµ ì‹¤íŒ¨: {e}")
                continue
        
        if num_batches > 0:
            avg_loss = total_loss / num_batches
            print(f"   âœ… íŒŒì¸ íŠœë‹ ì™„ë£Œ (í‰ê·  Loss: {avg_loss:.4f})")
        
        model.eval()
        return model
        
    except Exception as e:
        print(f"   âš ï¸ íŒŒì¸ íŠœë‹ ì‹¤íŒ¨: {e}")
        import traceback
        traceback.print_exc()
        return model

# ============================================================================
# Main
# ============================================================================
def main():
    os.makedirs(OUTPUT_DIR, exist_ok=True)
    
    print("="*70)
    print("=== Pruning ë²¤ì¹˜ë§ˆí¬ ===")
    print("="*70)
    
    # 1. ëª¨ë¸ ë° ë°ì´í„° ë¡œë“œ
    base_model, wm, rwm = load_base_model(device=device)
    img_tensor, ref_caption = load_test_data(device=device, transform=transform)
    
    results = []
    
    # 2. ì›ë³¸ ëª¨ë¸ ë²¤ì¹˜ë§ˆí¬ (Baseline)
    print("\n" + "="*70)
    print("=== [Baseline] ì›ë³¸ ëª¨ë¸ ===")
    print("="*70)
    result_baseline = run_benchmark(base_model, img_tensor, wm, rwm, "Original (Baseline)", ref_caption)
    baseline_params = None
    if result_baseline:
        baseline_params = result_baseline['total_params']
        results.append(result_baseline)
    
    # 3. ë‹¤ì–‘í•œ Pruning Rateë¡œ í…ŒìŠ¤íŠ¸
    for pruning_rate in PRUNING_RATES:
        # Magnitude-based Pruning
        print("\n" + "="*70)
        print(f"=== Magnitude Pruning ({pruning_rate*100:.0f}%) ===")
        print("="*70)
        try:
            pruned_model = apply_magnitude_pruning(base_model, pruning_rate)
            pruned_model.to(device)
            
            # í”„ë£¨ë‹ í›„ ë²¤ì¹˜ë§ˆí¬
            result = run_benchmark(
                pruned_model, img_tensor, wm, rwm, 
                f"Magnitude-{pruning_rate*100:.0f}%", ref_caption, baseline_params=baseline_params
            )
            if result:
                results.append(result)
            
            # íŒŒì¸ íŠœë‹
            fine_tuned_model = fine_tune_pruned_model(pruned_model, wm, epochs=1)
            fine_tuned_model.to(device)
            
            # íŒŒì¸ íŠœë‹ í›„ ë²¤ì¹˜ë§ˆí¬
            result_finetuned = run_benchmark(
                fine_tuned_model, img_tensor, wm, rwm,
                f"Magnitude-{pruning_rate*100:.0f}% (Fine-tuned)", ref_caption, baseline_params=baseline_params
            )
            if result_finetuned:
                results.append(result_finetuned)
            
            del pruned_model, fine_tuned_model
            gc.collect()
        except Exception as e:
            print(f"âš ï¸ Magnitude Pruning ({pruning_rate*100:.0f}%) ì‹¤íŒ¨: {e}")
            import traceback
            traceback.print_exc()
        
        # Structured Pruning
        print("\n" + "="*70)
        print(f"=== Structured Pruning ({pruning_rate*100:.0f}%) ===")
        print("="*70)
        try:
            pruned_model = apply_structured_pruning(base_model, pruning_rate)
            pruned_model.to(device)
            
            # í”„ë£¨ë‹ í›„ ë²¤ì¹˜ë§ˆí¬
            result = run_benchmark(
                pruned_model, img_tensor, wm, rwm, 
                f"Structured-{pruning_rate*100:.0f}%", ref_caption, baseline_params=baseline_params
            )
            if result:
                results.append(result)
            
            # íŒŒì¸ íŠœë‹
            fine_tuned_model = fine_tune_pruned_model(pruned_model, wm, epochs=1)
            fine_tuned_model.to(device)
            
            # íŒŒì¸ íŠœë‹ í›„ ë²¤ì¹˜ë§ˆí¬
            result_finetuned = run_benchmark(
                fine_tuned_model, img_tensor, wm, rwm,
                f"Structured-{pruning_rate*100:.0f}% (Fine-tuned)", ref_caption, baseline_params=baseline_params
            )
            if result_finetuned:
                results.append(result_finetuned)
            
            del pruned_model, fine_tuned_model
            gc.collect()
        except Exception as e:
            print(f"âš ï¸ Structured Pruning ({pruning_rate*100:.0f}%) ì‹¤íŒ¨: {e}")
            import traceback
            traceback.print_exc()
    
    # 4. Global Pruning í…ŒìŠ¤íŠ¸
    print("\n" + "="*70)
    print("=== Global Pruning (50%) ===")
    print("="*70)
    try:
        pruned_model = apply_global_pruning(base_model, 0.5)
        pruned_model.to(device)
        
        # í”„ë£¨ë‹ í›„ ë²¤ì¹˜ë§ˆí¬
        result = run_benchmark(pruned_model, img_tensor, wm, rwm, "Global-50%", ref_caption, baseline_params=baseline_params)
        if result:
            results.append(result)
        
        # íŒŒì¸ íŠœë‹
        fine_tuned_model = fine_tune_pruned_model(pruned_model, wm, epochs=1)
        fine_tuned_model.to(device)
        
        # íŒŒì¸ íŠœë‹ í›„ ë²¤ì¹˜ë§ˆí¬
        result_finetuned = run_benchmark(
            fine_tuned_model, img_tensor, wm, rwm,
            "Global-50% (Fine-tuned)", ref_caption, baseline_params=baseline_params
        )
        if result_finetuned:
            results.append(result_finetuned)
        
        del pruned_model, fine_tuned_model
        gc.collect()
    except Exception as e:
        print(f"âš ï¸ Global Pruning ì‹¤íŒ¨: {e}")
        import traceback
        traceback.print_exc()
    
    # 5. ê²°ê³¼ ìš”ì•½ ì¶œë ¥
    print("\n" + "="*70)
    print("=== ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼ ìš”ì•½ ===")
    print("="*70)
    if any(r.get('meteor_score') is not None for r in results):
        print(f"{'Method':<25} {'ì¶”ë¡ ì‹œê°„(ms)':<15} {'ëª¨ë¸í¬ê¸°(MB)':<15} {'í¬ê¸°ê°ì†Œ(%)':<15} {'Sparsity(%)':<15} {'METEOR':<10}")
        print("-"*100)
        for result in results:
            meteor_str = f"{result.get('meteor_score', 0):.4f}" if result.get('meteor_score') is not None else "N/A"
            sparsity = result.get('sparsity', 0) * 100
            size_reduction = result.get('size_reduction', 0)
            print(f"{result['precision']:<25} "
                  f"{result['mean_time_ms']:.2f}Â±{result['std_time_ms']:.2f}    "
                  f"{result['model_size_mb']:.2f}          "
                  f"{size_reduction:<15.2f} "
                  f"{sparsity:<15.2f} "
                  f"{meteor_str}")
    else:
        print(f"{'Method':<25} {'ì¶”ë¡ ì‹œê°„(ms)':<15} {'ëª¨ë¸í¬ê¸°(MB)':<15} {'í¬ê¸°ê°ì†Œ(%)':<15} {'Sparsity(%)':<15}")
        print("-"*85)
        for result in results:
            sparsity = result.get('sparsity', 0) * 100
            size_reduction = result.get('size_reduction', 0)
            print(f"{result['precision']:<25} "
                  f"{result['mean_time_ms']:.2f}Â±{result['std_time_ms']:.2f}    "
                  f"{result['model_size_mb']:.2f}          "
                  f"{size_reduction:<15.2f} "
                  f"{sparsity:<15.2f}")
    
    # 6. ê²°ê³¼ ì €ì¥
    print("\n" + "="*70)
    print("ê²°ê³¼ ì €ì¥ ì¤‘...")
    print("="*70)
    
    # JSON í˜•ì‹ìœ¼ë¡œ ê²°ê³¼ ì €ì¥
    import json
    results_dict = {
        'baseline_params': baseline_params,
        'results': []
    }
    
    for result in results:
        result_dict = {
            'precision': result['precision'],
            'mean_time_ms': float(result['mean_time_ms']),
            'std_time_ms': float(result['std_time_ms']),
            'min_time_ms': float(result['min_time_ms']),
            'max_time_ms': float(result['max_time_ms']),
            'model_size_mb': float(result['model_size_mb']),
            'model_size_mb_dense': float(result.get('model_size_mb_dense', 0)),
            'memory_usage_mb': float(result['memory_usage_mb']),
            'meteor_score': float(result.get('meteor_score', 0)) if result.get('meteor_score') is not None else None,
            'total_params': int(result['total_params']),
            'trainable_params': int(result.get('trainable_params', 0)),
            'nonzero_params': int(result.get('nonzero_params', 0)),
            'sparsity': float(result.get('sparsity', 0)),
            'size_reduction': float(result.get('size_reduction', 0)),
            'example_caption': result.get('example_caption', 'N/A')
        }
        results_dict['results'].append(result_dict)
    
    results_json_path = os.path.join(OUTPUT_DIR, 'pruning_results.json')
    with open(results_json_path, 'w', encoding='utf-8') as f:
        json.dump(results_dict, f, indent=2, ensure_ascii=False)
    print(f"âœ… ê²°ê³¼ JSON ì €ì¥: {results_json_path}")
    
    # 7. ì‹œê°í™”
    print("\n" + "="*70)
    print("Plot ìƒì„± ì¤‘...")
    print("="*70)
    plot_pruning_comparison(results)
    
    # íŒŒì¸ íŠœë‹ ë¹„êµ ê·¸ë˜í”„ ìƒì„±
    if result_baseline:
        plot_finetune_comparison(results, result_baseline)
    
    print("\n" + "="*70)
    print("=== ë²¤ì¹˜ë§ˆí¬ ì™„ë£Œ ===")
    print(f"ê²°ê³¼ ì €ì¥ ìœ„ì¹˜: {OUTPUT_DIR}")
    print(f"  - JSON: {results_json_path}")
    print(f"  - Plot: {os.path.join(OUTPUT_DIR, 'pruning_comparison_comprehensive.png')}")
    print("="*70)

if __name__ == "__main__":
    main()

