"""
νμΈνλ‹ κ΄€λ ¨ ν—¬νΌ ν•¨μλ“¤
"""

import os
import torch


def load_checkpoint(label, device):
    """μ²΄ν¬ν¬μΈνΈ λ΅λ“ λ° μ‹μ‘ epoch λ°ν™"""
    checkpoint_files = [f for f in os.listdir("pruning_results") 
                        if f.startswith(label) and f.endswith('_checkpoint.pt')]
    
    if not checkpoint_files:
        return None, 0, None
    
    # κ°€μ¥ μµμ‹  μ²΄ν¬ν¬μΈνΈ μ°ΎκΈ°
    epoch_numbers = []
    for f in checkpoint_files:
        try:
            epoch_num = int(f.split('epoch_')[-1].replace('_checkpoint.pt', ''))
            epoch_numbers.append((epoch_num, f))
        except ValueError:
            continue
    
    if not epoch_numbers:
        return None, 0, None
    
    epoch_numbers.sort()
    latest_epoch, latest_file = epoch_numbers[-1]
    checkpoint_path = os.path.join("pruning_results", latest_file)
    
    try:
        checkpoint = torch.load(checkpoint_path, map_location=device)
        return checkpoint, latest_epoch, checkpoint_path
    except Exception as e:
        print(f"   β οΈ μ²΄ν¬ν¬μΈνΈ λ΅λ“ μ‹¤ν¨: {e}")
        return None, 0, None


def setup_training(model, learning_rate, device):
    """ν•™μµ μ„¤μ •: Encoder Freeze, Optimizer μƒμ„±"""
    # Encoder Freeze
    if hasattr(model, 'encoder'):
        for param in model.encoder.parameters():
            param.requires_grad = False
        print(f"   π”’ Encoder Freeze: CNN νλΌλ―Έν„° ν•™μµ κΈμ§€")
    
    # Optimizer μ„¤μ •
    criterion = torch.nn.CrossEntropyLoss(ignore_index=0)
    trainable_params = filter(lambda p: p.requires_grad, model.parameters())
    optimizer = torch.optim.Adam(trainable_params, lr=learning_rate)
    
    print(f"   π“ ν•™μµλ¥ : {learning_rate}")
    
    # ν•™μµν•  νλΌλ―Έν„° κ°μ
    trainable_count = sum(p.numel() for p in model.parameters() if p.requires_grad)
    total_count = sum(p.numel() for p in model.parameters())
    print(f"   π“ ν•™μµ λ€μƒ νλΌλ―Έν„°: {trainable_count:,} / {total_count:,} ({100*trainable_count/total_count:.1f}%)")
    
    return optimizer, criterion


def save_checkpoint(model, optimizer, epoch, label, avg_loss, avg_val_loss, meteor_score):
    """μ²΄ν¬ν¬μΈνΈ μ €μ¥"""
    os.makedirs("pruning_results", exist_ok=True)
    checkpoint_path = os.path.join("pruning_results", f"{label}_epoch_{epoch+1}_checkpoint.pt")
    
    checkpoint = {
        'epoch': epoch + 1,
        'model_state_dict': model.state_dict(),
        'optimizer_state_dict': optimizer.state_dict(),
        'avg_loss': avg_loss,
        'avg_val_loss': avg_val_loss,
        'meteor_score': meteor_score,
    }
    torch.save(checkpoint, checkpoint_path)
    
    print(f"      π’Ύ μ²΄ν¬ν¬μΈνΈ μ €μ¥ μ™„λ£")
    return checkpoint_path


def print_checkpoint_info(checkpoint, latest_epoch):
    """μ²΄ν¬ν¬μΈνΈ μ •λ³΄ μ¶λ ¥"""
    print(f"   π“‚ μ²΄ν¬ν¬μΈνΈ λ°κ²¬: Epoch {latest_epoch}")
    if checkpoint.get('avg_loss'):
        print(f"   π“ μ΄μ „ ν‰κ·  Loss: {checkpoint['avg_loss']:.4f}")


def restore_optimizer(optimizer, optimizer_state):
    """Optimizer State λ³µκµ¬"""
    if optimizer_state is None:
        return
    
    try:
        optimizer.load_state_dict(optimizer_state)
        print(f"   β… Optimizer State λ³µκµ¬ μ™„λ£")
    except Exception as e:
        print(f"   β οΈ Optimizer State λ³µκµ¬ μ‹¤ν¨: {e}")
