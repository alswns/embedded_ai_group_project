"""
QAT Fine-tuning ì „ìš© ìŠ¤í¬ë¦½íŠ¸
ì •ì  ì–‘ìí™” í›„ QAT fine-tuningì„ ì ìš©í•˜ê³  ê²°ê³¼ë¥¼ ë¹„êµí•©ë‹ˆë‹¤.
"""
import torch
import torch.nn as nn
from torch.quantization import quantize_fx
import numpy as np
import os
import time
import platform
import matplotlib.pyplot as plt
from copy import deepcopy
import gc
import warnings

warnings.filterwarnings('ignore')

# ê³µí†µ ìœ í‹¸ë¦¬í‹° import
from src.utils import (
    setup_device,
    setup_matplotlib,
    get_image_transform,
    count_parameters,
    get_model_size_mb,
    get_peak_memory_mb,
    calculate_meteor,
    CaptionDataset,
    load_test_data,
    prepare_calibration_dataset,
    load_base_model,
    TEST_IMAGE_DIR,
    CAPTIONS_FILE,
)

# ============================================================================
# ì„¤ì •
# ============================================================================
setup_matplotlib()

OUTPUT_DIR = "qat_results"
QAT_CHECKPOINT_DIR = os.path.join(OUTPUT_DIR, "checkpoints")
QAT_CHECKPOINT_PATH = os.path.join(QAT_CHECKPOINT_DIR, "qat_checkpoint.pth")
NUM_RUNS = 50

# QAT ì„¤ì •
QAT_EPOCHS = 30  # QAT í•™ìŠµ epoch ìˆ˜ (ë” ë§ì€ í•™ìŠµìœ¼ë¡œ ë” ë‚˜ì€ ê²°ê³¼)

# ë””ë°”ì´ìŠ¤ ì„ íƒ
device = setup_device()

# ì´ë¯¸ì§€ ì „ì²˜ë¦¬
transform = get_image_transform()

# ============================================================================
# ë°ì´í„° ë¡œë“œ (ê³µí†µ ëª¨ë“ˆ ì‚¬ìš©)
# ============================================================================
# load_base_model, load_test_data, prepare_calibration_datasetëŠ” utilsì—ì„œ import

# ============================================================================
# ì²´í¬í¬ì¸íŠ¸ ê´€ë¦¬
# ============================================================================
def save_qat_checkpoint(model, optimizer, epoch, loss_history, word_map, checkpoint_path):
    """QAT ì²´í¬í¬ì¸íŠ¸ ì €ì¥"""
    os.makedirs(os.path.dirname(checkpoint_path), exist_ok=True)
    
    checkpoint = {
        'epoch': epoch,
        'model_state_dict': model.state_dict(),
        'optimizer_state_dict': optimizer.state_dict(),
        'loss_history': loss_history,
        'word_map': word_map,
        'qat_epochs': QAT_EPOCHS,
    }
    
    torch.save(checkpoint, checkpoint_path)
    print(f"   ğŸ’¾ ì²´í¬í¬ì¸íŠ¸ ì €ì¥: {checkpoint_path} (Epoch {epoch})")

def load_qat_checkpoint(checkpoint_path, model, optimizer=None):
    """QAT ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ"""
    if not os.path.exists(checkpoint_path):
        return None, None, 0, []
    
    print(f"   ğŸ“‚ ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ ì¤‘: {checkpoint_path}")
    checkpoint = torch.load(checkpoint_path, map_location='cpu')
    
    start_epoch = checkpoint.get('epoch', 0)
    loss_history = checkpoint.get('loss_history', [])
    word_map = checkpoint.get('word_map', None)
    
    # ëª¨ë¸ ìƒíƒœ ë¡œë“œ
    try:
        model.load_state_dict(checkpoint['model_state_dict'], strict=False)
        print(f"   âœ… ëª¨ë¸ ìƒíƒœ ë¡œë“œ ì™„ë£Œ (Epoch {start_epoch})")
    except Exception as e:
        print(f"   âš ï¸ ëª¨ë¸ ìƒíƒœ ë¡œë“œ ì‹¤íŒ¨: {e}")
        return None, word_map, 0, []
    
    # Optimizer ìƒíƒœ ë¡œë“œ
    if optimizer is not None and 'optimizer_state_dict' in checkpoint:
        try:
            optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
            print(f"   âœ… Optimizer ìƒíƒœ ë¡œë“œ ì™„ë£Œ")
        except Exception as e:
            print(f"   âš ï¸ Optimizer ìƒíƒœ ë¡œë“œ ì‹¤íŒ¨: {e}")
    
    return model, word_map, start_epoch, loss_history

# ============================================================================
# Quantization í•¨ìˆ˜
# ============================================================================
def convert_to_int8_static(model, word_map=None):
    """Int8 Static Quantization"""
    print("   ğŸ‘‰ Int8 ì •ì  ì–‘ìí™” ì ìš© ì¤‘...")
    
    machine = platform.machine().lower()
    if 'arm' in machine or 'aarch64' in machine:
        backend = 'qnnpack'
    elif 'x86' in machine or 'amd64' in machine:
        backend = 'fbgemm'
    else:
        backend = 'qnnpack'
    
    torch.backends.quantized.engine = backend
    print(f"   âš™ï¸ Quantization Engine: {backend}")

    model_cpu = deepcopy(model).cpu()
    model_cpu.eval()

    if word_map is None:
        print("   âš ï¸ word_mapì´ ì—†ì–´ Dynamic Quantizationìœ¼ë¡œ fallback")
        return torch.quantization.quantize_dynamic(model_cpu, {nn.Linear}, dtype=torch.qint8)

    print("   ğŸ“Š Calibration ë°ì´í„° ì¤€ë¹„ ì¤‘...")
    cal_images, _ = prepare_calibration_dataset(word_map, num_samples=1000, transform=transform)
    example_input = cal_images[0]

    try:
        qconfig_dict = {"": torch.quantization.get_default_qconfig(backend)}
        
        print("   ğŸ”§ ì¸ì½”ë” ì¤€ë¹„ (Prepare FX)...")
        model_cpu.encoder = quantize_fx.prepare_fx(model_cpu.encoder, qconfig_dict, example_input)

        print("   ğŸ”„ Calibration ì§„í–‰ ì¤‘...")
        with torch.no_grad():
            for i, img in enumerate(cal_images):
                model_cpu.encoder(img)

        print("   âš¡ ì¸ì½”ë” ë³€í™˜ (Convert FX)...")
        model_cpu.encoder = quantize_fx.convert_fx(model_cpu.encoder)

        print("   ğŸ”„ ë””ì½”ë” ë™ì  ì–‘ìí™” ì ìš©...")
        quantized_model = torch.quantization.quantize_dynamic(
            model_cpu,
            {nn.Linear, nn.GRU, nn.LSTM},
            dtype=torch.qint8
        )
        
        print("   âœ… ì •ì  ì–‘ìí™” ì™„ë£Œ!")
        return quantized_model

    except Exception as e:
        print(f"   âš ï¸ ì •ì  ì–‘ìí™” ì‹¤íŒ¨: {e}")
        return torch.quantization.quantize_dynamic(
            deepcopy(model).cpu(),
            {nn.Linear, nn.GRU},
            dtype=torch.qint8
        )

def convert_to_int8_qat(model, word_map=None, qat_epochs=3):
    """Int8 QAT (Quantization-Aware Training)"""
    print(f"   ğŸ‘‰ Int8 QAT ì ìš© ì¤‘ ({qat_epochs} epochs)...")
    
    machine = platform.machine().lower()
    if 'arm' in machine or 'aarch64' in machine:
        backend = 'qnnpack'
    elif 'x86' in machine or 'amd64' in machine:
        backend = 'fbgemm'
    else:
        backend = 'qnnpack'
    
    torch.backends.quantized.engine = backend
    print(f"   âš™ï¸ Quantization Engine: {backend}")

    model_cpu = deepcopy(model).cpu()
    model_cpu.train()

    if word_map is None:
        print("   âš ï¸ word_mapì´ ì—†ì–´ Dynamic Quantizationìœ¼ë¡œ fallback")
        return torch.quantization.quantize_dynamic(model_cpu, {nn.Linear}, dtype=torch.qint8)

    # ì²´í¬í¬ì¸íŠ¸ í™•ì¸
    checkpoint_exists = os.path.exists(QAT_CHECKPOINT_PATH)
    
    if not checkpoint_exists:
        # ì²´í¬í¬ì¸íŠ¸ê°€ ì—†ìœ¼ë©´ ì–‘ìí™” ì¤€ë¹„ ìˆ˜í–‰
        print("   ğŸ“Š Calibration ë°ì´í„° ì¤€ë¹„ ì¤‘...")
        cal_images, _ = prepare_calibration_dataset(word_map, num_samples=1000, transform=transform)
        example_input = cal_images[0]
        
        qconfig_dict = {"": torch.quantization.get_default_qat_qconfig(backend)}
        
        print("   ğŸ”§ ì¸ì½”ë” QAT ì¤€ë¹„ (Prepare QAT FX)...")
        model_cpu.encoder = quantize_fx.prepare_qat_fx(
            model_cpu.encoder,
            qconfig_dict,
            example_input
        )
        
        print("   ğŸ”„ Calibration ì§„í–‰ ì¤‘ (ì´ˆê¸° ì–‘ìí™” íŒŒë¼ë¯¸í„° ì„¤ì •)...")
        model_cpu.encoder.eval()
        with torch.no_grad():
            for img in cal_images:
                model_cpu.encoder(img)
        
        print(f"\n   [QAT Fine-tuning ì‹œì‘]")
        model_cpu.train()
    else:
        print("   ğŸ“‚ ì²´í¬í¬ì¸íŠ¸ ë°œê²¬ - ì–‘ìí™” ì¤€ë¹„ ë‹¨ê³„ ê±´ë„ˆëœ€")
        model_cpu.train()
    
    # í•™ìŠµ ë°ì´í„°ì…‹ ì¤€ë¹„
    try:
        from torch.utils.data import DataLoader
        
        MAX_CAPTION_LEN = 50
        
        dataset = CaptionDataset(
            images_dir=TEST_IMAGE_DIR,
            captions_file=CAPTIONS_FILE,
            transform=transform,
            word_map=word_map,
            max_len=MAX_CAPTION_LEN
        )
        
        if len(dataset) == 0:
            print("   âš ï¸ í•™ìŠµ ë°ì´í„°ê°€ ì—†ì–´ Static Quantizationìœ¼ë¡œ fallback")
            return convert_to_int8_static(model, word_map)
        
        dataloader = DataLoader(
            dataset, 
            batch_size=64, 
            shuffle=True, 
            num_workers=0,
            pin_memory=False
        )
        
        print(f"   ğŸ“š í•™ìŠµ ë°ì´í„°: {len(dataset)}ê°œ ìƒ˜í”Œ")
        
        # Mixed Precision ì„¤ì •
        # QATëŠ” ì–‘ìí™” ì—°ì‚°ì„ í¬í•¨í•˜ë¯€ë¡œ CPUì—ì„œë§Œ ì•ˆì •ì ìœ¼ë¡œ ë™ì‘
        # MPSëŠ” ì–‘ìí™” ì—°ì‚°(aten::_fused_moving_avg_obs_fq_helper)ì„ ì§€ì›í•˜ì§€ ì•ŠìŒ
        use_mixed_precision = False
        scaler = None
        qat_device = torch.device("cpu")
        
        if torch.cuda.is_available():
            # CUDAëŠ” ì–‘ìí™”ë¥¼ ì§€ì›í•˜ì§€ë§Œ, ì•ˆì •ì„±ì„ ìœ„í•´ CPU ì‚¬ìš© ê¶Œì¥
            # í•„ìš”ì‹œ ì•„ë˜ ì£¼ì„ì„ í•´ì œí•˜ì—¬ CUDA ì‚¬ìš© ê°€ëŠ¥
            # qat_device = torch.device("cuda")
            # model_cpu = model_cpu.to(qat_device)
            # use_mixed_precision = True
            # scaler = torch.cuda.amp.GradScaler()
            # print("   ğŸš€ GPU ì‚¬ìš© - FP16 Mixed Precision í™œì„±í™”")
            print("   ğŸ’» CPU ì‚¬ìš© - QATëŠ” CPUì—ì„œ ì•ˆì •ì ìœ¼ë¡œ ë™ì‘ (ì–‘ìí™” ì—°ì‚° ì§€ì›)")
        else:
            print("   ğŸ’» CPU ì‚¬ìš© - FP32 í•™ìŠµ")
        
        criterion = nn.CrossEntropyLoss(ignore_index=0)
        optimizer = torch.optim.Adam(model_cpu.parameters(), lr=1e-4)
        vocab_size = len(word_map)
        
        # ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ ì‹œë„
        start_epoch = 0
        loss_history = []
        
        if checkpoint_exists:
            loaded_model, loaded_word_map, loaded_epoch, loaded_loss_history = load_qat_checkpoint(
                QAT_CHECKPOINT_PATH, model_cpu, optimizer
            )
            
            if loaded_model is not None:
                model_cpu = loaded_model
                if loaded_word_map:
                    word_map = loaded_word_map
                start_epoch = loaded_epoch
                loss_history = loaded_loss_history
                
                if start_epoch >= qat_epochs:
                    print(f"   âœ… í•™ìŠµì´ ì´ë¯¸ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤ (Epoch {start_epoch}/{qat_epochs})")
                    print("   ğŸ”„ ì–‘ìí™” ë³€í™˜ ì§„í–‰...")
                else:
                    print(f"   ğŸ”„ ì²´í¬í¬ì¸íŠ¸ì—ì„œ ì´ì–´ì„œ í•™ìŠµ: Epoch {start_epoch + 1}/{qat_epochs}ë¶€í„° ì‹œì‘")
            else:
                print(f"   âš ï¸ ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ ì‹¤íŒ¨ - ìƒˆë¡œìš´ í•™ìŠµ ì‹œì‘")
        else:
            print(f"   ğŸ†• ìƒˆë¡œìš´ í•™ìŠµ ì‹œì‘: {qat_epochs} epochs")
        
        # í•™ìŠµ ë£¨í”„ (í•™ìŠµì´ ì™„ë£Œë˜ì§€ ì•Šì€ ê²½ìš°ì—ë§Œ)
        if start_epoch < qat_epochs:
            for epoch in range(start_epoch, qat_epochs):
                epoch_loss = 0
                num_batches = 0
                
                for batch_idx, (imgs, caps) in enumerate(dataloader):
                    # if batch_idx >= 30:  # ë” ë§ì€ ë°°ì¹˜ë¡œ í•™ìŠµ
                    #     break
                    
                    imgs = imgs.to(qat_device)
                    caps = caps.to(qat_device)
                    
                    optimizer.zero_grad()
                    
                    try:
                        # QATëŠ” CPUì—ì„œë§Œ ìˆ˜í–‰ (ì–‘ìí™” ì—°ì‚°ì´ MPSì—ì„œ ì§€ì›ë˜ì§€ ì•ŠìŒ)
                        outputs, alphas = model_cpu(imgs, caps)
                        targets = caps[:, 1:]
                        outputs = outputs[:, :targets.shape[1], :]
                        loss = criterion(outputs.reshape(-1, vocab_size), targets.reshape(-1))
                        loss.backward()
                        optimizer.step()
                        
                        epoch_loss += loss.item()
                        num_batches += 1
                    except Exception as e:
                        print(f"   âš ï¸ ë°°ì¹˜ {batch_idx} í•™ìŠµ ì‹¤íŒ¨: {e}")
                        continue
                
                if num_batches > 0:
                    avg_loss = epoch_loss / num_batches
                    loss_history.append(avg_loss)
                    print(f"      Epoch {epoch+1}/{qat_epochs}, Loss: {avg_loss:.4f}")
                    
                    # ì²´í¬í¬ì¸íŠ¸ ì €ì¥ (ë§¤ epochë§ˆë‹¤)
                    save_qat_checkpoint(
                        model_cpu, optimizer, epoch + 1, loss_history, word_map, QAT_CHECKPOINT_PATH
                    )
        else:
            print(f"   â­ï¸ í•™ìŠµ ì™„ë£Œ - ì–‘ìí™” ë³€í™˜ìœ¼ë¡œ ì§„í–‰")
        
        print("   ğŸ”„ CPUë¡œ ì´ë™ ì¤‘ (Quantization ì¤€ë¹„)...")
        model_cpu = model_cpu.cpu()
        
        print("   âš¡ QAT ëª¨ë¸ ë³€í™˜ (Convert FX)...")
        model_cpu.eval()
        model_cpu.encoder = quantize_fx.convert_fx(model_cpu.encoder)
        
        print("   ğŸ”„ ë””ì½”ë” ë™ì  ì–‘ìí™” ì ìš©...")
        quantized_model = torch.quantization.quantize_dynamic(
            model_cpu,
            {nn.Linear, nn.GRU, nn.LSTM},
            dtype=torch.qint8
        )
        quantized_model.eval()
        
        # ìµœì¢… ì–‘ìí™” ëª¨ë¸ ì €ì¥
        final_model_path = os.path.join(QAT_CHECKPOINT_DIR, "qat_final_model.pth")
        os.makedirs(os.path.dirname(final_model_path), exist_ok=True)
        torch.save({
            'model_state_dict': quantized_model.state_dict(),
            'word_map': word_map,
            'loss_history': loss_history,
            'qat_epochs': qat_epochs,
            'final_epoch': qat_epochs,
        }, final_model_path)
        print(f"   ğŸ’¾ ìµœì¢… ì–‘ìí™” ëª¨ë¸ ì €ì¥: {final_model_path}")
        
        # ìµœì¢… ì²´í¬í¬ì¸íŠ¸ ì €ì¥ (í•™ìŠµ ì™„ë£Œ í‘œì‹œ)
        save_qat_checkpoint(
            model_cpu, optimizer, qat_epochs, loss_history, word_map, QAT_CHECKPOINT_PATH
        )
        
        print("   âœ… QAT ì™„ë£Œ!")
        return quantized_model
        
    except Exception as e:
        print(f"   âš ï¸ QAT ì‹¤íŒ¨: {e}")
        import traceback
        traceback.print_exc()
        return convert_to_int8_static(model, word_map)

# ============================================================================
# ë²¤ì¹˜ë§ˆí¬ ì—”ì§„
# ============================================================================
def run_benchmark(model, img_tensor, wm, rwm, precision_name, ref_caption=None):
    """ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰"""
    print(f"\n[{precision_name}] ë²¤ì¹˜ë§ˆí¬ ì‹œì‘...")
    
    model_device = next(model.parameters()).device
    inp = img_tensor.clone().detach().to(model_device)
    
    if "Int8" in precision_name:
        inp = inp.float().cpu()
    
    # Warm-up
    with torch.no_grad():
        try:
            _ = model.generate(inp, wm, rwm, 20)
        except Exception as e:
            print(f"âš ï¸ Warm-up ì‹¤íŒ¨: {e}")
            return None
    
    # ì†ë„ ì¸¡ì •
    latencies = []
    start_mem = get_peak_memory_mb()
    peak_mem = start_mem
    
    for i in range(NUM_RUNS):
        gc.collect()
        if device.type == 'cuda': 
            torch.cuda.empty_cache()
            torch.cuda.synchronize()
        
        start = time.time()
        with torch.no_grad():
            gen_seq = model.generate(inp, wm, rwm, 20)
            
        if device.type == 'cuda': 
            torch.cuda.synchronize()
        
        latencies.append((time.time() - start) * 1000)
        
        current_mem = get_peak_memory_mb()
        peak_mem = max(peak_mem, current_mem)
        
        if (i + 1) % 10 == 0:
            print(f"   ì§„í–‰: {i+1}/{NUM_RUNS}")
    
    # METEOR ì ìˆ˜ ê³„ì‚°
    meteor_scores = []
    example_caption = "N/A"
    
    if ref_caption:
        for _ in range(5):
            with torch.no_grad():
                gen_seq = model.generate(inp, wm, rwm, 20)
            meteor = calculate_meteor(gen_seq, ref_caption)
            if meteor is not None:
                meteor_scores.append(meteor)
            if _ == 0:
                example_caption = ' '.join([w for w in gen_seq if w not in ['<start>', '<end>', '<pad>', '<unk>']])
    
    avg_meteor = np.mean(meteor_scores) if meteor_scores else None
    
    # ê²°ê³¼ ì •ë¦¬
    avg_time = np.mean(latencies)
    std_time = np.std(latencies)
    size_mb = get_model_size_mb(model)
    memory_usage = peak_mem - start_mem
    total_params, trainable_params = count_parameters(model)
    
    print(f"   â±ï¸ í‰ê·  ì‹œê°„: {avg_time:.2f} Â± {std_time:.2f} ms")
    print(f"   ğŸ’¾ ëª¨ë¸ í¬ê¸°: {size_mb:.2f} MB")
    print(f"   ğŸ“Š íŒŒë¼ë¯¸í„° ê°œìˆ˜: {total_params:,} (í•™ìŠµ ê°€ëŠ¥: {trainable_params:,})")
    print(f"   ğŸ§  ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: {memory_usage:.2f} MB")
    if avg_meteor is not None:
        print(f"   â­ METEOR: {avg_meteor:.4f}")
    print(f"   ğŸ“ ì˜ˆì‹œ ìº¡ì…˜: {example_caption}")
    
    return {
        'precision': precision_name,
        'mean_time_ms': avg_time,
        'std_time_ms': std_time,
        'min_time_ms': np.min(latencies),
        'max_time_ms': np.max(latencies),
        'model_size_mb': size_mb,
        'memory_usage_mb': memory_usage,
        'meteor_score': avg_meteor,
        'inference_times': latencies,
        'example_caption': example_caption,
        'total_params': total_params,
        'trainable_params': trainable_params
    }

# ============================================================================
# ì‹œê°í™”
# ============================================================================
def plot_qat_comparison(result_static, result_qat):
    """QAT Fine-tuning ì „í›„ ë¹„êµ ê·¸ë˜í”„"""
    if not result_static or not result_qat:
        return
    
    metrics = []
    static_values = []
    qat_values = []
    improvements = []
    
    # ì¶”ë¡  ì‹œê°„
    static_time = result_static['mean_time_ms']
    qat_time = result_qat['mean_time_ms']
    time_improvement = ((static_time - qat_time) / static_time) * 100
    metrics.append('ì¶”ë¡  ì‹œê°„\n(ms)')
    static_values.append(static_time)
    qat_values.append(qat_time)
    improvements.append(time_improvement)
    
    # METEOR ì ìˆ˜
    if result_static.get('meteor_score') and result_qat.get('meteor_score'):
        static_meteor = result_static['meteor_score']
        qat_meteor = result_qat['meteor_score']
        meteor_improvement = ((qat_meteor - static_meteor) / static_meteor) * 100
        metrics.append('METEOR\nì ìˆ˜')
        static_values.append(static_meteor * 100)
        qat_values.append(qat_meteor * 100)
        improvements.append(meteor_improvement)
    
    # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰
    static_mem = result_static['memory_usage_mb']
    qat_mem = result_qat['memory_usage_mb']
    mem_improvement = ((static_mem - qat_mem) / static_mem) * 100 if static_mem > 0 else 0
    metrics.append('ë©”ëª¨ë¦¬\n(MB)')
    static_values.append(static_mem)
    qat_values.append(qat_mem)
    improvements.append(mem_improvement)
    
    # ê·¸ë˜í”„ ìƒì„±
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))
    fig.suptitle('QAT Fine-tuning ì „í›„ ë¹„êµ', fontsize=16, fontweight='bold')
    
    x = np.arange(len(metrics))
    width = 0.35
    
    # 1. ê°’ ë¹„êµ
    bars1 = ax1.bar(x - width/2, static_values, width, label='Static (Before)', alpha=0.8, color='#e74c3c')
    bars2 = ax1.bar(x + width/2, qat_values, width, label='QAT (After)', alpha=0.8, color='#2ecc71')
    
    ax1.set_ylabel('ê°’', fontweight='bold')
    ax1.set_title('ê°’ ë¹„êµ', fontweight='bold')
    ax1.set_xticks(x)
    ax1.set_xticklabels(metrics)
    ax1.legend()
    ax1.grid(axis='y', alpha=0.3)
    
    for bars in [bars1, bars2]:
        for bar in bars:
            height = bar.get_height()
            ax1.text(bar.get_x() + bar.get_width()/2., height,
                    f'{height:.2f}',
                    ha='center', va='bottom', fontsize=9)
    
    # 2. ê°œì„ ìœ¨
    colors = ['#2ecc71' if imp > 0 else '#e74c3c' for imp in improvements]
    bars3 = ax2.bar(metrics, improvements, alpha=0.8, color=colors)
    ax2.axhline(y=0, color='black', linestyle='-', linewidth=0.5)
    ax2.set_ylabel('ê°œì„ ìœ¨ (%)', fontweight='bold')
    ax2.set_title('ê°œì„ ìœ¨', fontweight='bold')
    ax2.grid(axis='y', alpha=0.3)
    
    for bar, imp in zip(bars3, improvements):
        height = bar.get_height()
        ax2.text(bar.get_x() + bar.get_width()/2., height,
                f'{imp:+.2f}%',
                ha='center', va='bottom' if imp > 0 else 'top', fontsize=10, fontweight='bold')
    
    plt.tight_layout()
    plt.savefig(os.path.join(OUTPUT_DIR, 'qat_fine_tuning_comparison.png'), 
                dpi=300, bbox_inches='tight')
    print(f"âœ… QAT ë¹„êµ Plot ì €ì¥: {os.path.join(OUTPUT_DIR, 'qat_fine_tuning_comparison.png')}")
    plt.close()

# ============================================================================
# Main
# ============================================================================
def main():
    os.makedirs(OUTPUT_DIR, exist_ok=True)
    
    print("="*70)
    print("=== QAT Fine-tuning ë²¤ì¹˜ë§ˆí¬ ===")
    print("="*70)
    
    # 1. ëª¨ë¸ ë° ë°ì´í„° ë¡œë“œ
    base_model, wm, rwm = load_base_model(device=device)
    img_tensor, ref_caption = load_test_data(device=device, transform=transform)
    
    # 2. Int8 Static Quantization (Before)
    print("\n" + "="*70)
    print("=== [1ë‹¨ê³„] Int8 Static Quantization (Before Fine-tuning) ===")
    print("="*70)
    model_int8_static = convert_to_int8_static(base_model, wm)
    result_int8_static = run_benchmark(model_int8_static, img_tensor, wm, rwm, "Int8-Static (CPU)", ref_caption)
    del model_int8_static
    gc.collect()
    
    # 3. Int8 QAT (After Fine-tuning)
    print("\n" + "="*70)
    print("=== [2ë‹¨ê³„] Int8 QAT (After Fine-tuning) ===")
    print("="*70)
    model_int8_qat = convert_to_int8_qat(base_model, wm, qat_epochs=QAT_EPOCHS)
    result_int8_qat = run_benchmark(model_int8_qat, img_tensor, wm, rwm, "Int8-QAT (CPU)", ref_caption)
    del model_int8_qat
    gc.collect()
    
    # 4. ê²°ê³¼ ë¹„êµ ì¶œë ¥
    if result_int8_static and result_int8_qat:
        print("\n" + "="*70)
        print("=== ğŸ¯ QAT Fine-tuning ì „í›„ ë¹„êµ ê²°ê³¼ ===")
        print("="*70)
        print(f"{'Metric':<30} {'Static (Before)':<20} {'QAT (After)':<20} {'ê°œì„ ìœ¨':<15}")
        print("-"*85)
        
        # ì¶”ë¡  ì‹œê°„
        static_time = result_int8_static['mean_time_ms']
        qat_time = result_int8_qat['mean_time_ms']
        time_improvement = ((static_time - qat_time) / static_time) * 100
        time_emoji = "âœ…" if time_improvement > 0 else "âŒ"
        print(f"{'â±ï¸  ì¶”ë¡  ì‹œê°„ (ms)':<30} {static_time:<20.2f} {qat_time:<20.2f} {time_emoji} {time_improvement:>8.2f}%")
        
        # METEOR ì ìˆ˜
        if result_int8_static.get('meteor_score') and result_int8_qat.get('meteor_score'):
            static_meteor = result_int8_static['meteor_score']
            qat_meteor = result_int8_qat['meteor_score']
            meteor_improvement = ((qat_meteor - static_meteor) / static_meteor) * 100
            meteor_emoji = "âœ…" if meteor_improvement > 0 else "âŒ"
            print(f"{'â­ METEOR ì ìˆ˜':<30} {static_meteor:<20.4f} {qat_meteor:<20.4f} {meteor_emoji} {meteor_improvement:>8.2f}%")
        
        # ëª¨ë¸ í¬ê¸°
        static_size = result_int8_static['model_size_mb']
        qat_size = result_int8_qat['model_size_mb']
        print(f"{'ğŸ’¾ ëª¨ë¸ í¬ê¸° (MB)':<30} {static_size:<20.2f} {qat_size:<20.2f} {'-':>15}")
        
        # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰
        static_mem = result_int8_static['memory_usage_mb']
        qat_mem = result_int8_qat['memory_usage_mb']
        mem_improvement = ((static_mem - qat_mem) / static_mem) * 100 if static_mem > 0 else 0
        mem_emoji = "âœ…" if mem_improvement > 0 else "âŒ"
        print(f"{'ğŸ§  ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ (MB)':<30} {static_mem:<20.2f} {qat_mem:<20.2f} {mem_emoji} {mem_improvement:>8.2f}%")
        
        # íŒŒë¼ë¯¸í„° ê°œìˆ˜
        static_params = result_int8_static.get('total_params', 0)
        qat_params = result_int8_qat.get('total_params', 0)
        static_params_m = static_params / 1e6
        qat_params_m = qat_params / 1e6
        print(f"{'ğŸ“Š íŒŒë¼ë¯¸í„° ê°œìˆ˜ (M)':<30} {static_params_m:<20.2f} {qat_params_m:<20.2f} {'-':>15}")
        
        print("="*85)
        print("\nğŸ’¡ í•´ì„:")
        if time_improvement > 0:
            print(f"   âœ… ì¶”ë¡  ì‹œê°„ì´ {time_improvement:.2f}% ê°œì„ ë˜ì—ˆìŠµë‹ˆë‹¤ (ë¹ ë¦„)")
        else:
            print(f"   âš ï¸ ì¶”ë¡  ì‹œê°„ì´ {abs(time_improvement):.2f}% ëŠë ¤ì¡ŒìŠµë‹ˆë‹¤")
        
        if result_int8_static.get('meteor_score') and result_int8_qat.get('meteor_score'):
            if meteor_improvement > 0:
                print(f"   âœ… METEOR ì ìˆ˜ê°€ {meteor_improvement:.2f}% ê°œì„ ë˜ì—ˆìŠµë‹ˆë‹¤ (ì •í™•ë„ í–¥ìƒ)")
            else:
                print(f"   âš ï¸ METEOR ì ìˆ˜ê°€ {abs(meteor_improvement):.2f}% ê°ì†Œí–ˆìŠµë‹ˆë‹¤")
        
        if mem_improvement > 0:
            print(f"   âœ… ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì´ {mem_improvement:.2f}% ê°ì†Œí–ˆìŠµë‹ˆë‹¤ (íš¨ìœ¨ì )")
        else:
            print(f"   âš ï¸ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì´ {abs(mem_improvement):.2f}% ì¦ê°€í–ˆìŠµë‹ˆë‹¤")
        
        print("="*85)
        
        # ê·¸ë˜í”„ ìƒì„±
        print("\nğŸ“Š ê·¸ë˜í”„ ìƒì„± ì¤‘...")
        plot_qat_comparison(result_int8_static, result_int8_qat)
    
    print("\n" + "="*70)
    print("=== ë²¤ì¹˜ë§ˆí¬ ì™„ë£Œ ===")
    print(f"ê²°ê³¼ ì €ì¥ ìœ„ì¹˜: {OUTPUT_DIR}")
    print("="*70)

if __name__ == "__main__":
    main()

