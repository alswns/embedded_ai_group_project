#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Jetson Nanoìš© ìµœì†Œí™”ëœ ì´ë¯¸ì§€ ìº¡ì…”ë‹ ì‹œìŠ¤í…œ
ìµœëŒ€ ì•ˆì •ì„± ìš°ì„ 
"""

import cv2
import torch
import numpy as np
import os
import threading
import tempfile
import time
import psutil
import gc
import sys

print("ğŸ“¦ ëª¨ë“ˆ ë¡œë“œ ì¤‘...", file=sys.stderr)

try:
    from PIL import Image
    from torchvision import transforms
    from gtts import gTTS
    import pygame
    from src.muti_modal_model.model import MobileNetCaptioningModel
    from src.utils.quantization_utils import apply_dynamic_quantization
    print("âœ… ëª¨ë“  ëª¨ë“ˆ ë¡œë“œ ì™„ë£Œ", file=sys.stderr)
except ImportError as e:
    print("âŒ ëª¨ë“ˆ ë¡œë“œ ì‹¤íŒ¨: {}".format(e), file=sys.stderr)
    sys.exit(1)

# ============================================================================
# í™˜ê²½ ì„¤ì •
# ============================================================================
print("âš™ï¸  í™˜ê²½ ì„¤ì • ì¤‘...", file=sys.stderr)

# GPU ì™„ì „ ë¹„í™œì„±í™”
os.environ['CUDA_VISIBLE_DEVICES'] = ''
torch.backends.cudnn.enabled = False
torch.backends.cudnn.benchmark = False

# CPU ìµœì í™”
torch.set_num_threads(2)
torch.set_num_interop_threads(1)

# ê°•ì œ CPU ë””ë°”ì´ìŠ¤
device = torch.device("cpu")
print("ğŸ“ ë””ë°”ì´ìŠ¤: CPU", file=sys.stderr)

# ëª¨ë¸ ê²½ë¡œ
MODELS = {
    '1': {
        'name': 'Original Model',
        'path': 'models/lightweight_captioning_model.pth',
        'fallback': 'lightweight_captioning_model.pth'
    },
    '2': {
        'name': 'Pruned Model',
        'path': 'pruning_results/Pruning_epoch_1_checkpoint.pt',
        'fallback': None
    }
}

QUANTIZE_OPTIONS = {
    '1': {'name': 'FP32 (ì›ë³¸)', 'enabled': False},
    '2': {'name': 'FP16', 'enabled': True},
    '3': {'name': 'INT8', 'enabled': True}
}

# ì´ë¯¸ì§€ ì „ì²˜ë¦¬
transform = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406], 
                       std=[0.229, 0.224, 0.225])
])

print("âœ… í™˜ê²½ ì„¤ì • ì™„ë£Œ", file=sys.stderr)

# ============================================================================
# ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜
# ============================================================================

def select_model():
    """ëª¨ë¸ ì„ íƒ"""
    print("\nëª¨ë¸ ì„ íƒ:")
    for key, info in MODELS.items():
        exists = "âœ…" if os.path.exists(info['path']) else "âŒ"
        print("  {}. {} {}".format(key, info['name'], exists))
    
    while True:
        choice = input("ì„ íƒ (1-2): ").strip()
        if choice in MODELS:
            return choice
        print("ì˜ëª»ëœ ì…ë ¥ì…ë‹ˆë‹¤.")

def select_quantization():
    """ì–‘ìí™” ì˜µì…˜ ì„ íƒ"""
    print("\nì–‘ìí™” ì˜µì…˜:")
    for key, info in QUANTIZE_OPTIONS.items():
        status = "âœ…" if info['enabled'] else "âŒ"
        print("  {}. {} {}".format(key, info['name'], status))
    
    while True:
        choice = input("ì„ íƒ (1-3): ").strip()
        if choice in QUANTIZE_OPTIONS:
            return choice
        print("ì˜ëª»ëœ ì…ë ¥ì…ë‹ˆë‹¤.")

def load_model(model_choice):
    """ëª¨ë¸ ë¡œë“œ"""
    info = MODELS[model_choice]
    path = info['path']
    
    if not os.path.exists(path):
        if info['fallback'] and os.path.exists(info['fallback']):
            path = info['fallback']
        else:
            print("âŒ ëª¨ë¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {}".format(info['path']))
            return None, None, None, None
    
    try:
        print("ğŸ“‚ ëª¨ë¸ ë¡œë“œ: {}".format(path))
        checkpoint = torch.load(path, map_location='cpu', weights_only=False)
        
        if not isinstance(checkpoint, dict) or 'model_state_dict' not in checkpoint:
            print("âŒ ìœ íš¨í•˜ì§€ ì•Šì€ ëª¨ë¸ íŒŒì¼")
            return None, None, None, None
        
        word_map = checkpoint.get('word_map')
        rev_word_map = checkpoint.get('rev_word_map')
        vocab_size = checkpoint.get('vocab_size')
        
        if not (word_map and rev_word_map and vocab_size):
            print("âŒ ë‹¨ì–´ì¥ ì •ë³´ ì—†ìŒ")
            return None, None, None, None
        
        # ëª¨ë¸ ìƒì„±
        state_dict = checkpoint['model_state_dict']
        decoder_dim = checkpoint.get('decoder_dim', 512)
        attention_dim = checkpoint.get('attention_dim', 256)
        
        try:
            model = MobileNetCaptioningModel(
                vocab_size=vocab_size,
                embed_dim=300,
                decoder_dim=decoder_dim,
                attention_dim=attention_dim
            )
            model = model.to(device)
            model.load_state_dict(state_dict, strict=False)
            model.eval()
        except Exception as e:
            print("âŒ ëª¨ë¸ ìƒì„± ì‹¤íŒ¨: {}".format(e))
            return None, None, None, None
        
        # ë©”ëª¨ë¦¬ ì •ë¦¬
        del checkpoint, state_dict
        gc.collect()
        
        print("âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
        return model, word_map, rev_word_map, info['name']
        
    except Exception as e:
        print("âŒ ëª¨ë¸ ë¡œë“œ ì˜¤ë¥˜: {}".format(e))
        import traceback
        traceback.print_exc()
        return None, None, None, None

def apply_quantization(model, choice, name):
    """ì–‘ìí™” ì ìš©"""
    if choice == '1':
        print("FP32 (ì–‘ìí™” ì—†ìŒ)")
        return model.cpu(), name
    elif choice == '2':
        print("FP16ì€ CPUì—ì„œ ë¯¸ì§€ì› - FP32 ì‚¬ìš©")
        return model.cpu(), name
    elif choice == '3':
        print("INT8 ì ìš© ì‹œë„...")
        try:
            model = apply_dynamic_quantization(model)
            return model.cpu(), name + " + INT8"
        except Exception as e:
            print("INT8 ì‹¤íŒ¨ - FP32 ì‚¬ìš©: {}".format(e))
            return model.cpu(), name
    
    return model.cpu(), name

def generate_caption(model, word_map, rev_word_map, frame):
    """ìº¡ì…˜ ìƒì„±"""
    image_tensor = None
    try:
        model = model.cpu()
        model.eval()
        
        rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
        pil_image = Image.fromarray(rgb_frame)
        
        image_tensor = transform(pil_image).unsqueeze(0)
        
        start_time = time.time()
        with torch.no_grad():
            try:
                generated_words = model.generate(image_tensor, word_map, rev_word_map, max_len=50)
            except Exception as e:
                print("âš ï¸  ì¶”ë¡  ì˜¤ë¥˜: {}".format(e))
                gc.collect()
                return None, 0.0
        
        inference_time = (time.time() - start_time) * 1000
        caption = ' '.join([w for w in generated_words 
                           if w not in ['<start>', '<end>', '<pad>', '<unk>']])
        
        return caption, inference_time
    except Exception as e:
        print("ìº¡ì…˜ ìƒì„± ì˜¤ë¥˜: {}".format(e))
        return None, 0.0
    finally:
        if image_tensor is not None:
            del image_tensor
        gc.collect()

# ============================================================================
# ë©”ì¸
# ============================================================================

def main():
    print("\n" + "="*70)
    print("ğŸ“¸ ì´ë¯¸ì§€ ìº¡ì…”ë‹ ì‹œìŠ¤í…œ (Jetson Nano)")
    print("="*70)
    
    # ëª¨ë¸ ì„ íƒ
    model_choice = select_model()
    
    # ëª¨ë¸ ë¡œë“œ
    model, word_map, rev_word_map, model_name = load_model(model_choice)
    if model is None:
        print("âŒ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨")
        return
    
    # ì–‘ìí™” ì„ íƒ
    quant_choice = select_quantization()
    model, model_name = apply_quantization(model, quant_choice, model_name)
    
    print("\n" + "="*70)
    print("ëª¨ë¸: {}".format(model_name))
    print("="*70)
    print("\ní‚¤ ì…ë ¥: s (ìº¡ì…˜), r (ì¬ìƒ), q (ì¢…ë£Œ)\n")
    
    # ì¹´ë©”ë¼ ì‹œì‘
    cap = cv2.VideoCapture(0)
    if not cap.isOpened():
        print("âŒ ì¹´ë©”ë¼ ì—†ìŒ")
        return
    
    last_caption = None
    
    while True:
        ret, frame = cap.read()
        if not ret:
            break
        
        cv2.imshow('Image Captioning', frame)
        
        key = cv2.waitKey(1) & 0xFF
        
        if key == ord('q'):
            break
        elif key == ord('s'):
            caption, inf_time = generate_caption(model, word_map, rev_word_map, frame)
            if caption:
                last_caption = caption
                print("\nìº¡ì…˜: {}".format(caption))
                print("ì‹œê°„: {:.2f}ms\n".format(inf_time))
        elif key == ord('r') and last_caption:
            print("\nì´ì „ ìº¡ì…˜: {}".format(last_caption))
    
    cap.release()
    cv2.destroyAllWindows()
    print("ì¢…ë£Œ")

if __name__ == "__main__":
    main()
