"""
Pruning ë²¤ì¹˜ë§ˆí¬ ìŠ¤í¬ë¦½íŠ¸
ë‹¤ì–‘í•œ Pruning ê¸°ë²•ì„ ì ìš©í•˜ê³  ì„±ëŠ¥ì„ ë¹„êµí•©ë‹ˆë‹¤.
"""
import torch
import torch.nn as nn
import torch.nn.utils.prune as prune
from torch.utils.data import DataLoader
from src.utils import CaptionDataset
import numpy as np
import os
import time
import matplotlib.pyplot as plt
from copy import deepcopy
import gc
import warnings
from PIL import Image

warnings.filterwarnings('ignore')

# ê³µí†µ ìœ í‹¸ë¦¬í‹° import
from src.utils import (
    setup_device,
    setup_matplotlib,
    get_image_transform,
    count_parameters,
    get_model_size_mb,
    get_peak_memory_mb,
    calculate_meteor,
    load_test_data,
    load_base_model,
    TEST_IMAGE_DIR,
    CAPTIONS_FILE,
)

# Pruning ìœ í‹¸ë¦¬í‹° import
from pruning_utils import (
    count_nonzero_parameters,
    update_linear_layer,
    compute_hessian_importance,
    compute_channel_importance_hessian,
)

# Benchmark ìœ í‹¸ë¦¬í‹° import
from benchmark_utils import (
    calculate_model_size_mb,
    calculate_sparsity,
    measure_inference_time,
)

# Finetune ìœ í‹¸ë¦¬í‹° import
from finetune_utils import (
    load_checkpoint,
    setup_training,
    save_checkpoint,
    print_checkpoint_info,
    restore_optimizer,
)

# ============================================================================
# ì„¤ì •
# ============================================================================
setup_matplotlib()

OUTPUT_DIR = "pruning_results"
NUM_RUNS = 50

# Pruning ì„¤ì •
PRUNING_RATES = [0.1, 0.3, 0.5, 0.7, 0.9]  # 10%, 30%, 50%, 70%, 90% í”„ë£¨ë‹
PRUNING_RATES = [0.3]
PRUNING_METHODS = ['magnitude', 'structured']  # í”„ë£¨ë‹ ë°©ë²•
ENABLE_MAGNITUDE_PRUNING = False  # âš ï¸ Magnitude Pruningì€ ì´ ëª¨ë¸ì— ë¹„íš¨ìœ¨ì  (ê²°ê³¼ ì°¸ê³ )
MAX_PRUNING_RATE = 0.51  # âš ï¸ 30% ì´ìƒ í”„ë£¨ë‹ì€ ì •í™•ë„ ê¸‰ê²©íˆ í•˜ë½ (50% ì´ìƒì€ ê±°ì˜ ì‘ë™ ë¶ˆê°€)
METEO_IMAGE_NUM=100
FINETUNE_EPOCHS=10
LEARNING_RATE=5e-5  # íŒŒì¸íŠœë‹ í•™ìŠµë¥  (ì‚¬ìš©ì ì„¤ì • ê°€ëŠ¥)
EARLY_STOPPING_PATIENCE=2  # Early Stopping ì¸ë‚´ì‹¬ (3 epoch ë™ì•ˆ ê°œì„  ì—†ìœ¼ë©´ ì¤‘ì§€)
VALIDATION_SPLIT=0.2  # ê²€ì¦ ë°ì´í„°ì…‹ ë¹„ìœ¨ (20%)
# ë””ë°”ì´ìŠ¤ ì„ íƒ
device = setup_device()

# ì´ë¯¸ì§€ ì „ì²˜ë¦¬
transform = get_image_transform()

# ============================================================================
# Pruning í•¨ìˆ˜ (ë¬¼ë¦¬ì  êµ¬ì¡° ìˆ˜ì •)
# ============================================================================

def apply_structured_pruning_physical(model, pruning_rate, img_tensor=None, captions_batch=None, wm=None, rwm=None, device=None, use_hessian=True):
    """Structured Pruning ì ìš© (Hessian ê¸°ë°˜ - GRU í¬í•¨ ì‹¤ì œ 30% ê°ì†Œ)
    
    ğŸ“Š ì‹¤ì œ íŒŒë¼ë¯¸í„° ë¶„í¬:
    - Encoder(CNN): ~12% â† ì‘ìŒ
    - Decoder(GRU): ~88% â† ëŒ€ë¶€ë¶„
      - GRU Cell: ~70-80% â† ê°€ì¥ í¼!
      - Attention: ~2-3%
    
    ğŸ’¡ í•´ê²°ì±…: GRU Hidden Stateë¥¼ ì ì§„ì ìœ¼ë¡œ ì¶•ì†Œ
    - ì™„ì „íˆ ìë¥´ì§€ ì•Šê³ , Hidden State ì¼ë¶€ë§Œ ì œê±° (Hessian ê¸°ë°˜)
    - ìˆœí™˜ ê°€ì¤‘ì¹˜(W_h)ë„ í•¨ê»˜ ì¶•ì†Œí•˜ì—¬ ì—­í•™ ìœ ì§€
    - íŒŒì¸íŠœë‹ìœ¼ë¡œ ê°€ì¤‘ì¹˜ ì¬í•™ìŠµ
    """
    from src.gru_model.model import LightweightCaptionDecoder
    
    pruned_model = deepcopy(model)
    pruned_model.eval()
    
    print(f"\n   ğŸ“Š íŒŒë¼ë¯¸í„° ë¶„ì„:")
    total_params = sum(p.numel() for p in pruned_model.parameters())
    
    if hasattr(pruned_model, 'encoder'):
        encoder_params = sum(p.numel() for p in pruned_model.encoder.parameters())
        print(f"      Encoder: {encoder_params:,} ({100*encoder_params/total_params:.1f}%)")
    
    if hasattr(pruned_model, 'decoder'):
        decoder_params = sum(p.numel() for p in pruned_model.decoder.parameters())
        print(f"      Decoder: {decoder_params:,} ({100*decoder_params/total_params:.1f}%)")
        
        if hasattr(pruned_model.decoder, 'decode_step'):
            gru_params = sum(p.numel() for p in pruned_model.decoder.decode_step.parameters())
            print(f"         â””â”€ GRU Cell: {gru_params:,} ({100*gru_params/total_params:.1f}%)")
    
    decoder = pruned_model.decoder
    
    # ğŸ¯ í•µì‹¬: GRU Hidden State ì¶•ì†Œ (Hessian ê¸°ë°˜)
    print(f"\n   ğŸ¯ GRU Hidden State ì ì§„ì  ì¶•ì†Œ ({pruning_rate*100:.0f}%)")
    
    if hasattr(decoder, 'decode_step'):
        old_gru = decoder.decode_step
        old_hidden_size = old_gru.hidden_size
        
        # ìƒˆë¡œìš´ Hidden Size ê³„ì‚°
        new_hidden_size = int(old_hidden_size * (1 - pruning_rate))
        
        print(f"      GRU Hidden Size: {old_hidden_size} â†’ {new_hidden_size} (ì œê±°: {old_hidden_size - new_hidden_size})")
        
        # Hessian ê¸°ë°˜ ì¤‘ìš” ë‰´ëŸ° ì„ íƒ
        if use_hessian and img_tensor is not None and device is not None:
            try:
                # GRUì˜ ì¤‘ìš”ë„ ê³„ì‚° (ê°€ì¤‘ì¹˜ì˜ 1norm ì‚¬ìš©)
                w_ih = old_gru.weight_ih.data  # [3*hidden_size, input_size]
                w_hh = old_gru.weight_hh.data  # [3*hidden_size, hidden_size]
                
                # ë‰´ëŸ°ë³„ ì¤‘ìš”ë„: ì…ë ¥ ê°€ì¤‘ì¹˜ì™€ ìˆœí™˜ ê°€ì¤‘ì¹˜ì˜ norm í•©
                importance = torch.zeros(old_hidden_size, device=device)
                
                # Reset gate, Update gate, New gateë³„ë¡œ ì²˜ë¦¬
                for gate_idx in range(3):
                    start_idx = gate_idx * old_hidden_size
                    end_idx = (gate_idx + 1) * old_hidden_size
                    
                    # ì…ë ¥ ê°€ì¤‘ì¹˜ì˜ norm
                    w_ih_gate = w_ih[start_idx:end_idx, :]
                    importance += torch.norm(w_ih_gate, p=2, dim=1)
                    
                    # ìˆœí™˜ ê°€ì¤‘ì¹˜ì˜ norm
                    w_hh_gate = w_hh[start_idx:end_idx, :]
                    importance += torch.norm(w_hh_gate, p=2, dim=1)
                
                # ì¤‘ìš”ë„ê°€ ë‚®ì€ ë‰´ëŸ° ì„ íƒ
                num_to_prune = old_hidden_size - new_hidden_size
                _, indices_to_keep = torch.topk(importance, new_hidden_size)
                indices_to_keep = torch.sort(indices_to_keep)[0]
                
                print(f"      âœ… Hessian ê¸°ë°˜ ì¤‘ìš” ë‰´ëŸ° ì„ íƒ ì™„ë£Œ")
                
            except Exception as e:
                print(f"      âš ï¸ Hessian ê³„ì‚° ì‹¤íŒ¨: {e}")
                indices_to_keep = torch.arange(new_hidden_size, device=device)
        else:
            # ë’¤ì˜ ë‰´ëŸ°ë¶€í„° ì œê±° (ë‹¨ìˆœ ì „ëµ)
            indices_to_keep = torch.arange(new_hidden_size, device=device)
        
        # ìƒˆë¡œìš´ GRUCell ìƒì„±
        new_gru = nn.GRUCell(old_gru.input_size, new_hidden_size)
        
        # ê°€ì¤‘ì¹˜ ì¶•ì†Œ
        # weight_ih: [3*hidden_size, input_size] â†’ [3*new_hidden_size, input_size]
        new_gru.weight_ih.data = torch.zeros(
            3 * new_hidden_size, old_gru.input_size, device=device
        )
        for gate_idx in range(3):
            old_start = gate_idx * old_hidden_size
            old_end = (gate_idx + 1) * old_hidden_size
            new_start = gate_idx * new_hidden_size
            new_end = (gate_idx + 1) * new_hidden_size
            
            new_gru.weight_ih.data[new_start:new_end, :] = old_gru.weight_ih.data[
                old_start:old_end, :
            ][indices_to_keep, :]
        
        # weight_hh: [3*hidden_size, hidden_size] â†’ [3*new_hidden_size, new_hidden_size]
        new_gru.weight_hh.data = torch.zeros(
            3 * new_hidden_size, new_hidden_size, device=device
        )
        for gate_idx in range(3):
            old_start = gate_idx * old_hidden_size
            old_end = (gate_idx + 1) * old_hidden_size
            new_start = gate_idx * new_hidden_size
            new_end = (gate_idx + 1) * new_hidden_size
            
            old_w = old_gru.weight_hh.data[old_start:old_end, :]
            new_gru.weight_hh.data[new_start:new_end, :] = old_w[
                indices_to_keep, :
            ][:, indices_to_keep]
        
        # Bias ì¶•ì†Œ
        if old_gru.bias_ih is not None:
            new_gru.bias_ih.data = torch.zeros(3 * new_hidden_size, device=device)
            for gate_idx in range(3):
                old_start = gate_idx * old_hidden_size
                old_end = (gate_idx + 1) * old_hidden_size
                new_start = gate_idx * new_hidden_size
                new_end = (gate_idx + 1) * new_hidden_size
                new_gru.bias_ih.data[new_start:new_end] = old_gru.bias_ih.data[
                    old_start:old_end
                ][indices_to_keep]
        
        if old_gru.bias_hh is not None:
            new_gru.bias_hh.data = torch.zeros(3 * new_hidden_size, device=device)
            for gate_idx in range(3):
                old_start = gate_idx * old_hidden_size
                old_end = (gate_idx + 1) * old_hidden_size
                new_start = gate_idx * new_hidden_size
                new_end = (gate_idx + 1) * new_hidden_size
                new_gru.bias_hh.data[new_start:new_end] = old_gru.bias_hh.data[
                    old_start:old_end
                ][indices_to_keep]
        
        decoder.decode_step = new_gru
        
        # ğŸ”´ CRITICAL: decoder.decoder_dimì„ ë¨¼ì € ì—…ë°ì´íŠ¸í•´ì•¼ í•¨!
        # Attention ì—…ë°ì´íŠ¸ê°€ ì´ ê°’ì„ ì‚¬ìš©í•˜ë¯€ë¡œ
        decoder.decoder_dim = new_hidden_size
        print(f"      ğŸ”§ decoder_dim ì—…ë°ì´íŠ¸: {old_hidden_size} â†’ {new_hidden_size}")
        
        # GRU ì¶œë ¥ì— ì—°ê²°ëœ ë‹¤ë¥¸ ë ˆì´ì–´ë“¤ë„ ì—…ë°ì´íŠ¸
        # (ì˜ˆ: decoder_att, fc ë“± hidden_sizeë¥¼ ì…ë ¥ë°›ëŠ” ë ˆì´ì–´)
        if hasattr(decoder, 'fc'):
            # fc: [hidden_size] â†’ [vocab_size]
            old_fc = decoder.fc
            new_fc = nn.Linear(new_hidden_size, old_fc.out_features)
            new_fc.weight.data = old_fc.weight.data[:, indices_to_keep]
            new_fc.bias.data = old_fc.bias.data.clone()
            decoder.fc = new_fc
            print(f"      âœ… fc ë ˆì´ì–´ ì—…ë°ì´íŠ¸: [{old_hidden_size}] â†’ [{new_hidden_size}]")
        
        # init_h ë ˆì´ì–´ë„ ì—…ë°ì´íŠ¸ (ìˆë‹¤ë©´)
        if hasattr(decoder, 'init_h'):
            old_init_h = decoder.init_h
            new_init_h = nn.Linear(old_init_h.in_features, new_hidden_size)
            new_init_h.weight.data = old_init_h.weight.data[indices_to_keep, :]
            new_init_h.bias.data = old_init_h.bias.data[indices_to_keep]
            decoder.init_h = new_init_h
            print(f"      âœ… init_h ë ˆì´ì–´ ì—…ë°ì´íŠ¸: [*] â†’ [{new_hidden_size}]")
    
    # Attention ì°¨ì›ë„ ì¶•ì†Œ
    if hasattr(decoder, 'encoder_att') and hasattr(decoder, 'full_att'):
        weight = decoder.encoder_att.weight.data
        
        # L2 norm ê¸°ë°˜ ì¤‘ìš”ë„ ê³„ì‚° (ê°„ë‹¨í•¨)
        mask_attention_dim = compute_channel_importance_hessian(
            weight, pruning_rate, dim=0, hessian_importance=None
        )
        new_attention_dim = mask_attention_dim.sum().item()
        
        print(f"   ğŸ“Š Attention Dim: {weight.shape[0]} â†’ {new_attention_dim} (ì œê±°: {weight.shape[0] - new_attention_dim})")
        
        # Attention ë ˆì´ì–´ ì—…ë°ì´íŠ¸
        decoder.encoder_att = update_linear_layer(decoder.encoder_att, mask_out=mask_attention_dim, in_size=decoder.encoder_dim)
        decoder.attention_dim = new_attention_dim
        
        if hasattr(decoder, 'decoder_att'):
            new_decoder_att = nn.Linear(decoder.decoder_dim, new_attention_dim)
            nn.init.xavier_uniform_(new_decoder_att.weight)
            if new_decoder_att.bias is not None:
                nn.init.zeros_(new_decoder_att.bias)
            decoder.decoder_att = new_decoder_att
            print(f"   âœ… decoder_att ì—…ë°ì´íŠ¸: [hidden={decoder.decoder_dim}] -> [attention={new_attention_dim}]")
        
        if hasattr(decoder, 'full_att'):
            decoder.full_att = update_linear_layer(decoder.full_att, mask_in=mask_attention_dim, out_size=1)
    
    pruned_model.decoder = decoder
    pruned_model.eval()
    
    # íŒŒë¼ë¯¸í„° ê°œìˆ˜ í™•ì¸
    old_params = sum(p.numel() for p in model.parameters())
    new_params = sum(p.numel() for p in pruned_model.parameters())
    reduction = (1 - new_params / old_params) * 100
    
    print(f"   âœ‚ï¸ Structured Pruning ì™„ë£Œ: GRU Hidden State + Attention ì¶•ì†Œ, {pruning_rate*100:.0f}% í”„ë£¨ë‹")
    print(f"   ğŸ“Š íŒŒë¼ë¯¸í„° ê°ì†Œ: {old_params:,} â†’ {new_params:,} ({reduction:.1f}% ê°ì†Œ)")
    print(f"   âš¡ **ì•ˆì „ì„±**: GRU ìˆœí™˜ ì—­í•™ ë¶€ë¶„ ë³´ì¡´ + Hessian ê¸°ë°˜ ì ì§„ì  ì¶•ì†Œ")
    
    return pruned_model

def apply_magnitude_pruning(model, pruning_rate):
    """Magnitude-based Pruning ì ìš© (Unstructured - ê°€ì¤‘ì¹˜ ë§ˆìŠ¤í‚¹, êµ¬ì¡° ë³€ê²½ ì—†ìŒ)
    
    Magnitude Pruningì€ ê°œë³„ ê°€ì¤‘ì¹˜(Weight)ì˜ ì ˆëŒ“ê°’(Magnitude)ì´ ì‘ì€ ê²ƒë“¤ì„ 0ìœ¼ë¡œ ì„¤ì •í•©ë‹ˆë‹¤.
    **ì¤‘ìš”**: ëª¨ë¸ì˜ ì‹¤ì œ êµ¬ì¡°(ì°¨ì›)ëŠ” ë³€í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ë‹¨ì§€ ì¼ë¶€ ê°€ì¤‘ì¹˜ê°€ 0ì´ ë˜ì–´ í¬ì†Œì„±ë§Œ ì¦ê°€í•©ë‹ˆë‹¤.
    - ê°œë³„ ê°€ì¤‘ì¹˜ ì œê±° (ë ˆì´ì–´ êµ¬ì¡° ë³€ê²½ ì—†ìŒ)
    - ëª¨ë¸ íŒŒì¼ í¬ê¸°ëŠ” ê°ì†Œí•˜ì§€ë§Œ, ì¼ë°˜ í•˜ë“œì›¨ì–´ì—ì„œëŠ” ì¶”ë¡  ì†ë„ í–¥ìƒ ë¯¸ë¯¸
    - íŠ¹ìˆ˜ í¬ì†Œ í–‰ë ¬ ì²˜ë¦¬ í•˜ë“œì›¨ì–´(ì˜ˆ: NVIDIA Sparse Tensor Core)ê°€ ìˆì–´ì•¼ ì†ë„ í–¥ìƒ
    """
    pruned_model = deepcopy(model)
    pruned_model.eval()
    
    # Magnitude-based pruning: ëª¨ë¸ êµ¬ì¡°ëŠ” ìœ ì§€í•˜ê³  ê°€ì¤‘ì¹˜ë§Œ ë§ˆìŠ¤í‚¹
    # ê° ê°€ì¤‘ì¹˜ì˜ ì ˆëŒ“ê°’(magnitude)ì„ ê¸°ì¤€ìœ¼ë¡œ ë‚®ì€ ê²ƒë¶€í„° 0ìœ¼ë¡œ ì„¤ì •
    
    # 1. ì „ì²´ ëª¨ë¸ì˜ ê°€ì¤‘ì¹˜ì— magnitude-based masking ì ìš©
    pruned_params = 0
    for name, param in pruned_model.named_parameters():
        if param.dim() >= 2:  # 2ì°¨ì› ì´ìƒì˜ ê°€ì¤‘ì¹˜ë§Œ ì²˜ë¦¬ (bias ì œì™¸)
            # ì ˆëŒ“ê°’(magnitude) ê¸°ë°˜ ì¤‘ìš”ë„ ê³„ì‚°
            magnitude = torch.abs(param.data)
            
            # í”„ë£¨ë‹í•  ê°€ì¤‘ì¹˜ ê°œìˆ˜ ê³„ì‚°
            num_to_prune = int(pruning_rate * param.numel())
            
            if num_to_prune > 0 and num_to_prune < param.numel():
                # ê°€ì¤‘ì¹˜ë¥¼ í¬ê¸° ìˆœì„œë¡œ ì •ë ¬í•˜ì—¬ ê°€ì¥ ì‘ì€ ê²ƒë¶€í„° ì„ íƒ
                threshold = torch.kthvalue(magnitude.flatten(), num_to_prune).values
                
                # ì„ê³„ê°’ ì´í•˜ì˜ ê°€ì¤‘ì¹˜ë¥¼ 0ìœ¼ë¡œ ì„¤ì • (ë§ˆìŠ¤í‚¹)
                mask = magnitude > threshold
                param.data = param.data * mask.float()
                pruned_params += num_to_prune
    
    pruned_model.eval()
    
    # íŒŒë¼ë¯¸í„° ê°œìˆ˜ëŠ” ë³€í•˜ì§€ ì•ŠìŒ (êµ¬ì¡° ë³€ê²½ ì—†ìŒ)
    # í•˜ì§€ë§Œ 0ì´ ì•„ë‹Œ íŒŒë¼ë¯¸í„° ê°œìˆ˜ëŠ” ê°ì†Œ
    total_params = sum(p.numel() for p in pruned_model.parameters())
    nonzero_params = sum((p != 0).sum().item() for p in pruned_model.parameters())
    weight_sparsity = (1 - nonzero_params / total_params) * 100
    
    print(f"   âœ‚ï¸ Magnitude-based Pruning ì™„ë£Œ: ê°€ì¤‘ì¹˜ ë§ˆìŠ¤í‚¹ ê¸°ë°˜, {pruning_rate*100:.0f}% ê°€ì¤‘ì¹˜ ì œê±°")
    print(f"   ğŸ“Š í¬ì†Œì„±(Sparsity): {weight_sparsity:.1f}% (êµ¬ì¡° ë³€ê²½ ì—†ìŒ, {total_params:,}ê°œ íŒŒë¼ë¯¸í„° ìœ ì§€)")
    print(f"   ğŸ’¡ ì£¼ì˜: ì¼ë°˜ í•˜ë“œì›¨ì–´ì—ì„œëŠ” 0ì¸ ê°€ì¤‘ì¹˜ë„ ê³„ì‚°ë˜ë¯€ë¡œ ì‹¤ì œ ì†ë„ í–¥ìƒ ë¯¸ë¯¸")
    
    return pruned_model

# ============================================================================
# ë²¤ì¹˜ë§ˆí¬ ì—”ì§„
# ============================================================================

def run_pruning_benchmark(pruned_model, label, img_tensor, wm, rwm, ref_caption, baseline_params, device, results):
    """í”„ë£¨ë‹ëœ ëª¨ë¸ ë²¤ì¹˜ë§ˆí¬ ë° íŒŒì¸íŠœë‹ ì‹¤í–‰"""
    pruned_model.to(device)
    
    # í”„ë£¨ë‹ í›„ ë²¤ì¹˜ë§ˆí¬
    result = run_benchmark(pruned_model, img_tensor, wm, rwm, label, ref_caption, baseline_params=baseline_params)
    if result:
        results.append(result)
    
    # íŒŒì¸ íŠœë‹ (Epochë§ˆë‹¤ ë²¤ì¹˜ë§ˆí¬ ë° ëª¨ë¸ ì €ì¥ í¬í•¨)
    fine_tuned_model = fine_tune_pruned_model(
        pruned_model, wm, 
        img_tensor=img_tensor, wm=wm, rwm=rwm,
        ref_caption=ref_caption, baseline_params=baseline_params,
        epochs=FINETUNE_EPOCHS, label=label.replace(" ", "_").replace("%", "pct"),
        learning_rate=LEARNING_RATE
    )
    fine_tuned_model.to(device)
    
    # íŒŒì¸ íŠœë‹ í›„ ìµœì¢… ë²¤ì¹˜ë§ˆí¬
    result_finetuned = run_benchmark(fine_tuned_model, img_tensor, wm, rwm, f"{label} (Fine-tuned)", ref_caption, baseline_params=baseline_params)
    if result_finetuned:
        results.append(result_finetuned)
    
    del pruned_model, fine_tuned_model
    gc.collect()

def run_benchmark(model, img_tensor, wm, rwm, precision_name, ref_caption=None, baseline_params=None):
    print(f"\n[{precision_name}] ë²¤ì¹˜ë§ˆí¬ ì‹œì‘...")
    
    model_device = next(model.parameters()).device
    inp = img_tensor.clone().detach().to(model_device)
    
    # Warm-up
    with torch.no_grad():
        try:
            _ = model.generate(inp, wm, rwm, 20)
        except Exception as e:
            print(f"âš ï¸ Warm-up ì‹¤íŒ¨: {e}")
            return None
    
    # benchmark_utilsë¥¼ ì‚¬ìš©í•œ ì‹œê°„ ë° ë©”ëª¨ë¦¬ ì¸¡ì •
    inference_metrics = measure_inference_time(model, inp, num_runs=NUM_RUNS, warmup=5)
    
    latencies = [inference_metrics['mean_ms']] * NUM_RUNS  # í‰ê· ê°’ ì‚¬ìš©
    memory_usages = [get_peak_memory_mb()] * NUM_RUNS  # í‰ê·  ë©”ëª¨ë¦¬ ì‚¬ìš©
    
    print(f"   â±ï¸ í‰ê·  ì¶”ë¡  ì‹œê°„: {inference_metrics['mean_ms']:.2f} Â± {inference_metrics['std_ms']:.2f} ms")
    print(f"   ğŸ§  ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: {get_peak_memory_mb():.2f} MB")
    
    # METEOR ì ìˆ˜ ê³„ì‚° (10ê°œ ì´ë¯¸ì§€ë¡œ ì¸¡ì •)
    meteor_scores = []
    example_caption = "N/A"
    
    # 10ê°œì˜ ì´ë¯¸ì§€ë¡œ METEOR ì ìˆ˜ ì¸¡ì •
    test_images_meteor = []
    test_captions_meteor = []
    
    # í…ŒìŠ¤íŠ¸ ì´ë¯¸ì§€ ë””ë ‰í† ë¦¬ì—ì„œ 100ê°œ ì´ë¯¸ì§€ ë¡œë“œ
    if os.path.exists(TEST_IMAGE_DIR):
        image_files = [f for f in os.listdir(TEST_IMAGE_DIR) 
                      if f.lower().endswith(('.jpg', '.png', '.jpeg'))]
        if image_files:
            import random
            # ìµœëŒ€ 10ê°œ ì´ë¯¸ì§€ ì„ íƒ
            selected_files = random.sample(image_files, min(METEO_IMAGE_NUM, len(image_files)))
            
            # ê° ì´ë¯¸ì§€ì™€ ìº¡ì…˜ ë¡œë“œ
            for filename in selected_files:
                try:
                    img_path = os.path.join(TEST_IMAGE_DIR, filename)
                    img = Image.open(img_path).convert('RGB')
                    img_tensor_meteor = transform(img).unsqueeze(0).to(model_device)
                    test_images_meteor.append(img_tensor_meteor)
                    
                    # ì°¸ì¡° ìº¡ì…˜ ë¡œë“œ
                    ref_cap = None
                    if os.path.exists(CAPTIONS_FILE):
                        with open(CAPTIONS_FILE, 'r', encoding='utf-8') as f:
                            lines = f.readlines()
                            for line in lines:
                                if ',' in line:
                                    parts = line.split(',', 1)
                                    if len(parts) == 2 and parts[0].strip() == filename:
                                        ref_cap = parts[1].strip()
                                        break
                    
                    if ref_cap:
                        test_captions_meteor.append(ref_cap)
                    else:
                        test_captions_meteor.append(None)
                except Exception as e:
                    print(f"   âš ï¸ ì´ë¯¸ì§€ ë¡œë“œ ì‹¤íŒ¨ ({filename}): {e}")
                    continue
    
    # ì´ë¯¸ì§€ê°€ ë¶€ì¡±í•˜ë©´ ë”ë¯¸ ë°ì´í„°ë¡œ ì±„ì›€
    while len(test_images_meteor) < 10:
        dummy_img = torch.randn(1, 3, 224, 224).to(model_device)
        test_images_meteor.append(dummy_img)
        test_captions_meteor.append("a test image")
    
    # METEO_IMAGE_NUMê°œ ì´ë¯¸ì§€ì— ëŒ€í•´ METEOR ì ìˆ˜ ê³„ì‚°
    if test_images_meteor and any(test_captions_meteor):
        print(f"   ğŸ“Š METEOR ì ìˆ˜ ì¸¡ì • ì¤‘: {len([c for c in test_captions_meteor if c])}ê°œ ì´ë¯¸ì§€")
        for idx, (test_img, ref_cap) in enumerate(zip(test_images_meteor[:METEO_IMAGE_NUM], test_captions_meteor[:METEO_IMAGE_NUM])):
            if ref_cap:
                with torch.no_grad():
                    gen_seq = model.generate(test_img, wm, rwm, 20)
                meteor = calculate_meteor(gen_seq, ref_cap)
                if meteor is not None:
                    meteor_scores.append(meteor)
                if idx == 0:
                    example_caption = ' '.join([w for w in gen_seq if w not in ['<start>', '<end>', '<pad>', '<unk>']])
                    ref_caption = ref_cap
    
    avg_meteor = np.mean(meteor_scores) if meteor_scores else None
    
    # ê²°ê³¼ ì •ë¦¬
    avg_time = inference_metrics['mean_ms']
    std_time = inference_metrics['std_ms']
    
    # benchmark_utils ì‚¬ìš©: ëª¨ë¸ í¬ê¸° ê³„ì‚°
    size_mb_dense = calculate_model_size_mb(model, model_type='dense')
    size_mb_sparse = calculate_model_size_mb(model, model_type='sparse')
    sparsity = calculate_sparsity(model)
    
    # ì¶”ë¡  ê³¼ì •ì—ì„œì˜ í‰ê·  ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰
    memory_usage = np.mean(memory_usages) if memory_usages else 0.0
    total_params, trainable_params = count_parameters(model)
    nonzero_params, _ = count_nonzero_parameters(model)
    
    print(f"   â±ï¸ í‰ê·  ì‹œê°„: {avg_time:.2f} Â± {std_time:.2f} ms")
    print(f"   ğŸ’¾ ëª¨ë¸ í¬ê¸° (Dense): {size_mb_dense:.2f} MB")
    print(f"   ğŸ’¾ ëª¨ë¸ í¬ê¸° (Sparse): {size_mb_sparse:.2f} MB")
    print(f"   ğŸ“‰ í¬ê¸° ê°ì†Œìœ¨: {(1 - size_mb_sparse/size_mb_dense)*100:.2f}%")
    print(f"   ğŸ“Š ì´ íŒŒë¼ë¯¸í„°: {total_params:,} (0ì´ ì•„ë‹Œ: {nonzero_params:,})")
    print(f"   âœ‚ï¸ Sparsity: {sparsity:.2f}%")
    print(f"   ğŸ§  ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: {memory_usage:.5f} MB")
    if avg_meteor is not None:
        print(f"   â­ METEOR: {avg_meteor:.4f}")
    print(f"   ğŸ“ ì˜ˆì‹œ ìº¡ì…˜: {example_caption}")
    print(f"  ğŸ“ ì°¸ì¡° ìº¡ì…˜{ref_caption}")
    
    return {
        'precision': precision_name,
        'mean_time_ms': avg_time,
        'std_time_ms': std_time,
        'min_time_ms': inference_metrics['min_ms'],
        'max_time_ms': inference_metrics['max_ms'],
        'model_size_mb': size_mb_sparse,  # Sparse format í¬ê¸° ì‚¬ìš©
        'model_size_mb_dense': size_mb_dense,  # Dense format í¬ê¸°ë„ ì €ì¥
        'memory_usage_mb': memory_usage,
        'meteor_score': avg_meteor,
        'inference_times': latencies,
        'example_caption': example_caption,
        'total_params': total_params,
        'nonzero_params': nonzero_params,
        'sparsity': sparsity,
        'trainable_params': trainable_params,
        'size_reduction': (1 - size_mb_sparse/size_mb_dense)*100 if size_mb_dense > 0 else 0
    }

# ============================================================================
# ì‹œê°í™”
# ============================================================================
def plot_pruning_comparison(results):
    """Pruning ê²°ê³¼ ë¹„êµ ê·¸ë˜í”„ (íŒŒì¸ íŠœë‹ ì œì™¸)"""
    if not results:
        print("âŒ ê²°ê³¼ê°€ ì—†ì–´ plotì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    os.makedirs(OUTPUT_DIR, exist_ok=True)
    
    # íŒŒì¸ íŠœë‹ëœ ê²°ê³¼ ì œì™¸
    filtered_results = [r for r in results if '(Fine-tuned)' not in r['precision']]
    
    if not filtered_results:
        print("âŒ íŒŒì¸ íŠœë‹ ì œì™¸ í›„ ê²°ê³¼ê°€ ì—†ì–´ plotì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    precisions = [r['precision'] for r in filtered_results]
    mean_times = [r['mean_time_ms'] for r in filtered_results]
    std_times = [r['std_time_ms'] for r in filtered_results]
    model_sizes = [r['model_size_mb'] for r in filtered_results]
    memory_usages = [r['memory_usage_mb'] for r in filtered_results]
    meteor_scores = [r.get('meteor_score', None) for r in filtered_results]
    sparsities = [r.get('sparsity', 0) * 100 for r in filtered_results]
    nonzero_params_list = [r.get('nonzero_params', 0) for r in filtered_results]
    
    valid_meteor_scores = [s for s in meteor_scores if s is not None]
    valid_meteor_precisions = [p for p, s in zip(precisions, meteor_scores) if s is not None]
    
    # ìƒ‰ìƒ ì„¤ì •
    colors = plt.cm.viridis(np.linspace(0, 1, len(precisions)))
    
    # ì¢…í•© ë¹„êµ ê·¸ë˜í”„
    if valid_meteor_scores:
        fig, axes = plt.subplots(3, 2, figsize=(14, 15))
        fig.suptitle('Pruning ì„±ëŠ¥ ë¹„êµ ì¢…í•©', fontsize=16, fontweight='bold')
        
        # 1. ì¶”ë¡  ì‹œê°„
        axes[0, 0].bar(precisions, mean_times, alpha=0.8, color=colors, yerr=std_times, capsize=5)
        axes[0, 0].set_ylabel('ì¶”ë¡  ì‹œê°„ (ms)', fontweight='bold')
        axes[0, 0].set_title('ì¶”ë¡  ì‹œê°„', fontweight='bold')
        axes[0, 0].grid(axis='y', alpha=0.3)
        axes[0, 0].tick_params(axis='x', rotation=45)
        for i, (p, m, s) in enumerate(zip(precisions, mean_times, std_times)):
            axes[0, 0].text(i, m + s + 1, f'{m:.1f}', ha='center', va='bottom', fontsize=9)
        
        # 2. ëª¨ë¸ í¬ê¸° (Sparse format - ì‹¤ì œ ì €ì¥ í¬ê¸°)
        axes[0, 1].bar(precisions, model_sizes, alpha=0.8, color=colors, label='Sparse (ì‹¤ì œ ì €ì¥ í¬ê¸°)')
        axes[0, 1].set_ylabel('ëª¨ë¸ í¬ê¸° (MB)', fontweight='bold')
        axes[0, 1].set_title('ëª¨ë¸ í¬ê¸° (Sparse Format)', fontweight='bold')
        axes[0, 1].grid(axis='y', alpha=0.3)
        axes[0, 1].tick_params(axis='x', rotation=45)
        axes[0, 1].legend()
        for i, (p, s) in enumerate(zip(precisions, model_sizes)):
            axes[0, 1].text(i, s + 0.5, f'{s:.1f}', ha='center', va='bottom', fontsize=9)
        
        # 3. Sparsity (íŒŒë¼ë¯¸í„° ê°ì†Œìœ¨)
        axes[1, 0].bar(precisions, sparsities, alpha=0.8, color=colors)
        axes[1, 0].set_ylabel('íŒŒë¼ë¯¸í„° ê°ì†Œìœ¨ (%)', fontweight='bold')
        axes[1, 0].set_title('íŒŒë¼ë¯¸í„° ê°ì†Œìœ¨ (Baseline ëŒ€ë¹„)', fontweight='bold')
        axes[1, 0].grid(axis='y', alpha=0.3)
        axes[1, 0].tick_params(axis='x', rotation=45)
        # Yì¶• ë²”ìœ„ ì„¤ì • (0-100%)
        axes[1, 0].set_ylim(0, max(sparsities) * 1.2 if sparsities else 100)
        for i, (p, s) in enumerate(zip(precisions, sparsities)):
            axes[1, 0].text(i, s + max(sparsities) * 0.02 if sparsities else 1, 
                          f'{s:.1f}%', ha='center', va='bottom', fontsize=9)
        
        # 4. METEOR ì ìˆ˜
        axes[1, 1].bar(valid_meteor_precisions, valid_meteor_scores, alpha=0.8, 
                     color=colors[:len(valid_meteor_scores)])
        axes[1, 1].set_ylabel('METEOR ì ìˆ˜', fontweight='bold')
        axes[1, 1].set_title('METEOR ì ìˆ˜ (ìº¡ì…˜ í’ˆì§ˆ)', fontweight='bold')
        axes[1, 1].set_ylim(0, 1.0)
        axes[1, 1].grid(axis='y', alpha=0.3)
        axes[1, 1].tick_params(axis='x', rotation=45)
        for i, (p, s) in enumerate(zip(valid_meteor_precisions, valid_meteor_scores)):
            axes[1, 1].text(i, s + 0.01, f'{s:.2f}', ha='center', va='bottom', fontsize=9)
        
        # 5. ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰
        axes[2, 0].bar(precisions, memory_usages, alpha=0.8, color=colors)
        axes[2, 0].set_ylabel('ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ (MB)', fontweight='bold')
        axes[2, 0].set_title('ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰', fontweight='bold')
        axes[2, 0].grid(axis='y', alpha=0.3)
        axes[2, 0].tick_params(axis='x', rotation=45)
        for i, (p, m) in enumerate(zip(precisions, memory_usages)):
            axes[2, 0].text(i, m + max(memory_usages) * 0.02, f'{m:.1f}', ha='center', va='bottom', fontsize=9)
        
        # 6. ì´ íŒŒë¼ë¯¸í„° ê°œìˆ˜ (ë¬¼ë¦¬ì  êµ¬ì¡° ë³€ê²½ í›„ ì‹¤ì œ íŒŒë¼ë¯¸í„° ìˆ˜)
        total_params_list = [r.get('total_params', 0) for r in filtered_results]
        total_params_m = [p / 1e6 for p in total_params_list]
        axes[2, 1].bar(precisions, total_params_m, alpha=0.8, color=colors)
        axes[2, 1].set_ylabel('ì´ íŒŒë¼ë¯¸í„° (M)', fontweight='bold')
        axes[2, 1].set_title('ì´ íŒŒë¼ë¯¸í„° ê°œìˆ˜ (ë¬¼ë¦¬ì  êµ¬ì¡° ë³€ê²½ í›„)', fontweight='bold')
        axes[2, 1].grid(axis='y', alpha=0.3)
        axes[2, 1].tick_params(axis='x', rotation=45)
        for i, (p, tp_m) in enumerate(zip(precisions, total_params_m)):
            axes[2, 1].text(i, tp_m + max(total_params_m) * 0.02 if total_params_m else 0.1, 
                          f'{tp_m:.2f}M', ha='center', va='bottom', fontsize=9)
    
    plt.tight_layout()
    plt.savefig(os.path.join(OUTPUT_DIR, 'pruning_comparison_comprehensive.png'), 
                dpi=300, bbox_inches='tight')
    print(f"âœ… Plot ì €ì¥: {os.path.join(OUTPUT_DIR, 'pruning_comparison_comprehensive.png')}")
    plt.close()

def plot_finetune_comparison(results, baseline_result):
    """íŒŒì¸ íŠœë‹ ì „í›„ METEOR ì ìˆ˜ ë¹„êµ ê·¸ë˜í”„"""
    if not results or not baseline_result:
        print("âŒ ê²°ê³¼ê°€ ì—†ì–´ plotì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    os.makedirs(OUTPUT_DIR, exist_ok=True)
    
    # í”„ë£¨ë‹ í›„(Fine-tuned ì œì™¸)ì™€ íŒŒì¸íŠœë‹ í›„(Fine-tuned) ê²°ê³¼ ë¶„ë¦¬
    before_finetune = {r['precision']: r for r in results if '(Fine-tuned)' not in r['precision']}
    after_finetune = {r['precision'].replace(' (Fine-tuned)', ''): r for r in results if '(Fine-tuned)' in r['precision']}
    
    # ë§¤ì¹­ë˜ëŠ” ëª¨ë¸ ì°¾ê¸°
    model_names = []
    meteor_before = []
    meteor_after = []
    
    baseline_meteor = baseline_result.get('meteor_score', 0)
    
    for model_name in sorted(after_finetune.keys()):
        if model_name in before_finetune:
            before = before_finetune[model_name]
            after = after_finetune[model_name]
            
            model_names.append(model_name)
            meteor_before.append(before.get('meteor_score', 0))
            meteor_after.append(after.get('meteor_score', 0))
    
    if not model_names:
        print("âŒ íŒŒì¸ íŠœë‹ ì „í›„ ë¹„êµí•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    # ê·¸ë˜í”„ ìƒì„±
    fig, ax = plt.subplots(figsize=(12, 6))
    
    x = np.arange(len(model_names))
    width = 0.25
    
    # Baseline ì„ 
    ax.axhline(y=baseline_meteor, color='red', linestyle='--', linewidth=2, label=f'Baseline ({baseline_meteor:.4f})', alpha=0.7)
    
    # íŒŒì¸íŠœë‹ ì „ (í”„ë£¨ë‹ í›„)
    bars1 = ax.bar(x - width, meteor_before, width, label='Pruned (Before Fine-tuning)', alpha=0.8, color='steelblue')
    
    # íŒŒì¸íŠœë‹ í›„
    bars2 = ax.bar(x, meteor_after, width, label='Pruned (After Fine-tuning)', alpha=0.8, color='orange')
    
    # ë ˆì´ë¸” ë° ì œëª©
    ax.set_ylabel('METEOR Score', fontweight='bold', fontsize=12)
    ax.set_title('íŒŒì¸íŠœë‹ ì „í›„ METEOR ì ìˆ˜ ë¹„êµ', fontweight='bold', fontsize=14)
    ax.set_xticks(x)
    ax.set_xticklabels(model_names, rotation=45, ha='right')
    ax.legend(fontsize=10)
    ax.grid(axis='y', alpha=0.3)
    
    # ê°’ í‘œì‹œ
    for i, (before, after) in enumerate(zip(meteor_before, meteor_after)):
        # íŒŒì¸íŠœë‹ ì „
        ax.text(i - width, before + 0.005, f'{before:.4f}', ha='center', va='bottom', fontsize=9)
        # íŒŒì¸íŠœë‹ í›„
        ax.text(i, after + 0.005, f'{after:.4f}', ha='center', va='bottom', fontsize=9)
        
        # ê°œì„ ìœ¨ í‘œì‹œ
        improvement = ((after - before) / before * 100) if before != 0 else 0
        improvement_text = f'{improvement:+.1f}%'
        ax.text(i + width/2, max(before, after) + 0.01, improvement_text, 
                ha='center', va='bottom', fontsize=9, fontweight='bold', 
                color='green' if improvement > 0 else 'red')
    
    plt.tight_layout()
    plt.savefig(os.path.join(OUTPUT_DIR, 'pruning_finetune_meteor_comparison.png'), 
                dpi=300, bbox_inches='tight')
    print(f"âœ… íŒŒì¸íŠœë‹ METEOR ë¹„êµ Plot ì €ì¥: {os.path.join(OUTPUT_DIR, 'pruning_finetune_meteor_comparison.png')}")
    plt.close()

# ============================================================================
# íŒŒì¸ íŠœë‹ í•¨ìˆ˜
# ============================================================================
def fine_tune_pruned_model(model, word_map, img_tensor=None, wm=None, rwm=None, ref_caption=None, baseline_params=None, epochs=2, label="pruned_model", learning_rate=5e-5):
    """íŒŒì¸íŠœë‹ ìˆ˜í–‰ + Epochë§ˆë‹¤ ë²¤ì¹˜ë§ˆí¬ ë° ëª¨ë¸ ì €ì¥ + ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ"""
    print(f"\n   ğŸ”„ íŒŒì¸ íŠœë‹ ì‹œì‘ ({epochs} epoch)...")
    
    # ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ
    checkpoint, start_epoch, checkpoint_path = load_checkpoint(label, device)
    optimizer_state = checkpoint.get('optimizer_state_dict') if checkpoint else None
    if checkpoint:
        model.load_state_dict(checkpoint['model_state_dict'])
        print_checkpoint_info(checkpoint, start_epoch)
        print(f"   âœ… Epoch {start_epoch+1}ë¶€í„° ì¬ê°œí•©ë‹ˆë‹¤.")
    else:
        print(f"   â„¹ï¸ ì²˜ìŒë¶€í„° ì‹œì‘í•©ë‹ˆë‹¤.")
    
    # í•™ìŠµ ì„¤ì •
    optimizer, criterion = setup_training(model, learning_rate, device)
    restore_optimizer(optimizer, optimizer_state)
    
    # í•™ìŠµ ë°ì´í„°ì…‹ ì¤€ë¹„ (í•™ìŠµ/ê²€ì¦ ë¶„í• )
    try:
        full_dataset = CaptionDataset(
            images_dir=TEST_IMAGE_DIR,
            captions_file=CAPTIONS_FILE,
            transform=transform,
            word_map=word_map,
            max_len=50
        )
        
        if len(full_dataset) == 0:
            print("   âš ï¸ í•™ìŠµ ë°ì´í„°ê°€ ì—†ì–´ íŒŒì¸ íŠœë‹ì„ ê±´ë„ˆëœë‹ˆë‹¤.")
            return model
        
        # í•™ìŠµ/ê²€ì¦ ë°ì´í„°ì…‹ ë¶„í• 
        val_size = int(len(full_dataset) * VALIDATION_SPLIT)
        train_size = len(full_dataset) - val_size
        train_dataset, val_dataset = torch.utils.data.random_split(
            full_dataset, [train_size, val_size],
            generator=torch.Generator().manual_seed(42)
        )
        
        print(f"   ğŸ“Š ë°ì´í„°ì…‹ ë¶„í• : í•™ìŠµ({train_size}ê°œ) / ê²€ì¦({val_size}ê°œ)")
        
        # ì ì‘í˜• ë°°ì¹˜ ì‚¬ì´ì¦ˆ
        batch_size = 32 if train_size < 1000 else 64
        
        train_dataloader = DataLoader(
            train_dataset,
            batch_size=batch_size,
            shuffle=True,
            num_workers=0,
            pin_memory=False
        )
        
        val_dataloader = DataLoader(
            val_dataset,
            batch_size=batch_size,
            shuffle=False,
            num_workers=0,
            pin_memory=False
        )
        
        print(f"   ğŸ“š í•™ìŠµ ë°°ì¹˜: {len(train_dataloader)}ê°œ, ê²€ì¦ ë°°ì¹˜: {len(val_dataloader)}ê°œ")
        
        # ëª¨ë¸ì„ í•™ìŠµ ëª¨ë“œë¡œ ì „í™˜
        model.train()
        model.to(device)
        
        # ì²´í¬í¬ì¸íŠ¸ì—ì„œ Optimizer State ë³µêµ¬
        if optimizer_state is not None:
            try:
                optimizer.load_state_dict(optimizer_state)
                print(f"   âœ… Optimizer State ë³µêµ¬ ì™„ë£Œ (Learning Rate, Momentum ë“± ë³µì›)")
            except Exception as e:
                print(f"   âš ï¸ Optimizer State ë³µêµ¬ ì‹¤íŒ¨: {e}")
        
        # í•™ìŠµí•  íŒŒë¼ë¯¸í„° ê°œìˆ˜ ì¶œë ¥
        trainable_count = sum(p.numel() for p in model.parameters() if p.requires_grad)
        total_count = sum(p.numel() for p in model.parameters())
        print(f"   ğŸ“Š í•™ìŠµ ëŒ€ìƒ íŒŒë¼ë¯¸í„°: {trainable_count:,} / {total_count:,} ({100*trainable_count/total_count:.1f}%)")
        vocab_size = len(word_map)
        
        # Early Stopping ì„¤ì •
        best_meteor_score = -float('inf')
        patience_counter = 0
        
        # íŒŒì¸íŠœë‹ ì§„í–‰ (ì²´í¬í¬ì¸íŠ¸ ì´í›„ë¶€í„° ì‹œì‘)
        for epoch in range(start_epoch, epochs):
            print(f"   ğŸ‹ï¸ Epoch {epoch+1}/{epochs}")
            total_loss = 0
            num_batches = 0
            
            for batch_idx, (imgs, caps) in enumerate(train_dataloader):
                imgs = imgs.to(device)
                caps = caps.to(device)
                
                optimizer.zero_grad()
            
                try:
                    outputs, alphas = model(imgs, caps)
                    targets = caps[:, 1:]
                    outputs = outputs[:, :targets.shape[1], :]
                    loss = criterion(outputs.reshape(-1, vocab_size), targets.reshape(-1))
                    loss.backward()
                    optimizer.step()
                    
                    total_loss += loss.item()
                    num_batches += 1
                except Exception as e:
                    print(f"   âš ï¸ ë°°ì¹˜ {batch_idx} í•™ìŠµ ì‹¤íŒ¨: {e}")
                    continue
                
                # 10ê°œ ë°°ì¹˜ë§ˆë‹¤ ì§„í–‰ìƒí™© ì¶œë ¥
                if (batch_idx + 1) % 10 == 0:
                    print(f"      ë°°ì¹˜ {batch_idx + 1}/{len(train_dataloader)}, Loss: {total_loss / num_batches:.4f}")
            
            # ğŸ¯ Epoch ë - í•™ìŠµ Loss ê³„ì‚°
            if num_batches > 0:
                avg_loss = total_loss / num_batches
                print(f"   âœ… Epoch {epoch+1} ì™„ë£Œ (í•™ìŠµ Loss: {avg_loss:.4f})")
            
            # ğŸ” ê²€ì¦ ë°ì´í„°ì…‹ í‰ê°€
            print(f"   ğŸ“Š ê²€ì¦ ë°ì´í„° í‰ê°€ ì¤‘...")
            model.eval()
            val_loss = 0
            val_batches = 0
            
            with torch.no_grad():
                for val_imgs, val_caps in val_dataloader:
                    val_imgs = val_imgs.to(device)
                    val_caps = val_caps.to(device)
                    
                    try:
                        val_outputs, _ = model(val_imgs, val_caps)
                        val_targets = val_caps[:, 1:]
                        val_outputs = val_outputs[:, :val_targets.shape[1], :]
                        val_loss_batch = criterion(val_outputs.reshape(-1, vocab_size), val_targets.reshape(-1))
                        val_loss += val_loss_batch.item()
                        val_batches += 1
                    except Exception as e:
                        continue
            
            avg_val_loss = val_loss / val_batches if val_batches > 0 else float('inf')
            print(f"      ê²€ì¦ Loss: {avg_val_loss:.4f}")
            
            model.train()  # ë‹¤ì‹œ í•™ìŠµ ëª¨ë“œ
            
            # ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰ (img_tensor, wm, rwmì´ ì œê³µëœ ê²½ìš°)
            current_meteor_score = None
            if img_tensor is not None and wm is not None and rwm is not None:
                print(f"\n   ğŸ“Š Epoch {epoch+1} ë²¤ì¹˜ë§ˆí¬ ì‹œì‘...")
                model.eval()
                benchmark_result = run_benchmark(
                    model, img_tensor, wm, rwm, 
                    f"Fine-tuned (Epoch {epoch+1}/{epochs})",
                    ref_caption=ref_caption,
                    baseline_params=baseline_params
                )
                model.train()  # ë‹¤ì‹œ í•™ìŠµ ëª¨ë“œë¡œ
                
                # ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼ ì¶œë ¥
                if benchmark_result:
                    print(f"\n   ğŸ“ˆ Epoch {epoch+1} ê²°ê³¼:")
                    print(f"      â±ï¸ í‰ê·  ì‹œê°„: {benchmark_result['mean_time_ms']:.2f} ms")
                    print(f"      ğŸ’¾ ëª¨ë¸ í¬ê¸°: {benchmark_result['model_size_mb']:.2f} MB")
                    print(f"      ğŸ§  ë©”ëª¨ë¦¬: {benchmark_result['memory_usage_mb']:.2f} MB")
                    if benchmark_result.get('meteor_score'):
                        current_meteor_score = benchmark_result['meteor_score']
                        print(f"      â­ METEOR: {current_meteor_score:.4f}")
            
            # ğŸ›‘ Early Stopping ì²´í¬ (METEOR ì ìˆ˜ ê¸°ë°˜)
            if current_meteor_score is not None:
                if current_meteor_score > best_meteor_score:
                    best_meteor_score = current_meteor_score
                    patience_counter = 0
                    best_model_state = model.state_dict().copy()
                    print(f"   ğŸ‰ ìƒˆë¡œìš´ ìµœê³  METEOR ì ìˆ˜: {best_meteor_score:.4f} (Patience: {patience_counter}/{EARLY_STOPPING_PATIENCE})")
                else:
                    patience_counter += 1
                    print(f"   âš ï¸ METEOR ì ìˆ˜ ë¯¸ê°œì„ : {current_meteor_score:.4f} (Patience: {patience_counter}/{EARLY_STOPPING_PATIENCE})")
                    
                    if patience_counter >= EARLY_STOPPING_PATIENCE:
                        print(f"\n   ğŸ›‘ Early Stopping ë°œë™! Epoch {epoch+1}ì—ì„œ í•™ìŠµ ì¢…ë£Œ")
                        print(f"      ìµœê³  METEOR ì ìˆ˜: {best_meteor_score:.4f}")
                        # ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ë¡œë“œ
                        model.load_state_dict(best_model_state)
                        break
                
                # ì²´í¬í¬ì¸íŠ¸ ì €ì¥ (í•¨ìˆ˜ ì‚¬ìš©)
                save_checkpoint(model, optimizer, epoch, label, 
                               avg_loss if num_batches > 0 else None,
                               avg_val_loss, current_meteor_score)
        
        model.eval()
        return model
        
    except Exception as e:
        print(f"   âš ï¸ íŒŒì¸ íŠœë‹ ì‹¤íŒ¨: {e}")
        import traceback
        traceback.print_exc()
        return model

# ============================================================================
# Main
# ============================================================================
def main():
    os.makedirs(OUTPUT_DIR, exist_ok=True)
    
    print("="*70)
    print("=== Pruning ë²¤ì¹˜ë§ˆí¬ ===")
    print("="*70)
    
    # 1. ëª¨ë¸ ë° ë°ì´í„° ë¡œë“œ
    base_model, wm, rwm = load_base_model(device=device)
    img_tensor, ref_caption = load_test_data(device=device, transform=transform)
    
    results = []
    
    # 2. ì›ë³¸ ëª¨ë¸ ë²¤ì¹˜ë§ˆí¬ (Baseline)
    print("\n" + "="*70)
    print("=== [Baseline] ì›ë³¸ ëª¨ë¸ ===")
    print("="*70)
    result_baseline = run_benchmark(base_model, img_tensor, wm, rwm, "Original (Baseline)", ref_caption)
    baseline_params = None
    if result_baseline:
        baseline_params = result_baseline['total_params']
        results.append(result_baseline)
    
    # 3. ë‹¤ì–‘í•œ Pruning Rateë¡œ í…ŒìŠ¤íŠ¸
    for pruning_rate in PRUNING_RATES:
        # Magnitude-based Pruning (ì„ íƒì  - ì´ ëª¨ë¸ì—ëŠ” ë¹„íš¨ìœ¨ì )
        if ENABLE_MAGNITUDE_PRUNING:
            print("\n" + "="*70)
            print(f"=== Magnitude Pruning ({pruning_rate*100:.0f}%) ===")
            print("="*70)
            try:
                pruned_model = apply_magnitude_pruning(base_model, pruning_rate)
                run_pruning_benchmark(pruned_model, f"Magnitude-{pruning_rate*100:.0f}%", img_tensor, wm, rwm, ref_caption, baseline_params, device, results)
            except Exception as e:
                print(f"âš ï¸ Magnitude Pruning ({pruning_rate*100:.0f}%) ì‹¤íŒ¨: {e}")
                import traceback
                traceback.print_exc()
        
        # Structured Pruning
        print("\n" + "="*70)
        print(f"=== Structured Pruning ({pruning_rate*100:.0f}%) ===")
        print("="*70)
        
        # âš ï¸ 30% ì´ìƒ í”„ë£¨ë‹ì€ ì •í™•ë„ ê¸‰ê²©íˆ í•˜ë½í•˜ë¯€ë¡œ ê²½ê³ 
        if pruning_rate > MAX_PRUNING_RATE:
            print(f"   âš ï¸ ê²½ê³ : {pruning_rate*100:.0f}% í”„ë£¨ë‹ì€ ì •í™•ë„ ì†ì‹¤ì´ ë§¤ìš° í¼ (ê¶Œì¥: {MAX_PRUNING_RATE*100:.0f}% ì´í•˜)")
        
        try:
            pruned_model = apply_structured_pruning_physical(
                base_model, pruning_rate, 
                img_tensor=img_tensor, wm=wm, rwm=rwm, 
                device=device, use_hessian=True
            )
            run_pruning_benchmark(pruned_model, f"Structured-{pruning_rate*100:.0f}%", img_tensor, wm, rwm, ref_caption, baseline_params, device, results)
            # run_benchmark(pruned_model, img_tensor, wm, rwm, f"Structured-{pruning_rate*100:.0f}%", ref_caption)

        except Exception as e:
            print(f"âš ï¸ Structured Pruning ({pruning_rate*100:.0f}%) ì‹¤íŒ¨: {e}")
            import traceback
            traceback.print_exc()
    
    
    
    print("\n" + "="*70)
    print("=== ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼ ìš”ì•½ ===")
    print("="*70)
    if any(r.get('meteor_score') is not None for r in results):
        print(f"{'Method':<25} {'ì¶”ë¡ ì‹œê°„(ms)':<15} {'ëª¨ë¸í¬ê¸°(MB)':<15} {'í¬ê¸°ê°ì†Œ(%)':<15} {'Sparsity(%)':<15} {'METEOR':<10}")
        print("-"*100)
        for result in results:
            meteor_str = f"{result.get('meteor_score', 0):.4f}" if result.get('meteor_score') is not None else "N/A"
            sparsity = result.get('sparsity', 0) * 100
            size_reduction = result.get('size_reduction', 0)
            print(f"{result['precision']:<25} "
                  f"{result['mean_time_ms']:.2f}Â±{result['std_time_ms']:.2f}    "
                  f"{result['model_size_mb']:.2f}          "
                  f"{size_reduction:<15.2f} "
                  f"{sparsity:<15.2f} "
                  f"{meteor_str}")
    else:
        print(f"{'Method':<25} {'ì¶”ë¡ ì‹œê°„(ms)':<15} {'ëª¨ë¸í¬ê¸°(MB)':<15} {'í¬ê¸°ê°ì†Œ(%)':<15} {'Sparsity(%)':<15}")
        print("-"*85)
        for result in results:
            sparsity = result.get('sparsity', 0) * 100
            size_reduction = result.get('size_reduction', 0)
            print(f"{result['precision']:<25} "
                  f"{result['mean_time_ms']:.2f}Â±{result['std_time_ms']:.2f}    "
                  f"{result['model_size_mb']:.2f}          "
                  f"{size_reduction:<15.2f} "
                  f"{sparsity:<15.2f}")
    
    # 6. ê²°ê³¼ ì €ì¥
    print("\n" + "="*70)
    print("ê²°ê³¼ ì €ì¥ ì¤‘...")
    print("="*70)
    
    # JSON í˜•ì‹ìœ¼ë¡œ ê²°ê³¼ ì €ì¥
    import json
    results_dict = {
        'baseline_params': baseline_params,
        'results': []
    }
    
    for result in results:
        result_dict = {
            'precision': result['precision'],
            'mean_time_ms': float(result['mean_time_ms']),
            'std_time_ms': float(result['std_time_ms']),
            'min_time_ms': float(result['min_time_ms']),
            'max_time_ms': float(result['max_time_ms']),
            'model_size_mb': float(result['model_size_mb']),
            'model_size_mb_dense': float(result.get('model_size_mb_dense', 0)),
            'memory_usage_mb': float(result['memory_usage_mb']),
            'meteor_score': float(result.get('meteor_score', 0)) if result.get('meteor_score') is not None else None,
            'total_params': int(result['total_params']),
            'trainable_params': int(result.get('trainable_params', 0)),
            'nonzero_params': int(result.get('nonzero_params', 0)),
            'sparsity': float(result.get('sparsity', 0)),
            'size_reduction': float(result.get('size_reduction', 0)),
            'example_caption': result.get('example_caption', 'N/A')
        }
        results_dict['results'].append(result_dict)
    
    results_json_path = os.path.join(OUTPUT_DIR, 'pruning_results.json')
    with open(results_json_path, 'w', encoding='utf-8') as f:
        json.dump(results_dict, f, indent=2, ensure_ascii=False)
    print(f"âœ… ê²°ê³¼ JSON ì €ì¥: {results_json_path}")
    
    # 7. ì‹œê°í™”
    print("\n" + "="*70)
    print("Plot ìƒì„± ì¤‘...")
    print("="*70)
    plot_pruning_comparison(results)
    
    # íŒŒì¸ íŠœë‹ ë¹„êµ ê·¸ë˜í”„ ìƒì„±
    if result_baseline:
        plot_finetune_comparison(results, result_baseline)
    
    print("\n" + "="*70)
    print("=== ë²¤ì¹˜ë§ˆí¬ ì™„ë£Œ ===")
    print(f"ê²°ê³¼ ì €ì¥ ìœ„ì¹˜: {OUTPUT_DIR}")
    print(f"  - JSON: {results_json_path}")
    print(f"  - Plot: {os.path.join(OUTPUT_DIR, 'pruning_comparison_comprehensive.png')}")
    print("="*70)

if __name__ == "__main__":
    main()

