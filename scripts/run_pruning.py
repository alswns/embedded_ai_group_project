"""
Pruning ë²¤ì¹˜ë§ˆí¬ ìŠ¤í¬ë¦½íŠ¸
ë‹¤ì–‘í•œ Pruning ê¸°ë²•ì„ ì ìš©í•˜ê³  ì„±ëŠ¥ì„ ë¹„êµí•©ë‹ˆë‹¤.
"""
import torch
import torch.nn as nn
import torch.nn.utils.prune as prune
from torch.utils.data import DataLoader
from src.utils import CaptionDataset
import numpy as np
import os
import time
import matplotlib.pyplot as plt
from copy import deepcopy
import gc
import warnings
from PIL import Image

warnings.filterwarnings('ignore')

# ê³µí†µ ìœ í‹¸ë¦¬í‹° import
from src.utils import (
    setup_device,
    setup_matplotlib,
    get_image_transform,
    count_parameters,
    get_model_size_mb,
    get_peak_memory_mb,
    calculate_meteor,
    load_test_data,
    load_base_model,
    TEST_IMAGE_DIR,
    CAPTIONS_FILE,
)

# ============================================================================
# ì„¤ì •
# ============================================================================
setup_matplotlib()

OUTPUT_DIR = "pruning_results"
NUM_RUNS = 50

# Pruning ì„¤ì •
PRUNING_RATES = [0.1, 0.3, 0.5, 0.7, 0.9]  # 10%, 30%, 50%, 70%, 90% í”„ë£¨ë‹
PRUNING_RATES = [0.3]
PRUNING_METHODS = ['magnitude', 'structured']  # í”„ë£¨ë‹ ë°©ë²•
ENABLE_MAGNITUDE_PRUNING = False  # âš ï¸ Magnitude Pruningì€ ì´ ëª¨ë¸ì— ë¹„íš¨ìœ¨ì  (ê²°ê³¼ ì°¸ê³ )
MAX_PRUNING_RATE = 0.51  # âš ï¸ 30% ì´ìƒ í”„ë£¨ë‹ì€ ì •í™•ë„ ê¸‰ê²©íˆ í•˜ë½ (50% ì´ìƒì€ ê±°ì˜ ì‘ë™ ë¶ˆê°€)
METEO_IMAGE_NUM=100
# ë””ë°”ì´ìŠ¤ ì„ íƒ
device = setup_device()

# ì´ë¯¸ì§€ ì „ì²˜ë¦¬
transform = get_image_transform()

# ============================================================================
# Pruning ì „ìš© ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜
# ============================================================================
def count_nonzero_parameters(model):
    """0ì´ ì•„ë‹Œ íŒŒë¼ë¯¸í„° ê°œìˆ˜ ê³„ì‚° (í”„ë£¨ë‹ í›„)"""
    nonzero_params = 0
    total_params = 0
    for param in model.parameters():
        total_params += param.numel()
        nonzero_params += param.nonzero().size(0) if param.numel() > 0 else 0
    return nonzero_params, total_params

def convert_to_sparse_model(model):
    """Pruningëœ ëª¨ë¸ì„ ì‹¤ì œë¡œ sparse formatìœ¼ë¡œ ë³€í™˜í•˜ì—¬ í¬ê¸° ê°ì†Œ"""
    # ì£¼ì˜: ì‹¤ì œë¡œ ëª¨ë¸ êµ¬ì¡°ë¥¼ ë³€ê²½í•˜ëŠ” ê²ƒì€ ë³µì¡í•˜ë¯€ë¡œ
    # ì—¬ê¸°ì„œëŠ” ê°€ì¤‘ì¹˜ë¥¼ sparse tensorë¡œ ë³€í™˜í•˜ëŠ” ëŒ€ì‹ 
    # ì‹¤ì œ 0ì´ ì•„ë‹Œ íŒŒë¼ë¯¸í„°ë§Œ ê³„ì‚°í•˜ëŠ” ë°©ì‹ ì‚¬ìš©
    # ì‹¤ì œ ë°°í¬ ì‹œì—ëŠ” sparse formatìœ¼ë¡œ ì €ì¥/ë¡œë“œí•˜ëŠ” ê²ƒì´ ì¢‹ìŠµë‹ˆë‹¤
    return model

def save_sparse_model(model, path):
    """ëª¨ë¸ì„ sparse formatìœ¼ë¡œ ì €ì¥ (ì‹¤ì œ í¬ê¸° ê°ì†Œ)"""
    state_dict = {}
    for name, param in model.named_parameters():
        if param.numel() > 0:
            # 0ì´ ì•„ë‹Œ ê°’ë§Œ ì €ì¥
            nonzero_mask = param != 0
            if nonzero_mask.any():
                # Sparse formatìœ¼ë¡œ ì €ì¥
                sparse_param = param[nonzero_mask]
                indices = nonzero_mask.nonzero(as_tuple=False)
                state_dict[name] = {
                    'values': sparse_param.cpu(),
                    'indices': indices.cpu(),
                    'shape': list(param.shape),
                    'dtype': str(param.dtype)
                }
            else:
                # ëª¨ë“  ê°’ì´ 0ì¸ ê²½ìš°
                state_dict[name] = {
                    'values': torch.tensor([], dtype=param.dtype),
                    'indices': torch.tensor([], dtype=torch.long),
                    'shape': list(param.shape),
                    'dtype': str(param.dtype)
                }
        else:
            state_dict[name] = param.cpu()
    
    # ë²„í¼ë„ ì €ì¥
    for name, buffer in model.named_buffers():
        state_dict[name] = buffer.cpu()
    
    torch.save(state_dict, path)
    print(f"   ğŸ’¾ Sparse ëª¨ë¸ ì €ì¥: {path}")

def get_sparse_model_size_mb(model):
    """Sparse formatìœ¼ë¡œ ì €ì¥í–ˆì„ ë•Œì˜ ì‹¤ì œ ëª¨ë¸ í¬ê¸° ê³„ì‚° (ë” í˜„ì‹¤ì ì¸ ê³„ì‚°)"""
    total_size = 0
    
    for name, param in model.named_parameters():
        if param.numel() > 0:
            # 0ì´ ì•„ë‹Œ ê°’ì˜ ê°œìˆ˜
            nonzero_count = (param != 0).sum().item()
            total_params = param.numel()
            
            if nonzero_count > 0:
                # ê°’ ì €ì¥ (0ì´ ì•„ë‹Œ ê°’ë§Œ) - float32 = 4 bytes
                total_size += nonzero_count * param.element_size()
                
                # ì¸ë±ìŠ¤ ì €ì¥ (ë” íš¨ìœ¨ì ì¸ ì••ì¶• ì‚¬ìš©) - int16 = 2 bytes per dimension
                # CSR (Compressed Sparse Row) format ê°€ì •
                num_dimensions = len(param.shape)
                # row pointer: shape[0] + 1 elements
                # column indices: nonzero_count elements
                # í‰ê· ì ìœ¼ë¡œ ê° ë‹¤ì°¨ì›ë³„ë¡œ 2 bytes
                indices_size = int(nonzero_count * num_dimensions * 2)
                
                total_size += indices_size
                
                # ë©”íƒ€ë°ì´í„° ìµœì†Œí™” (í˜•íƒœ, dtype ë“±)
                total_size += 16  # ìµœì†Œ ë©”íƒ€ë°ì´í„°
            else:
                # ëª¨ë“  ê°’ì´ 0ì¸ ê²½ìš° ìµœì†Œ ë©”íƒ€ë°ì´í„°ë§Œ
                total_size += 8
    
    # ë²„í¼ í¬ê¸° (ë°°ì¹˜ ì •ê·œí™” ë“±)
    for name, buffer in model.named_buffers():
        total_size += buffer.nelement() * buffer.element_size()
    
    return total_size / 1024 / 1024

# ============================================================================
# ë°ì´í„° ë¡œë“œ (ê³µí†µ ëª¨ë“ˆ ì‚¬ìš©)
# ============================================================================
# load_base_model, load_test_dataëŠ” utilsì—ì„œ import

# ============================================================================
# Pruning í•¨ìˆ˜ (ë¬¼ë¦¬ì  êµ¬ì¡° ìˆ˜ì •)
# ============================================================================

def get_pruning_mask(weight, pruning_rate, dim=0, use_l2=True):
    """í”„ë£¨ë‹ ë§ˆìŠ¤í¬ ìƒì„± (ì œê±°í•  ì±„ë„/ë‰´ëŸ° ì‹ë³„)
    
    Args:
        weight: ê°€ì¤‘ì¹˜ í…ì„œ
        pruning_rate: í”„ë£¨ë‹ ë¹„ìœ¨
        dim: í”„ë£¨ë‹í•  ì°¨ì› (0: ì¶œë ¥, 1: ì…ë ¥)
        use_l2: Trueë©´ L2 norm ì‚¬ìš©, Falseë©´ L1 norm ì‚¬ìš©
    """
    # ì¤‘ìš”ë„ ê³„ì‚°: dim ì°¨ì›ì„ ë”°ë¼ ì¶•ì•½
    if use_l2:
        importance = torch.norm(weight, p=2, dim=1 if dim == 0 else 0)
    else:
        importance = torch.abs(weight).sum(dim=1 if dim == 0 else 0)
    
    # ì¤‘ìš”ë„ê°€ ë‚®ì€ ìˆœì„œë¡œ ì •ë ¬
    num_to_prune = int(pruning_rate * importance.numel())
    if num_to_prune == 0:
        return torch.ones(importance.numel(), dtype=torch.bool, device=weight.device)
    
    _, indices = torch.sort(importance)
    mask = torch.ones(importance.numel(), dtype=torch.bool, device=weight.device)
    mask[indices[:num_to_prune]] = False
    return mask

def update_linear_layer(old_layer, mask_in=None, mask_out=None, in_size=None, out_size=None):
    """ì„ í˜• ë ˆì´ì–´ë¥¼ ë§ˆìŠ¤í¬ì— ë”°ë¼ ì—…ë°ì´íŠ¸í•˜ê³  ìƒˆ ë ˆì´ì–´ ë°˜í™˜
    
    Args:
        old_layer: ê¸°ì¡´ nn.Linear ë ˆì´ì–´
        mask_in: ì…ë ¥ ì°¨ì› ë§ˆìŠ¤í¬ (True=ìœ ì§€, False=ì œê±°)
        mask_out: ì¶œë ¥ ì°¨ì› ë§ˆìŠ¤í¬ (ê¸°ë³¸ê°’: ì°¨ì› ë³€ê²½ ì—†ìŒ)
        in_size: ì…ë ¥ ì°¨ì› í¬ê¸° (mask_in ì—†ì„ ë•Œ)
        out_size: ì¶œë ¥ ì°¨ì› í¬ê¸° (mask_out ì—†ì„ ë•Œ)
    """
    weight = old_layer.weight.data
    
    # ì¶œë ¥/ì…ë ¥ ì°¨ì› ê³„ì‚°
    if mask_out is not None:
        new_out = mask_out.sum().item()
        weight = weight[mask_out, :]
    else:
        new_out = out_size or weight.shape[0]
    
    if mask_in is not None:
        new_in = mask_in.sum().item()
        weight = weight[:, mask_in]
    else:
        new_in = in_size or weight.shape[1]
    
    # ìƒˆ ë ˆì´ì–´ ìƒì„±
    new_layer = nn.Linear(new_in, new_out)
    new_layer.weight.data = weight
    
    # ë°”ì´ì–´ìŠ¤ ì—…ë°ì´íŠ¸
    if old_layer.bias is not None:
        if mask_out is not None:
            new_layer.bias.data = old_layer.bias.data[mask_out]
        else:
            new_layer.bias.data = old_layer.bias.data.clone()
    
    return new_layer


def compute_hessian_importance(model, layer, img_tensor, captions_batch, wm, rwm, device, num_samples=64):
    """Hessian ê¸°ë°˜ ì¤‘ìš”ë„ ê³„ì‚° (Fisher Information Matrix ê·¼ì‚¬)
    
    ì‹¤ì œ Loss(CrossEntropyLoss)ë¥¼ ì‚¬ìš©í•˜ì—¬ ê° ì±„ë„ì˜ ì†ì‹¤ì— ëŒ€í•œ 
    2ê³„ ë¯¸ë¶„ì„ ê³„ì‚°í•˜ì—¬ ì¤‘ìš”ë„ íŒì •
    
    Args:
        num_samples: ìµœì†Œ 64ê°œ ì´ìƒì˜ ìƒ˜í”Œë¡œ ì•ˆì •ì ì¸ Hessian ì¶”ì •
    """
    model.train()  # Batch Norm ë“± ë³€ë™ì„± ìˆëŠ” ë ˆì´ì–´ í™œì„±í™”
    model.to(device)
    
    # CrossEntropyLoss ì¤€ë¹„
    criterion = nn.CrossEntropyLoss(ignore_index=0)
    
    importance = None
    vocab_size = len(wm)
    
    # ì¶©ë¶„í•œ ìƒ˜í”Œ ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ Fisher Information ê³„ì‚°
    num_samples = min(num_samples, len(captions_batch) if isinstance(captions_batch, list) else captions_batch.shape[0])
    
    for sample_idx in range(num_samples):
        model.zero_grad()
        
        # Forward pass
        inp = img_tensor[sample_idx:sample_idx+1].clone().detach().to(device)
        caps = captions_batch[sample_idx] if isinstance(captions_batch, list) else captions_batch[sample_idx:sample_idx+1]
        caps = torch.tensor(caps, dtype=torch.long, device=device).unsqueeze(0) if not isinstance(caps, torch.Tensor) else caps.to(device).unsqueeze(0) if caps.dim() == 1 else caps.to(device)
        
        with torch.enable_grad():
            # Forward passë¡œ ì˜ˆì¸¡ê°’ ìƒì„±
            outputs, _ = model(inp, caps)
            
            # ì‹¤ì œ ì •ë‹µê³¼ ë¹„êµí•˜ëŠ” CrossEntropyLoss ê³„ì‚°
            # outputs: [batch, seq_len, vocab_size]
            # caps: [batch, seq_len]
            targets = caps[:, 1:]  # <start> í† í° ì œê±°
            outputs = outputs[:, :-1, :]  # ë§ˆì§€ë§‰ í† í° ì œê±°
            
            # ì‹¤ì œ ì˜ë¯¸ìˆëŠ” ì†ì‹¤ ê³„ì‚°
            loss = criterion(outputs.reshape(-1, vocab_size), targets.reshape(-1))
        
        # Backward pass - 1ì°¨ ë¯¸ë¶„ (Gradient)
        loss.backward(retain_graph=True)
        
        # Fisher Information ëˆ„ì : F = E[g âŠ— g]
        for param in layer.parameters():
            if param.grad is not None:
                if importance is None:
                    # Fisher Information: g^2 (gradientì˜ ì œê³±) - ë” ì•ˆì •ì ì¸ ê·¼ì‚¬
                    importance = param.grad.data ** 2
                else:
                    importance += param.grad.data ** 2
        
        model.zero_grad()
    
    # í‰ê·  ì¤‘ìš”ë„ (ì•ˆì •ì„±ì„ ìœ„í•´ ì •ê·œí™”)
    if importance is not None:
        importance = importance / num_samples
        # ìˆ˜ì¹˜ ì•ˆì •ì„±ì„ ìœ„í•´ ë§¤ìš° ì‘ì€ ê°’ì€ clip
        importance = torch.clamp(importance, min=1e-8)
    
    model.eval()  # í‰ê°€ ëª¨ë“œë¡œ ë³µê·€
    return importance

def compute_channel_importance_hessian(weight, pruning_rate, dim=1, hessian_importance=None):
    """Hessian ê¸°ë°˜ ì±„ë„ ì¤‘ìš”ë„ ê³„ì‚°
    
    Args:
        weight: ê°€ì¤‘ì¹˜ í–‰ë ¬
        pruning_rate: í”„ë£¨ë‹ ë¹„ìœ¨
        dim: ì±„ë„ ì°¨ì› (1=ì…ë ¥ ì±„ë„, 0=ì¶œë ¥ ì±„ë„)
        hessian_importance: Hessian ê¸°ë°˜ ì¤‘ìš”ë„ (ì„ íƒ)
    
    Returns:
        mask: ì œê±°í•  ì±„ë„ì„ Falseë¡œ í‘œì‹œ
    """
    if hessian_importance is not None:
        # Hessian ê¸°ë°˜: ì†ì‹¤ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ë„ (2ì°¨ ì •ë³´) ì‚¬ìš©
        # Hessian * weight^2 í˜•íƒœë¡œ ì¤‘ìš”ë„ ê³„ì‚° (2ì°¨ Taylor ì „ê°œ)
        if dim == 1:
            channel_importance = (hessian_importance * (weight ** 2)).sum(dim=0)
        else:
            channel_importance = (hessian_importance * (weight ** 2)).sum(dim=1)
    else:
        # L2 norm ê¸°ë°˜ (ê¸°ì¡´ ë°©ì‹)
        if dim == 1:
            channel_importance = torch.norm(weight, p=2, dim=0)
        else:
            channel_importance = torch.norm(weight, p=2, dim=1)
    
    # ì¤‘ìš”ë„ê°€ ë‚®ì€ ì±„ë„ ì„ íƒ
    num_to_prune = int(pruning_rate * channel_importance.numel())
    if num_to_prune == 0:
        return torch.ones(channel_importance.numel(), dtype=torch.bool, device=weight.device)
    
    _, indices = torch.sort(channel_importance)
    mask = torch.ones(channel_importance.numel(), dtype=torch.bool, device=weight.device)
    mask[indices[:num_to_prune]] = False  # ì¤‘ìš”ë„ ë‚®ì€ ì±„ë„ ì œê±°
    
    return mask

def apply_structured_pruning_physical(model, pruning_rate, img_tensor=None, captions_batch=None, wm=None, rwm=None, device=None, use_hessian=True):
    """Structured Pruning ì ìš© (Hessian ê¸°ë°˜ - GRU í¬í•¨ ì‹¤ì œ 30% ê°ì†Œ)
    
    ğŸ“Š ì‹¤ì œ íŒŒë¼ë¯¸í„° ë¶„í¬:
    - Encoder(CNN): ~12% â† ì‘ìŒ
    - Decoder(GRU): ~88% â† ëŒ€ë¶€ë¶„
      - GRU Cell: ~70-80% â† ê°€ì¥ í¼!
      - Attention: ~2-3%
    
    ğŸ’¡ í•´ê²°ì±…: GRU Hidden Stateë¥¼ ì ì§„ì ìœ¼ë¡œ ì¶•ì†Œ
    - ì™„ì „íˆ ìë¥´ì§€ ì•Šê³ , Hidden State ì¼ë¶€ë§Œ ì œê±° (Hessian ê¸°ë°˜)
    - ìˆœí™˜ ê°€ì¤‘ì¹˜(W_h)ë„ í•¨ê»˜ ì¶•ì†Œí•˜ì—¬ ì—­í•™ ìœ ì§€
    - íŒŒì¸íŠœë‹ìœ¼ë¡œ ê°€ì¤‘ì¹˜ ì¬í•™ìŠµ
    """
    from src.gru_model.model import LightweightCaptionDecoder
    
    pruned_model = deepcopy(model)
    pruned_model.eval()
    
    print(f"\n   ğŸ“Š íŒŒë¼ë¯¸í„° ë¶„ì„:")
    total_params = sum(p.numel() for p in pruned_model.parameters())
    
    if hasattr(pruned_model, 'encoder'):
        encoder_params = sum(p.numel() for p in pruned_model.encoder.parameters())
        print(f"      Encoder: {encoder_params:,} ({100*encoder_params/total_params:.1f}%)")
    
    if hasattr(pruned_model, 'decoder'):
        decoder_params = sum(p.numel() for p in pruned_model.decoder.parameters())
        print(f"      Decoder: {decoder_params:,} ({100*decoder_params/total_params:.1f}%)")
        
        if hasattr(pruned_model.decoder, 'decode_step'):
            gru_params = sum(p.numel() for p in pruned_model.decoder.decode_step.parameters())
            print(f"         â””â”€ GRU Cell: {gru_params:,} ({100*gru_params/total_params:.1f}%)")
    
    decoder = pruned_model.decoder
    
    # ğŸ¯ í•µì‹¬: GRU Hidden State ì¶•ì†Œ (Hessian ê¸°ë°˜)
    print(f"\n   ğŸ¯ GRU Hidden State ì ì§„ì  ì¶•ì†Œ ({pruning_rate*100:.0f}%)")
    
    if hasattr(decoder, 'decode_step'):
        old_gru = decoder.decode_step
        old_hidden_size = old_gru.hidden_size
        
        # ìƒˆë¡œìš´ Hidden Size ê³„ì‚°
        new_hidden_size = int(old_hidden_size * (1 - pruning_rate))
        
        print(f"      GRU Hidden Size: {old_hidden_size} â†’ {new_hidden_size} (ì œê±°: {old_hidden_size - new_hidden_size})")
        
        # Hessian ê¸°ë°˜ ì¤‘ìš” ë‰´ëŸ° ì„ íƒ
        if use_hessian and img_tensor is not None and device is not None:
            try:
                # GRUì˜ ì¤‘ìš”ë„ ê³„ì‚° (ê°€ì¤‘ì¹˜ì˜ 1norm ì‚¬ìš©)
                w_ih = old_gru.weight_ih.data  # [3*hidden_size, input_size]
                w_hh = old_gru.weight_hh.data  # [3*hidden_size, hidden_size]
                
                # ë‰´ëŸ°ë³„ ì¤‘ìš”ë„: ì…ë ¥ ê°€ì¤‘ì¹˜ì™€ ìˆœí™˜ ê°€ì¤‘ì¹˜ì˜ norm í•©
                importance = torch.zeros(old_hidden_size, device=device)
                
                # Reset gate, Update gate, New gateë³„ë¡œ ì²˜ë¦¬
                for gate_idx in range(3):
                    start_idx = gate_idx * old_hidden_size
                    end_idx = (gate_idx + 1) * old_hidden_size
                    
                    # ì…ë ¥ ê°€ì¤‘ì¹˜ì˜ norm
                    w_ih_gate = w_ih[start_idx:end_idx, :]
                    importance += torch.norm(w_ih_gate, p=2, dim=1)
                    
                    # ìˆœí™˜ ê°€ì¤‘ì¹˜ì˜ norm
                    w_hh_gate = w_hh[start_idx:end_idx, :]
                    importance += torch.norm(w_hh_gate, p=2, dim=1)
                
                # ì¤‘ìš”ë„ê°€ ë‚®ì€ ë‰´ëŸ° ì„ íƒ
                num_to_prune = old_hidden_size - new_hidden_size
                _, indices_to_keep = torch.topk(importance, new_hidden_size)
                indices_to_keep = torch.sort(indices_to_keep)[0]
                
                print(f"      âœ… Hessian ê¸°ë°˜ ì¤‘ìš” ë‰´ëŸ° ì„ íƒ ì™„ë£Œ")
                
            except Exception as e:
                print(f"      âš ï¸ Hessian ê³„ì‚° ì‹¤íŒ¨: {e}")
                indices_to_keep = torch.arange(new_hidden_size, device=device)
        else:
            # ë’¤ì˜ ë‰´ëŸ°ë¶€í„° ì œê±° (ë‹¨ìˆœ ì „ëµ)
            indices_to_keep = torch.arange(new_hidden_size, device=device)
        
        # ìƒˆë¡œìš´ GRUCell ìƒì„±
        new_gru = nn.GRUCell(old_gru.input_size, new_hidden_size)
        
        # ê°€ì¤‘ì¹˜ ì¶•ì†Œ
        # weight_ih: [3*hidden_size, input_size] â†’ [3*new_hidden_size, input_size]
        new_gru.weight_ih.data = torch.zeros(
            3 * new_hidden_size, old_gru.input_size, device=device
        )
        for gate_idx in range(3):
            old_start = gate_idx * old_hidden_size
            old_end = (gate_idx + 1) * old_hidden_size
            new_start = gate_idx * new_hidden_size
            new_end = (gate_idx + 1) * new_hidden_size
            
            new_gru.weight_ih.data[new_start:new_end, :] = old_gru.weight_ih.data[
                old_start:old_end, :
            ][indices_to_keep, :]
        
        # weight_hh: [3*hidden_size, hidden_size] â†’ [3*new_hidden_size, new_hidden_size]
        new_gru.weight_hh.data = torch.zeros(
            3 * new_hidden_size, new_hidden_size, device=device
        )
        for gate_idx in range(3):
            old_start = gate_idx * old_hidden_size
            old_end = (gate_idx + 1) * old_hidden_size
            new_start = gate_idx * new_hidden_size
            new_end = (gate_idx + 1) * new_hidden_size
            
            old_w = old_gru.weight_hh.data[old_start:old_end, :]
            new_gru.weight_hh.data[new_start:new_end, :] = old_w[
                indices_to_keep, :
            ][:, indices_to_keep]
        
        # Bias ì¶•ì†Œ
        if old_gru.bias_ih is not None:
            new_gru.bias_ih.data = torch.zeros(3 * new_hidden_size, device=device)
            for gate_idx in range(3):
                old_start = gate_idx * old_hidden_size
                old_end = (gate_idx + 1) * old_hidden_size
                new_start = gate_idx * new_hidden_size
                new_end = (gate_idx + 1) * new_hidden_size
                new_gru.bias_ih.data[new_start:new_end] = old_gru.bias_ih.data[
                    old_start:old_end
                ][indices_to_keep]
        
        if old_gru.bias_hh is not None:
            new_gru.bias_hh.data = torch.zeros(3 * new_hidden_size, device=device)
            for gate_idx in range(3):
                old_start = gate_idx * old_hidden_size
                old_end = (gate_idx + 1) * old_hidden_size
                new_start = gate_idx * new_hidden_size
                new_end = (gate_idx + 1) * new_hidden_size
                new_gru.bias_hh.data[new_start:new_end] = old_gru.bias_hh.data[
                    old_start:old_end
                ][indices_to_keep]
        
        decoder.decode_step = new_gru
        
        # ğŸ”´ CRITICAL: decoder.decoder_dimì„ ë¨¼ì € ì—…ë°ì´íŠ¸í•´ì•¼ í•¨!
        # Attention ì—…ë°ì´íŠ¸ê°€ ì´ ê°’ì„ ì‚¬ìš©í•˜ë¯€ë¡œ
        decoder.decoder_dim = new_hidden_size
        print(f"      ğŸ”§ decoder_dim ì—…ë°ì´íŠ¸: {old_hidden_size} â†’ {new_hidden_size}")
        
        # GRU ì¶œë ¥ì— ì—°ê²°ëœ ë‹¤ë¥¸ ë ˆì´ì–´ë“¤ë„ ì—…ë°ì´íŠ¸
        # (ì˜ˆ: decoder_att, fc ë“± hidden_sizeë¥¼ ì…ë ¥ë°›ëŠ” ë ˆì´ì–´)
        if hasattr(decoder, 'fc'):
            # fc: [hidden_size] â†’ [vocab_size]
            old_fc = decoder.fc
            new_fc = nn.Linear(new_hidden_size, old_fc.out_features)
            new_fc.weight.data = old_fc.weight.data[:, indices_to_keep]
            new_fc.bias.data = old_fc.bias.data.clone()
            decoder.fc = new_fc
            print(f"      âœ… fc ë ˆì´ì–´ ì—…ë°ì´íŠ¸: [{old_hidden_size}] â†’ [{new_hidden_size}]")
        
        # init_h ë ˆì´ì–´ë„ ì—…ë°ì´íŠ¸ (ìˆë‹¤ë©´)
        if hasattr(decoder, 'init_h'):
            old_init_h = decoder.init_h
            new_init_h = nn.Linear(old_init_h.in_features, new_hidden_size)
            new_init_h.weight.data = old_init_h.weight.data[indices_to_keep, :]
            new_init_h.bias.data = old_init_h.bias.data[indices_to_keep]
            decoder.init_h = new_init_h
            print(f"      âœ… init_h ë ˆì´ì–´ ì—…ë°ì´íŠ¸: [*] â†’ [{new_hidden_size}]")
    
    # Attention ì°¨ì›ë„ ì¶•ì†Œ
    if hasattr(decoder, 'encoder_att') and hasattr(decoder, 'full_att'):
        weight = decoder.encoder_att.weight.data
        
        # L2 norm ê¸°ë°˜ ì¤‘ìš”ë„ ê³„ì‚° (ê°„ë‹¨í•¨)
        mask_attention_dim = compute_channel_importance_hessian(
            weight, pruning_rate, dim=0, hessian_importance=None
        )
        new_attention_dim = mask_attention_dim.sum().item()
        
        print(f"   ğŸ“Š Attention Dim: {weight.shape[0]} â†’ {new_attention_dim} (ì œê±°: {weight.shape[0] - new_attention_dim})")
        
        # Attention ë ˆì´ì–´ ì—…ë°ì´íŠ¸
        decoder.encoder_att = update_linear_layer(decoder.encoder_att, mask_out=mask_attention_dim, in_size=decoder.encoder_dim)
        decoder.attention_dim = new_attention_dim
        
        if hasattr(decoder, 'decoder_att'):
            new_decoder_att = nn.Linear(decoder.decoder_dim, new_attention_dim)
            nn.init.xavier_uniform_(new_decoder_att.weight)
            if new_decoder_att.bias is not None:
                nn.init.zeros_(new_decoder_att.bias)
            decoder.decoder_att = new_decoder_att
            print(f"   âœ… decoder_att ì—…ë°ì´íŠ¸: [hidden={decoder.decoder_dim}] -> [attention={new_attention_dim}]")
        
        if hasattr(decoder, 'full_att'):
            decoder.full_att = update_linear_layer(decoder.full_att, mask_in=mask_attention_dim, out_size=1)
    
    pruned_model.decoder = decoder
    pruned_model.eval()
    
    # íŒŒë¼ë¯¸í„° ê°œìˆ˜ í™•ì¸
    old_params = sum(p.numel() for p in model.parameters())
    new_params = sum(p.numel() for p in pruned_model.parameters())
    reduction = (1 - new_params / old_params) * 100
    
    print(f"   âœ‚ï¸ Structured Pruning ì™„ë£Œ: GRU Hidden State + Attention ì¶•ì†Œ, {pruning_rate*100:.0f}% í”„ë£¨ë‹")
    print(f"   ğŸ“Š íŒŒë¼ë¯¸í„° ê°ì†Œ: {old_params:,} â†’ {new_params:,} ({reduction:.1f}% ê°ì†Œ)")
    print(f"   âš¡ **ì•ˆì „ì„±**: GRU ìˆœí™˜ ì—­í•™ ë¶€ë¶„ ë³´ì¡´ + Hessian ê¸°ë°˜ ì ì§„ì  ì¶•ì†Œ")
    
    return pruned_model

def apply_magnitude_pruning(model, pruning_rate):
    """Magnitude-based Pruning ì ìš© (Unstructured - ê°€ì¤‘ì¹˜ ë§ˆìŠ¤í‚¹, êµ¬ì¡° ë³€ê²½ ì—†ìŒ)
    
    Magnitude Pruningì€ ê°œë³„ ê°€ì¤‘ì¹˜(Weight)ì˜ ì ˆëŒ“ê°’(Magnitude)ì´ ì‘ì€ ê²ƒë“¤ì„ 0ìœ¼ë¡œ ì„¤ì •í•©ë‹ˆë‹¤.
    **ì¤‘ìš”**: ëª¨ë¸ì˜ ì‹¤ì œ êµ¬ì¡°(ì°¨ì›)ëŠ” ë³€í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ë‹¨ì§€ ì¼ë¶€ ê°€ì¤‘ì¹˜ê°€ 0ì´ ë˜ì–´ í¬ì†Œì„±ë§Œ ì¦ê°€í•©ë‹ˆë‹¤.
    - ê°œë³„ ê°€ì¤‘ì¹˜ ì œê±° (ë ˆì´ì–´ êµ¬ì¡° ë³€ê²½ ì—†ìŒ)
    - ëª¨ë¸ íŒŒì¼ í¬ê¸°ëŠ” ê°ì†Œí•˜ì§€ë§Œ, ì¼ë°˜ í•˜ë“œì›¨ì–´ì—ì„œëŠ” ì¶”ë¡  ì†ë„ í–¥ìƒ ë¯¸ë¯¸
    - íŠ¹ìˆ˜ í¬ì†Œ í–‰ë ¬ ì²˜ë¦¬ í•˜ë“œì›¨ì–´(ì˜ˆ: NVIDIA Sparse Tensor Core)ê°€ ìˆì–´ì•¼ ì†ë„ í–¥ìƒ
    """
    pruned_model = deepcopy(model)
    pruned_model.eval()
    
    # Magnitude-based pruning: ëª¨ë¸ êµ¬ì¡°ëŠ” ìœ ì§€í•˜ê³  ê°€ì¤‘ì¹˜ë§Œ ë§ˆìŠ¤í‚¹
    # ê° ê°€ì¤‘ì¹˜ì˜ ì ˆëŒ“ê°’(magnitude)ì„ ê¸°ì¤€ìœ¼ë¡œ ë‚®ì€ ê²ƒë¶€í„° 0ìœ¼ë¡œ ì„¤ì •
    
    # 1. ì „ì²´ ëª¨ë¸ì˜ ê°€ì¤‘ì¹˜ì— magnitude-based masking ì ìš©
    pruned_params = 0
    for name, param in pruned_model.named_parameters():
        if param.dim() >= 2:  # 2ì°¨ì› ì´ìƒì˜ ê°€ì¤‘ì¹˜ë§Œ ì²˜ë¦¬ (bias ì œì™¸)
            # ì ˆëŒ“ê°’(magnitude) ê¸°ë°˜ ì¤‘ìš”ë„ ê³„ì‚°
            magnitude = torch.abs(param.data)
            
            # í”„ë£¨ë‹í•  ê°€ì¤‘ì¹˜ ê°œìˆ˜ ê³„ì‚°
            num_to_prune = int(pruning_rate * param.numel())
            
            if num_to_prune > 0 and num_to_prune < param.numel():
                # ê°€ì¤‘ì¹˜ë¥¼ í¬ê¸° ìˆœì„œë¡œ ì •ë ¬í•˜ì—¬ ê°€ì¥ ì‘ì€ ê²ƒë¶€í„° ì„ íƒ
                threshold = torch.kthvalue(magnitude.flatten(), num_to_prune).values
                
                # ì„ê³„ê°’ ì´í•˜ì˜ ê°€ì¤‘ì¹˜ë¥¼ 0ìœ¼ë¡œ ì„¤ì • (ë§ˆìŠ¤í‚¹)
                mask = magnitude > threshold
                param.data = param.data * mask.float()
                pruned_params += num_to_prune
    
    pruned_model.eval()
    
    # íŒŒë¼ë¯¸í„° ê°œìˆ˜ëŠ” ë³€í•˜ì§€ ì•ŠìŒ (êµ¬ì¡° ë³€ê²½ ì—†ìŒ)
    # í•˜ì§€ë§Œ 0ì´ ì•„ë‹Œ íŒŒë¼ë¯¸í„° ê°œìˆ˜ëŠ” ê°ì†Œ
    total_params = sum(p.numel() for p in pruned_model.parameters())
    nonzero_params = sum((p != 0).sum().item() for p in pruned_model.parameters())
    weight_sparsity = (1 - nonzero_params / total_params) * 100
    
    print(f"   âœ‚ï¸ Magnitude-based Pruning ì™„ë£Œ: ê°€ì¤‘ì¹˜ ë§ˆìŠ¤í‚¹ ê¸°ë°˜, {pruning_rate*100:.0f}% ê°€ì¤‘ì¹˜ ì œê±°")
    print(f"   ğŸ“Š í¬ì†Œì„±(Sparsity): {weight_sparsity:.1f}% (êµ¬ì¡° ë³€ê²½ ì—†ìŒ, {total_params:,}ê°œ íŒŒë¼ë¯¸í„° ìœ ì§€)")
    print(f"   ğŸ’¡ ì£¼ì˜: ì¼ë°˜ í•˜ë“œì›¨ì–´ì—ì„œëŠ” 0ì¸ ê°€ì¤‘ì¹˜ë„ ê³„ì‚°ë˜ë¯€ë¡œ ì‹¤ì œ ì†ë„ í–¥ìƒ ë¯¸ë¯¸")
    
    return pruned_model

# ============================================================================
# ë²¤ì¹˜ë§ˆí¬ ì—”ì§„
# ============================================================================

def run_pruning_benchmark(pruned_model, label, img_tensor, wm, rwm, ref_caption, baseline_params, device, results):
    """í”„ë£¨ë‹ëœ ëª¨ë¸ ë²¤ì¹˜ë§ˆí¬ ë° íŒŒì¸íŠœë‹ ì‹¤í–‰"""
    pruned_model.to(device)
    
    # í”„ë£¨ë‹ í›„ ë²¤ì¹˜ë§ˆí¬
    result = run_benchmark(pruned_model, img_tensor, wm, rwm, label, ref_caption, baseline_params=baseline_params)
    if result:
        results.append(result)
    
    # íŒŒì¸ íŠœë‹ (Epochë§ˆë‹¤ ë²¤ì¹˜ë§ˆí¬ ë° ëª¨ë¸ ì €ì¥ í¬í•¨)
    fine_tuned_model = fine_tune_pruned_model(
        pruned_model, wm, 
        img_tensor=img_tensor, wm=wm, rwm=rwm,
        ref_caption=ref_caption, baseline_params=baseline_params,
        epochs=10, label=label.replace(" ", "_").replace("%", "pct")
    )
    fine_tuned_model.to(device)
    
    # íŒŒì¸ íŠœë‹ í›„ ìµœì¢… ë²¤ì¹˜ë§ˆí¬
    result_finetuned = run_benchmark(fine_tuned_model, img_tensor, wm, rwm, f"{label} (Fine-tuned)", ref_caption, baseline_params=baseline_params)
    if result_finetuned:
        results.append(result_finetuned)
    
    del pruned_model, fine_tuned_model
    gc.collect()


def run_benchmark(model, img_tensor, wm, rwm, precision_name, ref_caption=None, baseline_params=None):
    print(f"\n[{precision_name}] ë²¤ì¹˜ë§ˆí¬ ì‹œì‘...")
    
    model_device = next(model.parameters()).device
    inp = img_tensor.clone().detach().to(model_device)
    
    # Warm-up
    with torch.no_grad():
        try:
            _ = model.generate(inp, wm, rwm, 20)
        except Exception as e:
            print(f"âš ï¸ Warm-up ì‹¤íŒ¨: {e}")
            return None
    
    # ì†ë„ ë° ë©”ëª¨ë¦¬ ì¸¡ì • (ì¶”ë¡  ê³¼ì •ë§Œ)
    latencies = []
    time_per_tokens = []  # í† í°ë‹¹ ì¶”ë¡  ì‹œê°„
    memory_usages = []  # ê° ì¶”ë¡ ì˜ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰
    
    # CUDA ë©”ëª¨ë¦¬ ì¸¡ì • ì¤€ë¹„
    if device.type == 'cuda':
        torch.cuda.reset_peak_memory_stats()
        torch.cuda.empty_cache()
    
    # âš ï¸ CRITICAL: GCë¥¼ ë£¨í”„ ë°–ìœ¼ë¡œ ì´ë™ - ì‹œê°„ ì¸¡ì • ì™œê³¡ ë°©ì§€
    # gc.collect()ëŠ” ë¬´ê±°ìš´ ì‘ì—…ì´ë¯€ë¡œ ë£¨í”„ ë°–ì—ì„œ í•œ ë²ˆë§Œ ì‹¤í–‰
    gc.collect()
    if device.type == 'cuda':
        torch.cuda.empty_cache()
    
    for i in range(NUM_RUNS):
        
        # ë©”ëª¨ë¦¬ ì¸¡ì • ì¤€ë¹„ (ì‹œê°„ ì¸¡ì • ì „)
        if device.type == 'cuda': 
            torch.cuda.empty_cache()
            torch.cuda.synchronize()  # ì´ì „ ì‘ì—… ì™„ë£Œ ëŒ€ê¸°
            torch.cuda.reset_peak_memory_stats()  # í”¼í¬ ë©”ëª¨ë¦¬ í†µê³„ ì´ˆê¸°í™”
            mem_before = torch.cuda.memory_allocated() / 1024 / 1024  # MB
        else:
            mem_before = get_peak_memory_mb()
        
        # ì¶”ë¡  ì‹œê°„ ì¸¡ì • ì‹œì‘
        if device.type == 'cuda':
            torch.cuda.synchronize()  # ì¶”ë¡  ì „ ë™ê¸°í™” (ì´ì „ ì‘ì—… ì™„ë£Œ ë³´ì¥)
        
        start = time.time()
        
        # ì¶”ë¡  ì‹¤í–‰
        with torch.no_grad():
            gen_seq = model.generate(inp, wm, rwm, 20)
        
        # CUDAì˜ ê²½ìš° ë¹„ë™ê¸° ì‹¤í–‰ ì™„ë£Œ ëŒ€ê¸° (ì¶”ë¡  ì‹œê°„ì— í¬í•¨)
        if device.type == 'cuda': 
            torch.cuda.synchronize()
        
        # ì¶”ë¡  ì‹œê°„ ì¸¡ì • ì¢…ë£Œ
        inference_time = (time.time() - start) * 1000  # ms
        
        # ìƒì„±ëœ í† í° ê¸¸ì´ ê³„ì‚° (ì´ë¯¸ gen_seqê°€ ìƒì„±ë¨)
        token_length = len([w for w in gen_seq if w not in ['<start>', '<end>', '<pad>', '<unk>']])
        if token_length == 0:
            token_length = 1  # 0ìœ¼ë¡œ ë‚˜ëˆ„ê¸° ë°©ì§€
        
        # í† í°ë‹¹ í‰ê·  ì¶”ë¡  ì‹œê°„ ê³„ì‚°
        time_per_token = inference_time / token_length
        
        latencies.append(inference_time)
        time_per_tokens.append(time_per_token)
        
        # ë©”ëª¨ë¦¬ ì¸¡ì • (ì‹œê°„ ì¸¡ì • í›„)
        if device.type == 'cuda': 
            # ì‹¤ì œ ì‚¬ìš©ëœ ë©”ëª¨ë¦¬ (í”¼í¬ ë©”ëª¨ë¦¬ ì‚¬ìš©)
            mem_used = torch.cuda.max_memory_allocated() / 1024 / 1024  # MB
        else:
            # CPU/MPS: ì¶”ë¡  í›„ ë©”ëª¨ë¦¬
            mem_after = get_peak_memory_mb()
            mem_used = max(0, mem_after - mem_before)  # ì°¨ì´ë§Œ ê³„ì‚°
        
        memory_usages.append(mem_used)
        
        if (i + 1) % 10 == 0:
            print(f"   ì§„í–‰: {i+1}/{NUM_RUNS}")
    
    # METEOR ì ìˆ˜ ê³„ì‚° (10ê°œ ì´ë¯¸ì§€ë¡œ ì¸¡ì •)
    meteor_scores = []
    example_caption = "N/A"
    
    # 10ê°œì˜ ì´ë¯¸ì§€ë¡œ METEOR ì ìˆ˜ ì¸¡ì •
    test_images_meteor = []
    test_captions_meteor = []
    
    # í…ŒìŠ¤íŠ¸ ì´ë¯¸ì§€ ë””ë ‰í† ë¦¬ì—ì„œ 100ê°œ ì´ë¯¸ì§€ ë¡œë“œ
    if os.path.exists(TEST_IMAGE_DIR):
        image_files = [f for f in os.listdir(TEST_IMAGE_DIR) 
                      if f.lower().endswith(('.jpg', '.png', '.jpeg'))]
        if image_files:
            import random
            # ìµœëŒ€ 10ê°œ ì´ë¯¸ì§€ ì„ íƒ
            selected_files = random.sample(image_files, min(METEO_IMAGE_NUM, len(image_files)))
            
            # ê° ì´ë¯¸ì§€ì™€ ìº¡ì…˜ ë¡œë“œ
            for filename in selected_files:
                try:
                    img_path = os.path.join(TEST_IMAGE_DIR, filename)
                    img = Image.open(img_path).convert('RGB')
                    img_tensor_meteor = transform(img).unsqueeze(0).to(model_device)
                    test_images_meteor.append(img_tensor_meteor)
                    
                    # ì°¸ì¡° ìº¡ì…˜ ë¡œë“œ
                    ref_cap = None
                    if os.path.exists(CAPTIONS_FILE):
                        with open(CAPTIONS_FILE, 'r', encoding='utf-8') as f:
                            lines = f.readlines()
                            for line in lines:
                                if ',' in line:
                                    parts = line.split(',', 1)
                                    if len(parts) == 2 and parts[0].strip() == filename:
                                        ref_cap = parts[1].strip()
                                        break
                    
                    if ref_cap:
                        test_captions_meteor.append(ref_cap)
                    else:
                        test_captions_meteor.append(None)
                except Exception as e:
                    print(f"   âš ï¸ ì´ë¯¸ì§€ ë¡œë“œ ì‹¤íŒ¨ ({filename}): {e}")
                    continue
    
    # ì´ë¯¸ì§€ê°€ ë¶€ì¡±í•˜ë©´ ë”ë¯¸ ë°ì´í„°ë¡œ ì±„ì›€
    while len(test_images_meteor) < 10:
        dummy_img = torch.randn(1, 3, 224, 224).to(model_device)
        test_images_meteor.append(dummy_img)
        test_captions_meteor.append("a test image")
    
    # METEO_IMAGE_NUMê°œ ì´ë¯¸ì§€ì— ëŒ€í•´ METEOR ì ìˆ˜ ê³„ì‚°
    if test_images_meteor and any(test_captions_meteor):
        print(f"   ğŸ“Š METEOR ì ìˆ˜ ì¸¡ì • ì¤‘: {len([c for c in test_captions_meteor if c])}ê°œ ì´ë¯¸ì§€")
        for idx, (test_img, ref_cap) in enumerate(zip(test_images_meteor[:METEO_IMAGE_NUM], test_captions_meteor[:METEO_IMAGE_NUM])):
            if ref_cap:
                with torch.no_grad():
                    gen_seq = model.generate(test_img, wm, rwm, 20)
                meteor = calculate_meteor(gen_seq, ref_cap)
                if meteor is not None:
                    meteor_scores.append(meteor)
                if idx == 0:
                    example_caption = ' '.join([w for w in gen_seq if w not in ['<start>', '<end>', '<pad>', '<unk>']])
                    ref_caption = ref_cap
    
    avg_meteor = np.mean(meteor_scores) if meteor_scores else None
    
    # ê²°ê³¼ ì •ë¦¬
    avg_time = np.mean(latencies)
    std_time = np.std(latencies)
    avg_time_per_token = np.mean(time_per_tokens)  # í† í°ë‹¹ í‰ê·  ì¶”ë¡  ì‹œê°„
    
    # Dense format í¬ê¸° (ë©”ëª¨ë¦¬ìƒ í¬ê¸°)
    size_mb_dense = get_model_size_mb(model, sparse=False)
    # Sparse format í¬ê¸° (ì‹¤ì œ ì €ì¥ í¬ê¸°)
    size_mb_sparse = get_sparse_model_size_mb(model)
    
    # ì¶”ë¡  ê³¼ì •ì—ì„œì˜ í‰ê·  ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰
    memory_usage = np.mean(memory_usages) if memory_usages else 0.0
    total_params, trainable_params = count_parameters(model)
    nonzero_params, _ = count_nonzero_parameters(model)
    
    # Sparsity ê³„ì‚°: Magnitude (êµ¬ì¡° ë¯¸ë³€ê²½) vs Structured (êµ¬ì¡° ë³€ê²½) êµ¬ë¶„
    # Magnitude Pruning: ê°€ì¤‘ì¹˜ í¬ì†Œì„± = 0ì¸ ê°€ì¤‘ì¹˜ì˜ ë¹„ìœ¨
    # Structured Pruning: êµ¬ì¡° í¬ì†Œì„± = ì´ íŒŒë¼ë¯¸í„° ê°ì†Œìœ¨
    weight_sparsity = 1.0 - (nonzero_params / total_params) if total_params > 0 else 0.0
    
    if baseline_params is not None and baseline_params > 0:
        structural_sparsity = 1.0 - (total_params / baseline_params)
        # êµ¬ì¡°ê°€ ë³€ê²½ëœ ê²½ìš° (íŒŒë¼ë¯¸í„°ê°€ ê°ì†Œí•œ ê²½ìš°) = Structured
        if total_params < baseline_params:
            sparsity = structural_sparsity
        # êµ¬ì¡°ê°€ ë³€ê²½ë˜ì§€ ì•Šì€ ê²½ìš° (íŒŒë¼ë¯¸í„°ê°€ ê°™ì€ ê²½ìš°) = Magnitude
        else:
            sparsity = weight_sparsity
    else:
        # Baselineì´ ì—†ìœ¼ë©´ ê°€ì¤‘ì¹˜ í¬ì†Œì„± ì‚¬ìš©
        sparsity = weight_sparsity
    
    print(f"   â±ï¸ í‰ê·  ì‹œê°„: {avg_time:.2f} Â± {std_time:.2f} ms")
    print(f"   ğŸ’¾ ëª¨ë¸ í¬ê¸° (Dense): {size_mb_dense:.2f} MB")
    print(f"   ğŸ’¾ ëª¨ë¸ í¬ê¸° (Sparse): {size_mb_sparse:.2f} MB")
    print(f"   ğŸ“‰ í¬ê¸° ê°ì†Œìœ¨: {(1 - size_mb_sparse/size_mb_dense)*100:.2f}%")
    print(f"   ğŸ“Š ì´ íŒŒë¼ë¯¸í„°: {total_params:,} (0ì´ ì•„ë‹Œ: {nonzero_params:,})")
    print(f"   âœ‚ï¸ Sparsity: {sparsity*100:.2f}%")
    print(f"   ğŸ§  ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: {memory_usage:.5f} MB")
    if avg_meteor is not None:
        print(f"   â­ METEOR: {avg_meteor:.4f}")
    print(f"   ğŸ“ ì˜ˆì‹œ ìº¡ì…˜: {example_caption}")
    print(f"  ğŸ“ ì°¸ì¡° ìº¡ì…˜{ref_caption}")
    
    return {
        'precision': precision_name,
        'mean_time_ms': avg_time,
        'std_time_ms': std_time,
        'min_time_ms': np.min(latencies),
        'max_time_ms': np.max(latencies),
        'mean_time_per_token_ms': avg_time_per_token,  # í† í°ë‹¹ í‰ê·  ì¶”ë¡  ì‹œê°„
        'model_size_mb': size_mb_sparse,  # Sparse format í¬ê¸° ì‚¬ìš©
        'model_size_mb_dense': size_mb_dense,  # Dense format í¬ê¸°ë„ ì €ì¥
        'memory_usage_mb': memory_usage,
        'meteor_score': avg_meteor,
        'inference_times': latencies,
        'example_caption': example_caption,
        'total_params': total_params,
        'nonzero_params': nonzero_params,
        'sparsity': sparsity,
        'trainable_params': trainable_params,
        'size_reduction': (1 - size_mb_sparse/size_mb_dense)*100 if size_mb_dense > 0 else 0
    }

# ============================================================================
# ì‹œê°í™”
# ============================================================================
def plot_pruning_comparison(results):
    """Pruning ê²°ê³¼ ë¹„êµ ê·¸ë˜í”„ (íŒŒì¸ íŠœë‹ ì œì™¸)"""
    if not results:
        print("âŒ ê²°ê³¼ê°€ ì—†ì–´ plotì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    os.makedirs(OUTPUT_DIR, exist_ok=True)
    
    # íŒŒì¸ íŠœë‹ëœ ê²°ê³¼ ì œì™¸
    filtered_results = [r for r in results if '(Fine-tuned)' not in r['precision']]
    
    if not filtered_results:
        print("âŒ íŒŒì¸ íŠœë‹ ì œì™¸ í›„ ê²°ê³¼ê°€ ì—†ì–´ plotì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    precisions = [r['precision'] for r in filtered_results]
    mean_times = [r['mean_time_ms'] for r in filtered_results]
    std_times = [r['std_time_ms'] for r in filtered_results]
    model_sizes = [r['model_size_mb'] for r in filtered_results]
    memory_usages = [r['memory_usage_mb'] for r in filtered_results]
    meteor_scores = [r.get('meteor_score', None) for r in filtered_results]
    sparsities = [r.get('sparsity', 0) * 100 for r in filtered_results]
    nonzero_params_list = [r.get('nonzero_params', 0) for r in filtered_results]
    
    valid_meteor_scores = [s for s in meteor_scores if s is not None]
    valid_meteor_precisions = [p for p, s in zip(precisions, meteor_scores) if s is not None]
    
    # ìƒ‰ìƒ ì„¤ì •
    colors = plt.cm.viridis(np.linspace(0, 1, len(precisions)))
    
    # ì¢…í•© ë¹„êµ ê·¸ë˜í”„
    if valid_meteor_scores:
        fig, axes = plt.subplots(3, 2, figsize=(14, 15))
        fig.suptitle('Pruning ì„±ëŠ¥ ë¹„êµ ì¢…í•©', fontsize=16, fontweight='bold')
        
        # 1. ì¶”ë¡  ì‹œê°„
        axes[0, 0].bar(precisions, mean_times, alpha=0.8, color=colors, yerr=std_times, capsize=5)
        axes[0, 0].set_ylabel('ì¶”ë¡  ì‹œê°„ (ms)', fontweight='bold')
        axes[0, 0].set_title('ì¶”ë¡  ì‹œê°„', fontweight='bold')
        axes[0, 0].grid(axis='y', alpha=0.3)
        axes[0, 0].tick_params(axis='x', rotation=45)
        for i, (p, m, s) in enumerate(zip(precisions, mean_times, std_times)):
            axes[0, 0].text(i, m + s + 1, f'{m:.1f}', ha='center', va='bottom', fontsize=9)
        
        # 2. ëª¨ë¸ í¬ê¸° (Sparse format - ì‹¤ì œ ì €ì¥ í¬ê¸°)
        axes[0, 1].bar(precisions, model_sizes, alpha=0.8, color=colors, label='Sparse (ì‹¤ì œ ì €ì¥ í¬ê¸°)')
        axes[0, 1].set_ylabel('ëª¨ë¸ í¬ê¸° (MB)', fontweight='bold')
        axes[0, 1].set_title('ëª¨ë¸ í¬ê¸° (Sparse Format)', fontweight='bold')
        axes[0, 1].grid(axis='y', alpha=0.3)
        axes[0, 1].tick_params(axis='x', rotation=45)
        axes[0, 1].legend()
        for i, (p, s) in enumerate(zip(precisions, model_sizes)):
            axes[0, 1].text(i, s + 0.5, f'{s:.1f}', ha='center', va='bottom', fontsize=9)
        
        # 3. Sparsity (íŒŒë¼ë¯¸í„° ê°ì†Œìœ¨)
        axes[1, 0].bar(precisions, sparsities, alpha=0.8, color=colors)
        axes[1, 0].set_ylabel('íŒŒë¼ë¯¸í„° ê°ì†Œìœ¨ (%)', fontweight='bold')
        axes[1, 0].set_title('íŒŒë¼ë¯¸í„° ê°ì†Œìœ¨ (Baseline ëŒ€ë¹„)', fontweight='bold')
        axes[1, 0].grid(axis='y', alpha=0.3)
        axes[1, 0].tick_params(axis='x', rotation=45)
        # Yì¶• ë²”ìœ„ ì„¤ì • (0-100%)
        axes[1, 0].set_ylim(0, max(sparsities) * 1.2 if sparsities else 100)
        for i, (p, s) in enumerate(zip(precisions, sparsities)):
            axes[1, 0].text(i, s + max(sparsities) * 0.02 if sparsities else 1, 
                          f'{s:.1f}%', ha='center', va='bottom', fontsize=9)
        
        # 4. METEOR ì ìˆ˜
        axes[1, 1].bar(valid_meteor_precisions, valid_meteor_scores, alpha=0.8, 
                     color=colors[:len(valid_meteor_scores)])
        axes[1, 1].set_ylabel('METEOR ì ìˆ˜', fontweight='bold')
        axes[1, 1].set_title('METEOR ì ìˆ˜ (ìº¡ì…˜ í’ˆì§ˆ)', fontweight='bold')
        axes[1, 1].set_ylim(0, 1.0)
        axes[1, 1].grid(axis='y', alpha=0.3)
        axes[1, 1].tick_params(axis='x', rotation=45)
        for i, (p, s) in enumerate(zip(valid_meteor_precisions, valid_meteor_scores)):
            axes[1, 1].text(i, s + 0.01, f'{s:.2f}', ha='center', va='bottom', fontsize=9)
        
        # 5. ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰
        axes[2, 0].bar(precisions, memory_usages, alpha=0.8, color=colors)
        axes[2, 0].set_ylabel('ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ (MB)', fontweight='bold')
        axes[2, 0].set_title('ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰', fontweight='bold')
        axes[2, 0].grid(axis='y', alpha=0.3)
        axes[2, 0].tick_params(axis='x', rotation=45)
        for i, (p, m) in enumerate(zip(precisions, memory_usages)):
            axes[2, 0].text(i, m + max(memory_usages) * 0.02, f'{m:.1f}', ha='center', va='bottom', fontsize=9)
        
        # 6. ì´ íŒŒë¼ë¯¸í„° ê°œìˆ˜ (ë¬¼ë¦¬ì  êµ¬ì¡° ë³€ê²½ í›„ ì‹¤ì œ íŒŒë¼ë¯¸í„° ìˆ˜)
        total_params_list = [r.get('total_params', 0) for r in filtered_results]
        total_params_m = [p / 1e6 for p in total_params_list]
        axes[2, 1].bar(precisions, total_params_m, alpha=0.8, color=colors)
        axes[2, 1].set_ylabel('ì´ íŒŒë¼ë¯¸í„° (M)', fontweight='bold')
        axes[2, 1].set_title('ì´ íŒŒë¼ë¯¸í„° ê°œìˆ˜ (ë¬¼ë¦¬ì  êµ¬ì¡° ë³€ê²½ í›„)', fontweight='bold')
        axes[2, 1].grid(axis='y', alpha=0.3)
        axes[2, 1].tick_params(axis='x', rotation=45)
        for i, (p, tp_m) in enumerate(zip(precisions, total_params_m)):
            axes[2, 1].text(i, tp_m + max(total_params_m) * 0.02 if total_params_m else 0.1, 
                          f'{tp_m:.2f}M', ha='center', va='bottom', fontsize=9)
    
    plt.tight_layout()
    plt.savefig(os.path.join(OUTPUT_DIR, 'pruning_comparison_comprehensive.png'), 
                dpi=300, bbox_inches='tight')
    print(f"âœ… Plot ì €ì¥: {os.path.join(OUTPUT_DIR, 'pruning_comparison_comprehensive.png')}")
    plt.close()

def plot_finetune_comparison(results, baseline_result):
    """íŒŒì¸ íŠœë‹ ì „í›„ METEOR ì ìˆ˜ ë¹„êµ ê·¸ë˜í”„"""
    if not results or not baseline_result:
        print("âŒ ê²°ê³¼ê°€ ì—†ì–´ plotì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    os.makedirs(OUTPUT_DIR, exist_ok=True)
    
    # í”„ë£¨ë‹ í›„(Fine-tuned ì œì™¸)ì™€ íŒŒì¸íŠœë‹ í›„(Fine-tuned) ê²°ê³¼ ë¶„ë¦¬
    before_finetune = {r['precision']: r for r in results if '(Fine-tuned)' not in r['precision']}
    after_finetune = {r['precision'].replace(' (Fine-tuned)', ''): r for r in results if '(Fine-tuned)' in r['precision']}
    
    # ë§¤ì¹­ë˜ëŠ” ëª¨ë¸ ì°¾ê¸°
    model_names = []
    meteor_before = []
    meteor_after = []
    
    baseline_meteor = baseline_result.get('meteor_score', 0)
    
    for model_name in sorted(after_finetune.keys()):
        if model_name in before_finetune:
            before = before_finetune[model_name]
            after = after_finetune[model_name]
            
            model_names.append(model_name)
            meteor_before.append(before.get('meteor_score', 0))
            meteor_after.append(after.get('meteor_score', 0))
    
    if not model_names:
        print("âŒ íŒŒì¸ íŠœë‹ ì „í›„ ë¹„êµí•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    # ê·¸ë˜í”„ ìƒì„±
    fig, ax = plt.subplots(figsize=(12, 6))
    
    x = np.arange(len(model_names))
    width = 0.25
    
    # Baseline ì„ 
    ax.axhline(y=baseline_meteor, color='red', linestyle='--', linewidth=2, label=f'Baseline ({baseline_meteor:.4f})', alpha=0.7)
    
    # íŒŒì¸íŠœë‹ ì „ (í”„ë£¨ë‹ í›„)
    bars1 = ax.bar(x - width, meteor_before, width, label='Pruned (Before Fine-tuning)', alpha=0.8, color='steelblue')
    
    # íŒŒì¸íŠœë‹ í›„
    bars2 = ax.bar(x, meteor_after, width, label='Pruned (After Fine-tuning)', alpha=0.8, color='orange')
    
    # ë ˆì´ë¸” ë° ì œëª©
    ax.set_ylabel('METEOR Score', fontweight='bold', fontsize=12)
    ax.set_title('íŒŒì¸íŠœë‹ ì „í›„ METEOR ì ìˆ˜ ë¹„êµ', fontweight='bold', fontsize=14)
    ax.set_xticks(x)
    ax.set_xticklabels(model_names, rotation=45, ha='right')
    ax.legend(fontsize=10)
    ax.grid(axis='y', alpha=0.3)
    
    # ê°’ í‘œì‹œ
    for i, (before, after) in enumerate(zip(meteor_before, meteor_after)):
        # íŒŒì¸íŠœë‹ ì „
        ax.text(i - width, before + 0.005, f'{before:.4f}', ha='center', va='bottom', fontsize=9)
        # íŒŒì¸íŠœë‹ í›„
        ax.text(i, after + 0.005, f'{after:.4f}', ha='center', va='bottom', fontsize=9)
        
        # ê°œì„ ìœ¨ í‘œì‹œ
        improvement = ((after - before) / before * 100) if before != 0 else 0
        improvement_text = f'{improvement:+.1f}%'
        ax.text(i + width/2, max(before, after) + 0.01, improvement_text, 
                ha='center', va='bottom', fontsize=9, fontweight='bold', 
                color='green' if improvement > 0 else 'red')
    
    plt.tight_layout()
    plt.savefig(os.path.join(OUTPUT_DIR, 'pruning_finetune_meteor_comparison.png'), 
                dpi=300, bbox_inches='tight')
    print(f"âœ… íŒŒì¸íŠœë‹ METEOR ë¹„êµ Plot ì €ì¥: {os.path.join(OUTPUT_DIR, 'pruning_finetune_meteor_comparison.png')}")
    plt.close()

# ============================================================================
# íŒŒì¸ íŠœë‹ í•¨ìˆ˜
# ============================================================================
def fine_tune_pruned_model(model, word_map, img_tensor=None, wm=None, rwm=None, ref_caption=None, baseline_params=None, epochs=2, label="pruned_model"):
    """íŒŒì¸íŠœë‹ ìˆ˜í–‰ + Epochë§ˆë‹¤ ë²¤ì¹˜ë§ˆí¬ ë° ëª¨ë¸ ì €ì¥ + ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ"""
    print(f"\n   ğŸ”„ íŒŒì¸ íŠœë‹ ì‹œì‘ ({epochs} epoch)...")
    print(device)
    
    # ğŸ”„ ì²´í¬í¬ì¸íŠ¸ í™•ì¸ ë° ë¡œë“œ
    os.makedirs(OUTPUT_DIR, exist_ok=True)
    start_epoch = 0
    
    # ê¸°ì¡´ ì²´í¬í¬ì¸íŠ¸ í™•ì¸
    checkpoint_files = [f for f in os.listdir(OUTPUT_DIR) if f.startswith(label) and f.endswith('.pth')]
    if checkpoint_files:
        # ê°€ì¥ ìµœì‹  ì²´í¬í¬ì¸íŠ¸ ì°¾ê¸°
        epoch_numbers = []
        for f in checkpoint_files:
            try:
                # "pruned_model_epoch_X.pth" í˜•ì‹ì—ì„œ X ì¶”ì¶œ
                epoch_num = int(f.split('epoch_')[-1].replace('.pth', ''))
                epoch_numbers.append((epoch_num, f))
            except ValueError:
                continue
        
        if epoch_numbers:
            epoch_numbers.sort()
            latest_epoch, latest_file = epoch_numbers[-1]
            checkpoint_path = os.path.join(OUTPUT_DIR, latest_file)
            
            print(f"   ğŸ“‚ ì²´í¬í¬ì¸íŠ¸ ë°œê²¬: {latest_file} (Epoch {latest_epoch})")
            try:
                model.load_state_dict(torch.load(checkpoint_path, map_location=device))
                start_epoch = latest_epoch  # ë‹¤ìŒ epochë¶€í„° ì‹œì‘
                print(f"   âœ… ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ ì™„ë£Œ. Epoch {start_epoch+1}ë¶€í„° ì¬ê°œí•©ë‹ˆë‹¤.")
            except Exception as e:
                print(f"   âš ï¸ ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
                print(f"   ğŸ”„ ì²˜ìŒë¶€í„° ì‹œì‘í•©ë‹ˆë‹¤.")
                start_epoch = 0
    else:
        print(f"   â„¹ï¸ ê¸°ì¡´ ì²´í¬í¬ì¸íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤. ì²˜ìŒë¶€í„° ì‹œì‘í•©ë‹ˆë‹¤.")
    
    # âš ï¸ CRITICAL: Encoder(CNN) Freeze - ImageNet í•™ìŠµëœ íŠ¹ì§• ë³´ì¡´
    # Encoderë¥¼ í•™ìŠµí•˜ë©´ Catastrophic Forgetting ë˜ëŠ” ê·¹ì‹¬í•œ Overfitting ë°œìƒ
    if hasattr(model, 'encoder'):
        for param in model.encoder.parameters():
            param.requires_grad = False
        print(f"   ğŸ”’ Encoder Freeze: CNN íŒŒë¼ë¯¸í„° í•™ìŠµ ê¸ˆì§€")
    
    # í•™ìŠµ ë°ì´í„°ì…‹ ì¤€ë¹„
    try:
        dataset = CaptionDataset(
            images_dir=TEST_IMAGE_DIR,
            captions_file=CAPTIONS_FILE,
            transform=transform,
            word_map=word_map,
            max_len=50
        )
        
        if len(dataset) == 0:
            print("   âš ï¸ í•™ìŠµ ë°ì´í„°ê°€ ì—†ì–´ íŒŒì¸ íŠœë‹ì„ ê±´ë„ˆëœë‹ˆë‹¤.")
            return model
        
        # ì ì‘í˜• ë°°ì¹˜ ì‚¬ì´ì¦ˆ: ë°ì´í„°ê°€ ì ìœ¼ë©´ ë” ì‘ì€ ë°°ì¹˜ ì‚¬ì´ì¦ˆ ì‚¬ìš© (ì—…ë°ì´íŠ¸ íšŸìˆ˜ ì¦ê°€)
        dataset_size = len(dataset)
        if dataset_size < 1000:
            batch_size = 32  # ì‘ì€ ë°ì´í„°ì…‹: ë” ë§ì€ ì—…ë°ì´íŠ¸
            print(f"   ğŸ“Š ë°°ì¹˜ ì‚¬ì´ì¦ˆ ì¡°ì •: {dataset_size}ê°œ ìƒ˜í”Œ â†’ batch_size=32 (ë¹ˆë²ˆí•œ ì—…ë°ì´íŠ¸)")
        else:
            batch_size = 64  # ì¤‘ê°„ í¬ê¸°: ê· í˜•ì¡íŒ ì—…ë°ì´íŠ¸
            print(f"   ğŸ“Š ë°°ì¹˜ ì‚¬ì´ì¦ˆ: batch_size=64")
        
        dataloader = DataLoader(
            dataset,
            batch_size=batch_size,
            shuffle=True,
            num_workers=0,
            pin_memory=False
        )
        
        print(f"   ğŸ“š í•™ìŠµ ë°ì´í„°: {len(dataset)}ê°œ ìƒ˜í”Œ, {len(dataloader)}ê°œ ë°°ì¹˜")
        
        # ëª¨ë¸ì„ í•™ìŠµ ëª¨ë“œë¡œ ì „í™˜
        model.train()
        model.to(device)
        
        # Optimizer ë° Loss ì„¤ì •
        criterion = nn.CrossEntropyLoss(ignore_index=0)
        
        # âš ï¸ CRITICAL: Decoderë§Œ í•™ìŠµ (EncoderëŠ” requires_grad=False)
        # filter()ë¡œ requires_grad=Trueì¸ íŒŒë¼ë¯¸í„°ë§Œ Optimizerì— ì „ë‹¬
        trainable_params = filter(lambda p: p.requires_grad, model.parameters())
        optimizer = torch.optim.Adam(trainable_params, lr=5e-5)
        
        # í•™ìŠµí•  íŒŒë¼ë¯¸í„° ê°œìˆ˜ ì¶œë ¥
        trainable_count = sum(p.numel() for p in model.parameters() if p.requires_grad)
        total_count = sum(p.numel() for p in model.parameters())
        print(f"   ğŸ“Š í•™ìŠµ ëŒ€ìƒ íŒŒë¼ë¯¸í„°: {trainable_count:,} / {total_count:,} ({100*trainable_count/total_count:.1f}%)")
        vocab_size = len(word_map)
        
        # íŒŒì¸íŠœë‹ ì§„í–‰ (ì²´í¬í¬ì¸íŠ¸ ì´í›„ë¶€í„° ì‹œì‘)
        for epoch in range(start_epoch, epochs):
            print(f"   ğŸ‹ï¸ Epoch {epoch+1}/{epochs}")
            total_loss = 0
            num_batches = 0
            
            for batch_idx, (imgs, caps) in enumerate(dataloader):
                imgs = imgs.to(device)
                caps = caps.to(device)
                
                optimizer.zero_grad()
            
                try:
                    outputs, alphas = model(imgs, caps)
                    targets = caps[:, 1:]
                    outputs = outputs[:, :targets.shape[1], :]
                    loss = criterion(outputs.reshape(-1, vocab_size), targets.reshape(-1))
                    loss.backward()
                    optimizer.step()
                    
                    total_loss += loss.item()
                    num_batches += 1
                except Exception as e:
                    print(f"   âš ï¸ ë°°ì¹˜ {batch_idx} í•™ìŠµ ì‹¤íŒ¨: {e}")
                    continue
                
                # 10ê°œ ë°°ì¹˜ë§ˆë‹¤ ì§„í–‰ìƒí™© ì¶œë ¥
                if (batch_idx + 1) % 10 == 0:
                    print(f"      ë°°ì¹˜ {batch_idx + 1}/{len(dataloader)}, Loss: {total_loss / num_batches:.4f}")
            
            # ğŸ¯ Epoch ë - ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰
            if num_batches > 0:
                avg_loss = total_loss / num_batches
                print(f"   âœ… Epoch {epoch+1} ì™„ë£Œ (í‰ê·  Loss: {avg_loss:.4f})")
            
            # ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰ (img_tensor, wm, rwmì´ ì œê³µëœ ê²½ìš°)
            if img_tensor is not None and wm is not None and rwm is not None:
                print(f"\n   ğŸ“Š Epoch {epoch+1} ë²¤ì¹˜ë§ˆí¬ ì‹œì‘...")
                model.eval()
                benchmark_result = run_benchmark(
                    model, img_tensor, wm, rwm, 
                    f"Fine-tuned (Epoch {epoch+1}/{epochs})",
                    ref_caption=ref_caption,
                    baseline_params=baseline_params
                )
                model.train()  # ë‹¤ì‹œ í•™ìŠµ ëª¨ë“œë¡œ
                
                # ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼ ì¶œë ¥
                if benchmark_result:
                    print(f"\n   ğŸ“ˆ Epoch {epoch+1} ê²°ê³¼:")
                    print(f"      â±ï¸ í‰ê·  ì‹œê°„: {benchmark_result['mean_time_ms']:.2f} ms")
                    print(f"      ğŸ’¾ ëª¨ë¸ í¬ê¸°: {benchmark_result['model_size_mb']:.2f} MB")
                    print(f"      ğŸ§  ë©”ëª¨ë¦¬: {benchmark_result['memory_usage_mb']:.2f} MB")
                    if benchmark_result.get('meteor_score'):
                        print(f"      â­ METEOR: {benchmark_result['meteor_score']:.4f}")
                
                # ëª¨ë¸ ì €ì¥
                os.makedirs(OUTPUT_DIR, exist_ok=True)
                model_save_path = os.path.join(OUTPUT_DIR, f"pruned_model_epoch_{epoch+1}.pth")
                torch.save(model.state_dict(), model_save_path)
                print(f"      ğŸ’¾ ëª¨ë¸ ì €ì¥: {model_save_path}")
        
        model.eval()
        return model
        
    except Exception as e:
        print(f"   âš ï¸ íŒŒì¸ íŠœë‹ ì‹¤íŒ¨: {e}")
        import traceback
        traceback.print_exc()
        return model

# ============================================================================
# Main
# ============================================================================
def main():
    os.makedirs(OUTPUT_DIR, exist_ok=True)
    
    print("="*70)
    print("=== Pruning ë²¤ì¹˜ë§ˆí¬ ===")
    print("="*70)
    
    # 1. ëª¨ë¸ ë° ë°ì´í„° ë¡œë“œ
    base_model, wm, rwm = load_base_model(device=device)
    img_tensor, ref_caption = load_test_data(device=device, transform=transform)
    
    results = []
    
    # 2. ì›ë³¸ ëª¨ë¸ ë²¤ì¹˜ë§ˆí¬ (Baseline)
    print("\n" + "="*70)
    print("=== [Baseline] ì›ë³¸ ëª¨ë¸ ===")
    print("="*70)
    result_baseline = run_benchmark(base_model, img_tensor, wm, rwm, "Original (Baseline)", ref_caption)
    baseline_params = None
    if result_baseline:
        baseline_params = result_baseline['total_params']
        results.append(result_baseline)
    
    # 3. ë‹¤ì–‘í•œ Pruning Rateë¡œ í…ŒìŠ¤íŠ¸
    for pruning_rate in PRUNING_RATES:
        # Magnitude-based Pruning (ì„ íƒì  - ì´ ëª¨ë¸ì—ëŠ” ë¹„íš¨ìœ¨ì )
        if ENABLE_MAGNITUDE_PRUNING:
            print("\n" + "="*70)
            print(f"=== Magnitude Pruning ({pruning_rate*100:.0f}%) ===")
            print("="*70)
            try:
                pruned_model = apply_magnitude_pruning(base_model, pruning_rate)
                run_pruning_benchmark(pruned_model, f"Magnitude-{pruning_rate*100:.0f}%", img_tensor, wm, rwm, ref_caption, baseline_params, device, results)
            except Exception as e:
                print(f"âš ï¸ Magnitude Pruning ({pruning_rate*100:.0f}%) ì‹¤íŒ¨: {e}")
                import traceback
                traceback.print_exc()
        
        # Structured Pruning
        print("\n" + "="*70)
        print(f"=== Structured Pruning ({pruning_rate*100:.0f}%) ===")
        print("="*70)
        
        # âš ï¸ 30% ì´ìƒ í”„ë£¨ë‹ì€ ì •í™•ë„ ê¸‰ê²©íˆ í•˜ë½í•˜ë¯€ë¡œ ê²½ê³ 
        if pruning_rate > MAX_PRUNING_RATE:
            print(f"   âš ï¸ ê²½ê³ : {pruning_rate*100:.0f}% í”„ë£¨ë‹ì€ ì •í™•ë„ ì†ì‹¤ì´ ë§¤ìš° í¼ (ê¶Œì¥: {MAX_PRUNING_RATE*100:.0f}% ì´í•˜)")
        
        try:
            pruned_model = apply_structured_pruning_physical(
                base_model, pruning_rate, 
                img_tensor=img_tensor, wm=wm, rwm=rwm, 
                device=device, use_hessian=True
            )
            run_pruning_benchmark(pruned_model, f"Structured-{pruning_rate*100:.0f}%", img_tensor, wm, rwm, ref_caption, baseline_params, device, results)
            # run_benchmark(pruned_model, img_tensor, wm, rwm, f"Structured-{pruning_rate*100:.0f}%", ref_caption)

        except Exception as e:
            print(f"âš ï¸ Structured Pruning ({pruning_rate*100:.0f}%) ì‹¤íŒ¨: {e}")
            import traceback
            traceback.print_exc()
    
    
    
    print("\n" + "="*70)
    print("=== ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼ ìš”ì•½ ===")
    print("="*70)
    if any(r.get('meteor_score') is not None for r in results):
        print(f"{'Method':<25} {'ì¶”ë¡ ì‹œê°„(ms)':<15} {'ëª¨ë¸í¬ê¸°(MB)':<15} {'í¬ê¸°ê°ì†Œ(%)':<15} {'Sparsity(%)':<15} {'METEOR':<10}")
        print("-"*100)
        for result in results:
            meteor_str = f"{result.get('meteor_score', 0):.4f}" if result.get('meteor_score') is not None else "N/A"
            sparsity = result.get('sparsity', 0) * 100
            size_reduction = result.get('size_reduction', 0)
            print(f"{result['precision']:<25} "
                  f"{result['mean_time_ms']:.2f}Â±{result['std_time_ms']:.2f}    "
                  f"{result['model_size_mb']:.2f}          "
                  f"{size_reduction:<15.2f} "
                  f"{sparsity:<15.2f} "
                  f"{meteor_str}")
    else:
        print(f"{'Method':<25} {'ì¶”ë¡ ì‹œê°„(ms)':<15} {'ëª¨ë¸í¬ê¸°(MB)':<15} {'í¬ê¸°ê°ì†Œ(%)':<15} {'Sparsity(%)':<15}")
        print("-"*85)
        for result in results:
            sparsity = result.get('sparsity', 0) * 100
            size_reduction = result.get('size_reduction', 0)
            print(f"{result['precision']:<25} "
                  f"{result['mean_time_ms']:.2f}Â±{result['std_time_ms']:.2f}    "
                  f"{result['model_size_mb']:.2f}          "
                  f"{size_reduction:<15.2f} "
                  f"{sparsity:<15.2f}")
    
    # 6. ê²°ê³¼ ì €ì¥
    print("\n" + "="*70)
    print("ê²°ê³¼ ì €ì¥ ì¤‘...")
    print("="*70)
    
    # JSON í˜•ì‹ìœ¼ë¡œ ê²°ê³¼ ì €ì¥
    import json
    results_dict = {
        'baseline_params': baseline_params,
        'results': []
    }
    
    for result in results:
        result_dict = {
            'precision': result['precision'],
            'mean_time_ms': float(result['mean_time_ms']),
            'std_time_ms': float(result['std_time_ms']),
            'min_time_ms': float(result['min_time_ms']),
            'max_time_ms': float(result['max_time_ms']),
            'model_size_mb': float(result['model_size_mb']),
            'model_size_mb_dense': float(result.get('model_size_mb_dense', 0)),
            'memory_usage_mb': float(result['memory_usage_mb']),
            'meteor_score': float(result.get('meteor_score', 0)) if result.get('meteor_score') is not None else None,
            'total_params': int(result['total_params']),
            'trainable_params': int(result.get('trainable_params', 0)),
            'nonzero_params': int(result.get('nonzero_params', 0)),
            'sparsity': float(result.get('sparsity', 0)),
            'size_reduction': float(result.get('size_reduction', 0)),
            'example_caption': result.get('example_caption', 'N/A')
        }
        results_dict['results'].append(result_dict)
    
    results_json_path = os.path.join(OUTPUT_DIR, 'pruning_results.json')
    with open(results_json_path, 'w', encoding='utf-8') as f:
        json.dump(results_dict, f, indent=2, ensure_ascii=False)
    print(f"âœ… ê²°ê³¼ JSON ì €ì¥: {results_json_path}")
    
    # 7. ì‹œê°í™”
    print("\n" + "="*70)
    print("Plot ìƒì„± ì¤‘...")
    print("="*70)
    plot_pruning_comparison(results)
    
    # íŒŒì¸ íŠœë‹ ë¹„êµ ê·¸ë˜í”„ ìƒì„±
    if result_baseline:
        plot_finetune_comparison(results, result_baseline)
    
    print("\n" + "="*70)
    print("=== ë²¤ì¹˜ë§ˆí¬ ì™„ë£Œ ===")
    print(f"ê²°ê³¼ ì €ì¥ ìœ„ì¹˜: {OUTPUT_DIR}")
    print(f"  - JSON: {results_json_path}")
    print(f"  - Plot: {os.path.join(OUTPUT_DIR, 'pruning_comparison_comprehensive.png')}")
    print("="*70)

if __name__ == "__main__":
    main()

