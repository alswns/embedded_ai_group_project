"""
νμΈνλ‹ κ΄€λ ¨ ν—¬νΌ ν•¨μλ“¤
"""

import os
import torch
import torch.nn as nn
from torch.utils.data import DataLoader
from tqdm import tqdm


def load_checkpoint(label, device, checkpoint_dir="pruning_results"):
    """
    μ²΄ν¬ν¬μΈνΈ λ΅λ“ λ° μ‹μ‘ epoch λ°ν™
    
    Returns:
        tuple: (checkpoint_dict, start_epoch, checkpoint_path)
    """
    if not os.path.exists(checkpoint_dir):
        return None, 0, None
    
    checkpoint_files = [f for f in os.listdir(checkpoint_dir) 
                        if f.startswith(label) and f.endswith('')]
    
    if not checkpoint_files:
        return None, 0, None
    
    # κ°€μ¥ μµμ‹  μ²΄ν¬ν¬μΈνΈ μ°ΎκΈ°
    epoch_numbers = []
    for f in checkpoint_files:
        
        try:
            epoch_num = int(f.split('epoch_')[-1].replace('_checkpoint.pt', ''))
            epoch_numbers.append((epoch_num, f))
        except ValueError:
            continue
    
    if not epoch_numbers:
        return None, 0, None
    
    epoch_numbers.sort()
    latest_epoch, latest_file = epoch_numbers[-1]
    checkpoint_path = os.path.join(checkpoint_dir, latest_file)
    
    try:
        checkpoint = torch.load(checkpoint_path, map_location=device, weights_only=False)
        start_epoch = checkpoint.get('epoch', latest_epoch)
        return checkpoint, start_epoch, checkpoint_path
    except Exception as e:
        print(f"   β οΈ μ²΄ν¬ν¬μΈνΈ λ΅λ“ μ‹¤ν¨: {e}")
        return None, 0, None


def setup_training(model, learning_rate, device, freeze_encoder=True):
    """ν•™μµ μ„¤μ •: Encoder Freeze, Optimizer μƒμ„±"""
    # Encoder Freeze
    if freeze_encoder and hasattr(model, 'encoder'):
        for param in model.encoder.parameters():
            param.requires_grad = False
        print(f"   π”’ Encoder Freeze: CNN νλΌλ―Έν„° ν•™μµ κΈμ§€")
    
    # Optimizer μ„¤μ •
    criterion = nn.CrossEntropyLoss(ignore_index=0)
    trainable_params = filter(lambda p: p.requires_grad, model.parameters())
    optimizer = torch.optim.Adam(trainable_params, lr=learning_rate)
    
    print(f"   π“ ν•™μµλ¥ : {learning_rate}")
    
    # ν•™μµν•  νλΌλ―Έν„° κ°μ
    trainable_count = sum(p.numel() for p in model.parameters() if p.requires_grad)
    total_count = sum(p.numel() for p in model.parameters())
    print(f"   π“ ν•™μµ λ€μƒ νλΌλ―Έν„°: {trainable_count:,} / {total_count:,} ({100*trainable_count/total_count:.1f}%)")
    
    return optimizer, criterion


def save_checkpoint(model, optimizer, epoch, label, word_map, rev_word_map, vocab_size,
                   avg_train_loss=None, avg_val_loss=None, meteor_score=None,
                   checkpoint_dir="pruning_results"):
    """μ²΄ν¬ν¬μΈνΈ μ €μ¥ (ν†µμΌλ ν•μ‹)"""
    os.makedirs(checkpoint_dir, exist_ok=True)
    checkpoint_path = os.path.join(checkpoint_dir, f"{label}_epoch_{epoch+1}_checkpoint.pt")
    
    checkpoint = {
        'model_state_dict': model.state_dict(),
        'word_map': word_map,
        'rev_word_map': rev_word_map,
        'vocab_size': vocab_size,
        'epoch': epoch + 1,
        'optimizer_state_dict': optimizer.state_dict(),
    }
    
    if avg_train_loss is not None:
        checkpoint['train_loss'] = avg_train_loss
    if avg_val_loss is not None:
        checkpoint['val_loss'] = avg_val_loss
    if meteor_score is not None:
        checkpoint['meteor_score'] = meteor_score
    
    torch.save(checkpoint, checkpoint_path)
    print(f"      π’Ύ μ²΄ν¬ν¬μΈνΈ μ €μ¥ μ™„λ£: {checkpoint_path}")
    return checkpoint_path


def print_checkpoint_info(checkpoint, latest_epoch):
    """μ²΄ν¬ν¬μΈνΈ μ •λ³΄ μ¶λ ¥"""
    print(f"   π“‚ μ²΄ν¬ν¬μΈνΈ λ°κ²¬: Epoch {latest_epoch}")
    
    if 'train_loss' in checkpoint:
        print(f"   π“ μ΄μ „ ν•™μµ Loss: {checkpoint['train_loss']:.4f}")
    if 'val_loss' in checkpoint:
        print(f"   π“ μ΄μ „ κ²€μ¦ Loss: {checkpoint['val_loss']:.4f}")
    if 'meteor_score' in checkpoint:
        print(f"   β­ μ΄μ „ METEOR: {checkpoint['meteor_score']:.4f}")
    if 'vocab_size' in checkpoint:
        print(f"   π“ μ–΄νμ§‘ ν¬κΈ°: {checkpoint['vocab_size']:,}")


def restore_optimizer(optimizer, optimizer_state):
    """Optimizer State λ³µκµ¬"""
    if optimizer_state is None:
        return
    
    try:
        optimizer.load_state_dict(optimizer_state)
        print(f"   β… Optimizer State λ³µκµ¬ μ™„λ£")
    except Exception as e:
        print(f"   β οΈ Optimizer State λ³µκµ¬ μ‹¤ν¨: {e}")


def load_model_checkpoint(checkpoint_path, device):
    """μ €μ¥λ λ¨λΈ μ²΄ν¬ν¬μΈνΈ λ΅λ“"""
    try:
        checkpoint = torch.load(checkpoint_path, map_location=device, weights_only=False)
        print(f"β… μ²΄ν¬ν¬μΈνΈ λ΅λ“ μ™„λ£: {checkpoint_path}")
        
        if 'epoch' in checkpoint:
            print(f"   π“‚ Epoch: {checkpoint['epoch']}")
        if 'vocab_size' in checkpoint:
            print(f"   π“ μ–΄νμ§‘ ν¬κΈ°: {checkpoint['vocab_size']:,}")
        if 'train_loss' in checkpoint:
            print(f"   π“ ν•™μµ Loss: {checkpoint['train_loss']:.4f}")
        if 'val_loss' in checkpoint:
            print(f"   π“ κ²€μ¦ Loss: {checkpoint['val_loss']:.4f}")
        
        return checkpoint
    except Exception as e:
        print(f"β μ²΄ν¬ν¬μΈνΈ λ΅λ“ μ‹¤ν¨: {e}")
        return None


def load_model_from_checkpoint(checkpoint_path, model, device):
    """μ²΄ν¬ν¬μΈνΈμ—μ„ λ¨λΈκ³Ό word_map λ΅λ“"""
    checkpoint = load_model_checkpoint(checkpoint_path, device)
    if checkpoint is None:
        return None, None, None, None
    
    if 'model_state_dict' in checkpoint:
        model.load_state_dict(checkpoint['model_state_dict'])
        print(f"   β… λ¨λΈ κ°€μ¤‘μΉ λ΅λ“ μ™„λ£")
    
    word_map = checkpoint.get('word_map')
    rev_word_map = checkpoint.get('rev_word_map')
    vocab_size = checkpoint.get('vocab_size')
    
    return model, word_map, rev_word_map, vocab_size


def apply_magnitude_mask(model):
    """Magnitude Pruningμ λ§μ¤ν¬ κ°•μ  μ μ© (λ§¤ λ°°μΉλ§λ‹¤ νΈμ¶)"""
    if hasattr(model, '_magnitude_pruning_masks'):
        for mask_key, (module, param_name, mask) in model._magnitude_pruning_masks.items():
            param = getattr(module, param_name)
            param.data = param.data * mask.to(param.device)


def fine_tune_model(model, train_dataloader, val_dataloader, word_map, device,
                   epochs=10, learning_rate=5e-5, label="finetuned",
                   early_stopping_patience=3, benchmark_fn=None,
                   img_tensor=None, wm=None, rwm=None, ref_caption=None, baseline_params=None):
    """
    νμΈνλ‹ μν–‰ (ν†µν•© ν•¨μ)
    
    Args:
        model: νμΈνλ‹ν•  λ¨λΈ
        train_dataloader: ν•™μµ λ°μ΄ν„°λ΅λ”
        val_dataloader: κ²€μ¦ λ°μ΄ν„°λ΅λ”
        word_map: λ‹¨μ–΄ β†’ μΈλ±μ¤ λ§¤ν•‘
        device: λ””λ°”μ΄μ¤
        epochs: μ—ν¬ν¬ μ
        learning_rate: ν•™μµλ¥ 
        label: μ²΄ν¬ν¬μΈνΈ λ μ΄λΈ”
        early_stopping_patience: Early Stopping μΈλ‚΄μ‹¬
        benchmark_fn: λ²¤μΉλ§ν¬ ν•¨μ (optional)
        img_tensor, wm, rwm, ref_caption, baseline_params: λ²¤μΉλ§ν¬μ© νλΌλ―Έν„°
    
    Returns:
        model: νμΈνλ‹λ λ¨λΈ
    """
    print(f"\n   π”„ νμΈ νλ‹ μ‹μ‘ ({epochs} epoch)...")
    
    # μ²΄ν¬ν¬μΈνΈ λ΅λ“
    checkpoint, start_epoch, checkpoint_path = load_checkpoint(label, device)
    optimizer_state = checkpoint.get('optimizer_state_dict') if checkpoint else None
    
    if checkpoint:
        model.load_state_dict(checkpoint['model_state_dict'])
        print_checkpoint_info(checkpoint, start_epoch)
        print(f"   β… Epoch {start_epoch+1}λ¶€ν„° μ¬κ°ν•©λ‹λ‹¤.")
    else:
        print(f"   β„ΉοΈ μ²μλ¶€ν„° μ‹μ‘ν•©λ‹λ‹¤.")
    
    # ν•™μµ μ„¤μ •
    optimizer, criterion = setup_training(model, learning_rate, device)
    restore_optimizer(optimizer, optimizer_state)
    
    # λ¨λΈ μ„¤μ •
    model.train()
    model.to(device)
    
    vocab_size = len(word_map)
    rev_word_map = {v: k for k, v in word_map.items()}
    
    # Early Stopping μ„¤μ •
    best_meteor_score = -float('inf')
    best_loss = float('inf')
    patience_counter = 0
    best_model_state = None
    
    # νμΈνλ‹ μ§„ν–‰
    for epoch in range(start_epoch, epochs):
        print(f"   π‹οΈ Epoch {epoch+1}/{epochs}")
        total_loss = 0
        num_batches = 0
        
        train_iter = tqdm(enumerate(train_dataloader), total=len(train_dataloader), 
                         desc=f"      ν•™μµ μ¤‘", ncols=100)
        
        for batch_idx, (imgs, caps) in train_iter:
            imgs = imgs.to(device)
            caps = caps.to(device)
            
            optimizer.zero_grad()
            
            try:
                outputs, alphas = model(imgs, caps)
                targets = caps[:, 1:]
                outputs = outputs[:, :targets.shape[1], :]
                loss = criterion(outputs.reshape(-1, vocab_size), targets.reshape(-1))
                loss.backward()
                optimizer.step()
                
                # Magnitude Pruning λ§μ¤ν¬ κ°•μ  μ μ©
                apply_magnitude_mask(model)
                
                total_loss += loss.item()
                num_batches += 1
            except Exception as e:
                print(f"   β οΈ λ°°μΉ {batch_idx} ν•™μµ μ‹¤ν¨: {e}")
                continue
            
            if (batch_idx + 1) % 10 == 0:
                train_iter.set_postfix(loss=f"{total_loss / num_batches:.4f}")
        
        # Epoch μ™„λ£
        avg_loss = total_loss / num_batches if num_batches > 0 else float('inf')
        print(f"   β… Epoch {epoch+1} μ™„λ£ (ν•™μµ Loss: {avg_loss:.4f})")
        
        # κ²€μ¦
        print(f"   π“ κ²€μ¦ λ°μ΄ν„° ν‰κ°€ μ¤‘...")
        model.eval()
        val_loss = 0
        val_batches = 0
        
        with torch.no_grad():
            for val_imgs, val_caps in tqdm(val_dataloader, desc="      κ²€μ¦ μ¤‘", ncols=100):
                val_imgs = val_imgs.to(device)
                val_caps = val_caps.to(device)
                
                try:
                    val_outputs, _ = model(val_imgs, val_caps)
                    val_targets = val_caps[:, 1:]
                    val_outputs = val_outputs[:, :val_targets.shape[1], :]
                    val_loss_batch = criterion(val_outputs.reshape(-1, vocab_size), val_targets.reshape(-1))
                    val_loss += val_loss_batch.item()
                    val_batches += 1
                except:
                    continue
        
        avg_val_loss = val_loss / val_batches if val_batches > 0 else float('inf')
        print(f"      κ²€μ¦ Loss: {avg_val_loss:.4f}")
        
        model.train()
        
        # λ²¤μΉλ§ν¬ μ‹¤ν–‰ (μµμ…)
        current_meteor_score = None
        if benchmark_fn and img_tensor is not None and wm is not None and rwm is not None:
            print(f"\n   π“ Epoch {epoch+1} λ²¤μΉλ§ν¬ μ‹μ‘...")
            model.eval()
            benchmark_result = benchmark_fn(
                model, img_tensor, wm, rwm,
                f"Fine-tuned (Epoch {epoch+1}/{epochs})",
                ref_caption=ref_caption,
                baseline_params=baseline_params
            )
            model.train()
            
            if benchmark_result and benchmark_result.get('meteor_score'):
                current_meteor_score = benchmark_result['meteor_score']
                print(f"      β­ METEOR: {current_meteor_score:.4f}")
        
        # Early Stopping μ²΄ν¬
        if current_meteor_score is not None and best_meteor_score is not None:
            
            if current_meteor_score > best_meteor_score:
                best_meteor_score = current_meteor_score
                patience_counter = 0
                best_model_state = model.state_dict().copy()
                print(f"   π‰ μƒλ΅μ΄ μµκ³  METEOR μ μ: {best_meteor_score:.4f}")
            elif avg_val_loss < best_loss:
                best_loss = avg_val_loss
                patience_counter = 0
                best_model_state = model.state_dict().copy()
                print(f"   π‰ μƒλ΅μ΄ μµμ € κ²€μ¦ Loss: {best_loss:.4f}")
                
            else:
                patience_counter += 1
                print(f"   β οΈ METEOR λ―Έκ°μ„  (Patience: {patience_counter}/{early_stopping_patience})")
                print(f"   β οΈ κ²€μ¦ Loss λ―Έκ°μ„  (Patience: {patience_counter}/{early_stopping_patience})")
                
                if patience_counter >= early_stopping_patience:
                    print(f"\n   π›‘ Early Stopping λ°λ™! Epoch {epoch+1}μ—μ„ ν•™μµ μΆ…λ£")
                    if best_model_state:
                        model.load_state_dict(best_model_state)
                    break
            
            # μ²΄ν¬ν¬μΈνΈ μ €μ¥
            save_checkpoint(
                model, optimizer, epoch, label,
                word_map=word_map,
                rev_word_map=rev_word_map,
                vocab_size=vocab_size,
                avg_train_loss=avg_loss,
                avg_val_loss=avg_val_loss,
                meteor_score=current_meteor_score
            )
    
    print(f"\n   β… νμΈ νλ‹ μ™„λ£")
    return model
