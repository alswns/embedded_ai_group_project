"""
ì–‘ìí™”(Quantization) ê´€ë ¨ ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ë“¤
- Dynamic Quantization (INT8)
- Static Quantization (Calibration)
- QAT (Quantization Aware Training)
"""

import torch
import torch.nn as nn
from copy import deepcopy
from tqdm import tqdm


# ============================================================================
# ì–‘ìí™” ì—”ì§„ ì„¤ì •
# ============================================================================
def setup_quantization_engine():
    """
    PyTorch ì–‘ìí™” ì—”ì§„ ì„¤ì •
    
    PyTorchëŠ” ì—¬ëŸ¬ ë°±ì—”ë“œë¥¼ ì§€ì›:
    - 'fbgemm': x86 CPU (Linux/Windows)
    - 'qnnpack': ARM CPU (ëª¨ë°”ì¼)
    """
    try:
        # CPUì—ì„œ ì‚¬ìš© ê°€ëŠ¥í•œ ì—”ì§„ í™•ì¸
        import torch.backends.quantized as quantized_backends
        
        # fbgemm ìš°ì„  ì‹œë„ (x86 CPU)
        if hasattr(torch.backends, 'quantized'):
            try:
                torch.backends.quantized.engine = 'fbgemm'
                print("âœ… ì–‘ìí™” ì—”ì§„: fbgemm (x86 CPU)")
                return 'fbgemm'
            except:
                pass
        
        # qnnpack ì‹œë„ (ARM CPU, í´ë°±)
        try:
            torch.backends.quantized.engine = 'qnnpack'
            print("âœ… ì–‘ìí™” ì—”ì§„: qnnpack (ARM CPU)")
            return 'qnnpack'
        except:
            pass
        
        # ëª¨ë‘ ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ê°’ ì‚¬ìš©
        print("âš ï¸ ì–‘ìí™” ì—”ì§„ì„ ìë™ìœ¼ë¡œ ì„ íƒí–ˆìŠµë‹ˆë‹¤.")
        return None
    
    except Exception as e:
        print("âš ï¸ ì–‘ìí™” ì—”ì§„ ì„¤ì • ì‹¤íŒ¨: {}".format(e))
        return None


# ============================================================================
# Dynamic Quantization
# ============================================================================
def apply_dynamic_quantization(model, dtype=torch.qint8, inplace=False):
    """
    ë™ì  ì–‘ìí™” ì ìš© (ì¶”ê°€ í•™ìŠµ ë¶ˆí•„ìš”)
    
    FP32 â†’ INT8 ìë™ ë³€í™˜
    - CPU ì¶”ë¡ : 2-3ë°° ê°€ì†
    - ë©”ëª¨ë¦¬: 4ë°° ê°ì†Œ
    - ì •í™•ë„: 1-2% ì†ì‹¤
    
    Args:
        model: ì–‘ìí™”í•  ëª¨ë¸
        dtype: ì–‘ìí™” ë°ì´í„° íƒ€ì… (torch.qint8, torch.qint32)
        inplace: ì›ë³¸ ëª¨ë¸ ìˆ˜ì • ì—¬ë¶€
    
    Returns:
        ì–‘ìí™”ëœ ëª¨ë¸
    """
    # ì—”ì§„ ì„¤ì •
    setup_quantization_engine()
    
    if not inplace:
        model = deepcopy(model)
    
    try:
        print("   ğŸ”„ ë™ì  ì–‘ìí™” ì ìš© ì¤‘...")
        
        # CPUë¡œ ì´ë™ (ì–‘ìí™”ëŠ” CPUì—ì„œë§Œ ì§€ì›)
        model_device = next(model.parameters()).device
        model = model.cpu()
        
        # Dynamic Quantization ì ìš©
        quantized_model = torch.quantization.quantize_dynamic(
            model,
            qconfig_spec={torch.nn.Linear,torch.nn.Conv2d},  # Linear ë ˆì´ì–´ë§Œ ì–‘ìí™”
            dtype=dtype
        )
        
        # ì›ë˜ deviceë¡œ ë³µì›
        quantized_model = quantized_model.to(model_device)
        
        print("   âœ… ë™ì  ì–‘ìí™” ì™„ë£Œ")
        return quantized_model
    
    except RuntimeError as e:
        if "NoQEngine" in str(e):
            print("   âŒ ì–‘ìí™” ì—”ì§„ ì˜¤ë¥˜: {}".format(e))
            print("      í•´ê²°: torch ì¬ì„¤ì¹˜ ë˜ëŠ” ë‹¤ë¥¸ ì–‘ìí™” ë°©ì‹ ì‚¬ìš©")
            return model
        else:
            raise


# ============================================================================
# Static Quantization
# ============================================================================
def apply_static_quantization(model, calibration_dataloader, device='cpu', inplace=False):
    """
    ì •ì  ì–‘ìí™” ì ìš© (Calibration í•„ìš”)
    
    ë™ì  ì–‘ìí™”ë³´ë‹¤ ì •í™•ë„ ìš°ìˆ˜
    - CPU ì¶”ë¡ : 3-4ë°° ê°€ì†
    - ë©”ëª¨ë¦¬: 4ë°° ê°ì†Œ
    - ì •í™•ë„: 0.5-1% ì†ì‹¤
    
    Args:
        model: ì–‘ìí™”í•  ëª¨ë¸
        calibration_dataloader: Calibrationìš© ë°ì´í„°ë¡œë”
        device: ì‹¤í–‰ device
        inplace: ì›ë³¸ ëª¨ë¸ ìˆ˜ì • ì—¬ë¶€
    
    Returns:
        ì–‘ìí™”ëœ ëª¨ë¸
    """
    setup_quantization_engine()
    
    if not inplace:
        model = deepcopy(model)
    
    try:
        print("   ğŸ”„ ì •ì  ì–‘ìí™” ì¤€ë¹„ ì¤‘...")
        
        # CPUë¡œ ì´ë™
        model_device = model.device if hasattr(model, 'device') else device
        model = model.cpu()
        
        # Quantization config ì„¤ì •
        model.qconfig = torch.quantization.get_default_qconfig('fbgemm')
        
        # Prepare (ì–‘ìí™” ì¤€ë¹„)
        torch.quantization.prepare(model, inplace=True)
        
        # Calibration (ë²”ìœ„ ì¸¡ì •)
        print("   ğŸ“Š Calibration ì§„í–‰ ì¤‘...")
        model.eval()
        with torch.no_grad():
            for batch_idx, (imgs, caps) in enumerate(calibration_dataloader):
                imgs = imgs.cpu()
                try:
                    _ = model(imgs, caps)
                except:
                    # caps ì—†ì´ ì‹œë„
                    _ = model(imgs)
                
                if (batch_idx + 1) % 10 == 0:
                    print("      Calibration: {} batches".format(batch_idx + 1))
                
                # ì²˜ìŒ 50ê°œ ë°°ì¹˜ë§Œ ì‚¬ìš© (ì¶©ë¶„í•œ ë²”ìœ„ ì¸¡ì •)
                if batch_idx >= 50:
                    break
        
        # Convert (ì–‘ìí™” ì ìš©)
        print("   âœ… ì •ì  ì–‘ìí™” ì™„ë£Œ (Calibration)")
        torch.quantization.convert(model, inplace=True)
        
        # ì›ë˜ deviceë¡œ ë³µì›
        model = model.to(model_device)
        return model
    
    except Exception as e:
        print("   âŒ ì •ì  ì–‘ìí™” ì‹¤íŒ¨: {}".format(e))
        return model


# ============================================================================
# QAT (Quantization Aware Training)
# ============================================================================
def apply_qat(model, train_dataloader, epochs=3, device='cpu', 
             learning_rate=1e-4, inplace=False):
    """
    ì–‘ìí™” ì¸ì‹ í•™ìŠµ (QAT) - ì¬í•™ìŠµìœ¼ë¡œ ì •í™•ë„ ìµœëŒ€í™”
    
    ì–‘ìí™”ë¥¼ ê³ ë ¤í•˜ì—¬ ëª¨ë¸ ì¬í•™ìŠµ
    - CPU ì¶”ë¡ : 3-4ë°° ê°€ì†
    - ë©”ëª¨ë¦¬: 4ë°° ê°ì†Œ
    - ì •í™•ë„: ê±°ì˜ ë¬´ì†ì‹¤ (~0.1%)
    
    Args:
        model: ì–‘ìí™”í•  ëª¨ë¸
        train_dataloader: í•™ìŠµìš© ë°ì´í„°ë¡œë”
        epochs: QAT ì—í¬í¬ ìˆ˜ (ë³´í†µ 3-5)
        device: ì‹¤í–‰ device
        learning_rate: í•™ìŠµë¥  (ë³´í†µ ì›ë˜ì˜ 1/10)
        inplace: ì›ë³¸ ëª¨ë¸ ìˆ˜ì • ì—¬ë¶€
    
    Returns:
        ì–‘ìí™”ëœ ëª¨ë¸
    """
    setup_quantization_engine()
    
    if not inplace:
        model = deepcopy(model)
    
    try:
        print("   ğŸ”„ QAT ì¤€ë¹„ ì¤‘...")
        
        # CPUë¡œ ì´ë™
        model_device = next(model.parameters()).device
        model = model.cpu()

        # ëª¨ë¸ì„ í•™ìŠµ ëª¨ë“œë¡œ
        model.train()

        # QAT config ì„¤ì •
        model.qconfig = torch.quantization.get_default_qat_qconfig('fbgemm')
        
        # Prepare for QAT
        torch.quantization.prepare_qat(model, inplace=True)
        
        
        # Optimizer ì„¤ì • (í° learning rate ë¶ˆí•„ìš”)
        optimizer = torch.optim.Adam(
            filter(lambda p: p.requires_grad, model.parameters()),
            lr=learning_rate
        )
        criterion = torch.nn.CrossEntropyLoss()
        
        # QAT í•™ìŠµ ë£¨í”„ (ì§§ê²Œ, 3-5 ì—í¬í¬)
        print("   ğŸ“š QAT í•™ìŠµ ì‹œì‘ ({} ì—í¬í¬)...".format(epochs))
        for epoch in range(epochs):
            total_loss = 0
            num_batches = 0
            
            for batch_idx, (imgs, caps) in enumerate(tqdm(train_dataloader, 
                                                          desc="QAT Epoch {}/{}".format(epoch+1, epochs),
                                                          disable=True)):
                imgs = imgs.cpu()
                caps = caps.cpu()
                
                optimizer.zero_grad()
                
                try:
                    outputs, _ = model(imgs, caps)
                    targets = caps[:, 1:]
                    outputs = outputs[:, :targets.shape[1], :]
                    vocab_size = outputs.shape[-1]
                    
                    loss = criterion(outputs.reshape(-1, vocab_size), targets.reshape(-1))
                    loss.backward()
                    optimizer.step()
                    
                    total_loss += loss.item()
                    num_batches += 1
                except Exception as e:
                    continue
                
                # ì²˜ìŒ 20ê°œ ë°°ì¹˜ë§Œ ì‚¬ìš© (ì¶©ë¶„í•œ í•™ìŠµ)
                if batch_idx >= 20:
                    break
            
            avg_loss = total_loss / num_batches if num_batches > 0 else 0
            print("      Epoch {} Loss: {:.4f}".format(epoch+1, avg_loss))
        
        # Convert (ì–‘ìí™” ì ìš©)
        print("   âœ… QAT ì™„ë£Œ (Convert)")
        torch.quantization.convert(model, inplace=True)
        
        # ì›ë˜ deviceë¡œ ë³µì›
        model = model.to(model_device)
        return model
    
    except Exception as e:
        print("   âŒ QAT ì‹¤íŒ¨: {}".format(e))
        return model


# ============================================================================
# ëª¨ë¸ í¬ê¸° ë¹„êµ
# ============================================================================
def get_quantized_model_size_mb(model):
    """ì–‘ìí™”ëœ ëª¨ë¸ í¬ê¸° ê³„ì‚° (MB)"""
    param_size = 0
    
    for param in model.parameters():
        # ì–‘ìí™”ëœ parameter í™•ì¸
        if hasattr(param, 'q_scale'):  # Quantized parameter
            # INT8: 1 ë°”ì´íŠ¸ + scale/zero_point
            param_size += param.numel() * 1  # INT8 = 1 byte
        else:
            # ì¼ë°˜ parameter (FP32)
            param_size += param.numel() * 4  # FP32 = 4 bytes
    
    for buffer in model.buffers():
        if buffer.dtype in [torch.qint8, torch.uint8]:
            param_size += buffer.numel() * 1
        else:
            param_size += buffer.numel() * 4
    
    return param_size / (1024 * 1024)


def print_quantization_stats(original_model, quantized_model):
    """ì–‘ìí™” ì „í›„ ëª¨ë¸ í†µê³„ ì¶œë ¥"""
    from .pruning_utils import count_nonzero_parameters
    from .model_utils import count_parameters
    
    orig_params, _ = count_parameters(original_model)
    quant_params, _ = count_parameters(quantized_model)
    
    # í¬ê¸° ì¶”ì •
    orig_size = (orig_params * 4) / (1024 * 1024)  # FP32
    quant_size = (quant_params * 1) / (1024 * 1024)  # INT8 (ëŒ€ëµ)
    
    print("\nğŸ“Š ì–‘ìí™” í†µê³„:")
    print("   ì›ë³¸ ëª¨ë¸:")
    print("      â€¢ íŒŒë¼ë¯¸í„°: {}".format(orig_params))
    print("      â€¢ í¬ê¸°: {:.2f} MB (FP32)".format(orig_size))
    print("   ì–‘ìí™” ëª¨ë¸:")
    print("      â€¢ íŒŒë¼ë¯¸í„°: {}".format(quant_params))
    print("      â€¢ í¬ê¸°: {:.2f} MB (INT8)".format(quant_size))
    print("      â€¢ ê°ì†Œìœ¨: {:.1f}%".format((1 - quant_size/orig_size)*100))