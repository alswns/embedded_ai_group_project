"""
Pruning ê´€ë ¨ ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ë“¤
- íŒŒë¼ë¯¸í„° ê³„ì‚°
- í”„ë£¨ë‹ ë§ˆìŠ¤í¬ ìƒì„±
- Hessian ì¤‘ìš”ë„ ê³„ì‚°
- ë ˆì´ì–´ ì—…ë°ì´íŠ¸
"""

import torch
import torch.nn as nn
import numpy as np


def count_nonzero_parameters(model):
    """0ì´ ì•„ë‹Œ íŒŒë¼ë¯¸í„° ê°œìˆ˜ ê³„ì‚° (í”„ë£¨ë‹ í›„)"""
    nonzero_params = 0
    total_params = 0
    for param in model.parameters():
        total_params += param.numel()
        nonzero_params += param.nonzero().size(0) if param.numel() > 0 else 0
    return nonzero_params, total_params


def convert_to_sparse_model(model):
    """Pruningëœ ëª¨ë¸ì„ ì‹¤ì œë¡œ sparse formatìœ¼ë¡œ ë³€í™˜í•˜ì—¬ í¬ê¸° ê°ì†Œ"""
    # ì‹¤ì œ 0ì´ ì•„ë‹Œ íŒŒë¼ë¯¸í„°ë§Œ ê³„ì‚°
    pass


def save_sparse_model(model, path):
    """Sparse ëª¨ë¸ ì €ì¥"""
    try:
        # í¬ê¸° ê³„ì‚°
        total_size = 0
        nonzero_count = 0
        
        for name, param in model.named_parameters():
            total_size += param.numel()
            nonzero_count += (param != 0).sum().item()
        
        state_dict = model.state_dict()
        torch.save(state_dict, path)
        
        return True
    except Exception as e:
        print(f"âŒ ì €ì¥ ì‹¤íŒ¨: {e}")
        return False


def get_sparse_model_size_mb(model):
    """Sparse ëª¨ë¸ì˜ ë©”ëª¨ë¦¬ í¬ê¸° ê³„ì‚° (MB)"""
    param_size = 0
    buffer_size = 0
    
    # Sparse tensorìš© ì¶”ì •ì¹˜
    for param in model.parameters():
        if param.is_sparse:
            indices_size = param._indices().numel() * 8  # int64
            values_size = param._values().numel() * 4  # float32
            param_size += indices_size + values_size
        else:
            nonzero = (param != 0).sum().item()
            if nonzero > 0:
                param_size += nonzero * 4  # float32
    
    for buffer in model.buffers():
        if buffer.is_sparse:
            indices_size = buffer._indices().numel() * 8
            values_size = buffer._values().numel() * 4
            buffer_size += indices_size + values_size
        else:
            buffer_size += buffer.numel() * 4
    
    return (param_size + buffer_size) / (1024 * 1024)


def get_pruning_mask(weight, pruning_rate, dim=0, use_l2=True):
    """
    L2 norm ë˜ëŠ” magnitude ê¸°ë°˜ í”„ë£¨ë‹ ë§ˆìŠ¤í¬ ìƒì„±
    
    Args:
        weight: í”„ë£¨ë‹í•  ê°€ì¤‘ì¹˜ í…ì„œ
        pruning_rate: í”„ë£¨ë‹ ë¹„ìœ¨ (0.0 ~ 1.0)
        dim: í”„ë£¨ë‹ ì°¨ì› (0: ì¶œë ¥, 1: ì…ë ¥)
        use_l2: Trueë©´ L2 norm, Falseë©´ magnitude
    
    Returns:
        mask: ìœ ì§€í•  ì±„ë„ (True)ì™€ ì œê±°í•  ì±„ë„ (False)
    """
    if dim == 0:
        importance = torch.norm(weight, p=2, dim=tuple(range(1, len(weight.shape))))
    else:
        importance = torch.norm(weight, p=2, dim=0)
    
    num_to_prune = int(pruning_rate * len(importance))
    if num_to_prune == 0:
        return torch.ones(len(importance), dtype=torch.bool, device=weight.device)
    
    _, indices = torch.topk(importance, num_to_prune, largest=False)
    mask = torch.ones(len(importance), dtype=torch.bool, device=weight.device)
    mask[indices] = False
    
    return mask


def update_linear_layer(old_layer, mask_in=None, mask_out=None, in_size=None, out_size=None):
    """
    ì„ í˜• ë ˆì´ì–´ì˜ ê°€ì¤‘ì¹˜ë¥¼ ë§ˆìŠ¤í¬ ë˜ëŠ” í¬ê¸°ì— ë”°ë¼ ì—…ë°ì´íŠ¸
    
    Args:
        old_layer: ì›ë³¸ Linear ë ˆì´ì–´
        mask_in: ì…ë ¥ ë§ˆìŠ¤í¬ (True: ìœ ì§€, False: ì œê±°)
        mask_out: ì¶œë ¥ ë§ˆìŠ¤í¬
        in_size: ìƒˆë¡œìš´ ì…ë ¥ í¬ê¸°
        out_size: ìƒˆë¡œìš´ ì¶œë ¥ í¬ê¸°
    
    Returns:
        new_layer: ì—…ë°ì´íŠ¸ëœ Linear ë ˆì´ì–´
    """
    if mask_in is not None:
        in_features = mask_in.sum().item()
    else:
        in_features = in_size if in_size is not None else old_layer.in_features
    
    if mask_out is not None:
        out_features = mask_out.sum().item()
    else:
        out_features = out_size if out_size is not None else old_layer.out_features
    
    new_layer = nn.Linear(in_features, out_features, bias=old_layer.bias is not None)
    
    # ê°€ì¤‘ì¹˜ ì—…ë°ì´íŠ¸
    if mask_in is not None and mask_out is not None:
        new_layer.weight.data = old_layer.weight.data[mask_out, :][:, mask_in]
    elif mask_out is not None:
        new_layer.weight.data = old_layer.weight.data[mask_out, :]
    elif mask_in is not None:
        new_layer.weight.data = old_layer.weight.data[:, mask_in]
    else:
        new_layer.weight.data = old_layer.weight.data
    
    # Bias ì—…ë°ì´íŠ¸
    if old_layer.bias is not None and mask_out is not None:
        new_layer.bias.data = old_layer.bias.data[mask_out]
    elif old_layer.bias is not None:
        new_layer.bias.data = old_layer.bias.data
    
    return new_layer


def compute_channel_importance_hessian(weight, pruning_rate, dim=1, hessian_importance=None):
    """
    Hessian ë˜ëŠ” L2 norm ê¸°ë°˜ ì±„ë„ ì¤‘ìš”ë„ ê³„ì‚°
    
    Args:
        weight: ê°€ì¤‘ì¹˜ í…ì„œ
        pruning_rate: í”„ë£¨ë‹ ë¹„ìœ¨
        dim: í”„ë£¨ë‹ ì°¨ì›
        hessian_importance: Hessian ì¤‘ìš”ë„ (ìˆìœ¼ë©´ ì‚¬ìš©)
    
    Returns:
        mask: ìœ ì§€í•  ì±„ë„ ë§ˆìŠ¤í¬
    """
    if hessian_importance is not None:
        # Hessian ê¸°ë°˜: ì†ì‹¤ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ë„ (2ì°¨ ì •ë³´) ì‚¬ìš©
        if dim == 1:
            channel_importance = (hessian_importance * (weight ** 2)).sum(dim=0)
        else:
            channel_importance = (hessian_importance * (weight ** 2)).sum(dim=1)
    else:
        # L2 norm ê¸°ë°˜
        if dim == 1:
            channel_importance = torch.norm(weight, p=2, dim=0)
        else:
            channel_importance = torch.norm(weight, p=2, dim=1)
    
    # ì¤‘ìš”ë„ê°€ ë‚®ì€ ì±„ë„ ì„ íƒ
    num_to_prune = int(pruning_rate * channel_importance.numel())
    if num_to_prune == 0:
        return torch.ones(channel_importance.numel(), dtype=torch.bool, device=weight.device)
    
    _, indices = torch.sort(channel_importance)
    mask = torch.ones(channel_importance.numel(), dtype=torch.bool, device=weight.device)
    mask[indices[:num_to_prune]] = False
    
    return mask


def compute_hessian_importance(model, layer, img_tensor, captions_batch, wm, rwm, device, num_samples=64):
    """
    Hessian í–‰ë ¬ì„ ê·¼ì‚¬í•˜ì—¬ ì¤‘ìš”ë„ ê³„ì‚°
    
    Fisher Information Matrixë¥¼ ì´ìš©: F = E[g * g^T]
    ì—¬ê¸°ì„œ gëŠ” gradient
    """
    print(f"      ğŸ” Hessian ê³„ì‚° ì¤‘ ({num_samples}ê°œ ìƒ˜í”Œ)...")
    
    # ì…ë ¥ í…ì„œ ì¤€ë¹„
    model.eval()
    model.to(device)
    
    # Hessian ê·¼ì‚¬ (Diagonal approximation)
    hessian = None
    criterion = nn.CrossEntropyLoss(ignore_index=0)
    
    for i in range(num_samples):
        idx = i % len(img_tensor)
        img = img_tensor[idx:idx+1].to(device)
        caps = captions_batch[idx:idx+1].to(device)
        
        model.zero_grad()
        
        try:
            # Forward pass
            outputs, _ = model(img, caps)
            targets = caps[:, 1:]
            outputs_trimmed = outputs[:, :targets.shape[1], :]
            
            # Loss
            vocab_size = outputs_trimmed.shape[-1]
            loss = criterion(outputs_trimmed.reshape(-1, vocab_size), targets.reshape(-1))
            
            # Backward pass
            loss.backward()
            
            # Gradient ìˆ˜ì§‘
            if hasattr(layer, 'weight') and layer.weight.grad is not None:
                grad = layer.weight.grad.data
                if hessian is None:
                    hessian = grad ** 2
                else:
                    hessian += grad ** 2
        except Exception as e:
            continue
    
    if hessian is not None:
        hessian = hessian / num_samples
    
    return hessian
