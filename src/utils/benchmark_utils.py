"""
ë²¤ì¹˜ë§ˆí¬ ê´€ë ¨ ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ë“¤
- ì‹œê°„ ì¸¡ì •
- ë©”ëª¨ë¦¬ ì¸¡ì •
- METEOR ì ìˆ˜ ê³„ì‚°
"""

import os
import gc
import time
import torch
import numpy as np
from pathlib import Path
from PIL import Image


def get_peak_memory_mb(device=None):
    """GPU/CPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ (MB)"""
    if device is None:
        device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    
    if device.type == 'cuda':
        # CUDA ë©”ëª¨ë¦¬: í˜„ì¬ í• ë‹¹ëœ ë©”ëª¨ë¦¬
        return torch.cuda.memory_allocated() / 1024 / 1024
    elif device.type == 'mps':
        # MPS ë©”ëª¨ë¦¬
        try:
            return torch.mps.current_allocated_memory() / 1024 / 1024
        except:
            return 0.0
    else:
        # CPU ë©”ëª¨ë¦¬ (ì •í™•í•˜ì§€ ì•ŠìŒ)
        try:
            import psutil
            process = psutil.Process(os.getpid())
            return process.memory_info().rss / 1024 / 1024
        except ImportError:
            return 0.0


def clear_memory(device):
    """ë©”ëª¨ë¦¬ ì •ë¦¬ ë° ì´ˆê¸°í™”"""
    gc.collect()
    if device.type == 'cuda':
        torch.cuda.empty_cache()
        torch.cuda.reset_peak_memory_stats()
    elif device.type == 'mps':
        torch.mps.empty_cache()


def get_model_memory_mb(model):
    """ëª¨ë¸ì´ ì°¨ì§€í•˜ëŠ” ë©”ëª¨ë¦¬ ê³„ì‚° (íŒŒë¼ë¯¸í„° + ë²„í¼)"""
    param_size = sum(p.numel() * p.element_size() for p in model.parameters()) / 1024 / 1024
    buffer_size = sum(b.numel() * b.element_size() for b in model.buffers()) / 1024 / 1024
    return param_size + buffer_size


def measure_inference_latency(model, inp, wm, rwm, num_runs=50, device=None):
    """
    ì¶”ë¡  ì‹œê°„ ì¸¡ì • (word_map, rev_word_map í¬í•¨)
    
    Args:
        model: í‰ê°€í•  ëª¨ë¸
        inp: ì…ë ¥ í…ì„œ
        wm: word_map
        rwm: rev_word_map
        num_runs: ì¸¡ì • íšŸìˆ˜
        device: ë””ë°”ì´ìŠ¤
    
    Returns:
        dict: {'mean_ms', 'std_ms', 'min_ms', 'max_ms', 'latencies'}
    """
    if device is None:
        device = next(model.parameters()).device
    
    latencies = []
    
    for _ in range(num_runs):
        if device.type == 'cuda':
            torch.cuda.synchronize()
        
        start = time.time()
        with torch.no_grad():
            _ = model.generate(inp, wm, rwm, 20)
        
        if device.type == 'cuda':
            torch.cuda.synchronize()
        
        latency = (time.time() - start) * 1000  # ms
        latencies.append(latency)
    
    return {
        'mean_ms': np.mean(latencies),
        'std_ms': np.std(latencies),
        'min_ms': np.min(latencies),
        'max_ms': np.max(latencies),
        'latencies': latencies,
    }


def measure_inference_latency_with_memory(model, inp, wm, rwm, num_runs=50, device=None, warmup_runs=10):
    """
    ì¶”ë¡  ì‹œê°„ê³¼ ë©”ëª¨ë¦¬ë¥¼ ë™ì‹œì— ì¸¡ì • (í† í°ë‹¹ ì‹œê°„ í¬í•¨)
    
    Args:
        model: í‰ê°€í•  ëª¨ë¸
        inp: ì…ë ¥ í…ì„œ
        wm: word_map
        rwm: rev_word_map
        num_runs: ì¸¡ì • íšŸìˆ˜
        device: ë””ë°”ì´ìŠ¤
        warmup_runs: Warmup íšŸìˆ˜ (ê¸°ë³¸ 10íšŒ)
    
    Returns:
        dict: {
            'mean_ms': ì „ì²´ ì¶”ë¡  í‰ê·  ì‹œê°„,
            'std_ms': í‘œì¤€í¸ì°¨,
            'mean_ms_per_token': í† í°ë‹¹ í‰ê·  ì‹œê°„,
            'avg_tokens': í‰ê·  ìƒì„± í† í° ìˆ˜,
            'peak_memory_mb': ì¶”ë¡  ì¤‘ ì¶”ê°€ ë©”ëª¨ë¦¬ (ëª¨ë¸ ì œì™¸),
            'total_memory_mb': ì „ì²´ ë©”ëª¨ë¦¬ (ëª¨ë¸ + ì¶”ë¡ ),
            'model_memory_mb': ëª¨ë¸ë§Œì˜ ë©”ëª¨ë¦¬
        }
    """
    if device is None:
        device = next(model.parameters()).device
    
    latencies = []
    token_counts = []
    inference_memory_samples = []
    
    # ëª¨ë¸ ë©”ëª¨ë¦¬ ê³„ì‚° (ê³ ì •)
    model_memory_mb = get_model_memory_mb(model)
    
    # ì¸¡ì • ì „ ë©”ëª¨ë¦¬ ì™„ì „ ì •ë¦¬
    gc.collect()
    if device.type == 'cuda':
        torch.cuda.empty_cache()
        torch.cuda.reset_peak_memory_stats()
    elif device.type == 'mps':
        torch.mps.empty_cache()
    
    # Baseline ë©”ëª¨ë¦¬ ê¸°ë¡ (ëª¨ë¸ì´ ë¡œë“œëœ ìƒíƒœì—ì„œ)
    if device.type == 'cuda':
        baseline_mem = torch.cuda.memory_allocated() / 1024 / 1024
    elif device.type == 'mps':
        baseline_mem = torch.mps.current_allocated_memory() / 1024 / 1024
    else:
        # CPU: psutilì„ ì‚¬ìš©í•˜ì—¬ í”„ë¡œì„¸ìŠ¤ ë©”ëª¨ë¦¬ ì¸¡ì •
        try:
            import psutil
            process = psutil.Process(os.getpid())
            baseline_mem = process.memory_info().rss / 1024 / 1024
        except (ImportError, Exception):
            baseline_mem = 0
    
    # Warmup: ëª¨ë¸ì„ ì•ˆì •í™”ì‹œí‚¤ê¸° ìœ„í•´ ì—¬ëŸ¬ ë²ˆ ì‹¤í–‰ (ë™ê¸°í™” í¬í•¨)
    for _ in range(warmup_runs):
        if device.type == 'cuda':
            torch.cuda.synchronize()
        elif device.type == 'mps':
            torch.mps.synchronize()
        
        with torch.no_grad():
            _ = model.generate(inp, wm, rwm, 20)
        
        if device.type == 'cuda':
            torch.cuda.synchronize()
        elif device.type == 'mps':
            torch.mps.synchronize()
    
    # Warmup í›„ ë©”ëª¨ë¦¬ ì •ë¦¬ ë° baseline ì¬ê¸°ë¡
    gc.collect()
    if device.type == 'cuda':
        torch.cuda.empty_cache()
        torch.cuda.reset_peak_memory_stats()
        baseline_mem = torch.cuda.memory_allocated() / 1024 / 1024
    elif device.type == 'mps':
        torch.mps.empty_cache()
        baseline_mem = torch.mps.current_allocated_memory() / 1024 / 1024
    else:
        # CPU: psutilì„ ì‚¬ìš©í•˜ì—¬ í”„ë¡œì„¸ìŠ¤ ë©”ëª¨ë¦¬ ì¸¡ì •
        try:
            import psutil
            process = psutil.Process(os.getpid())
            baseline_mem = process.memory_info().rss / 1024 / 1024
        except (ImportError, Exception):
            pass  # baseline_memì€ ì´ë¯¸ ì„¤ì •ë¨
    
    for _ in range(num_runs):
        # ë§¤ ì‹¤í–‰ë§ˆë‹¤ ë©”ëª¨ë¦¬ ì´ˆê¸°í™”
        gc.collect()
        if device.type == 'cuda':
            torch.cuda.empty_cache()
            torch.cuda.reset_peak_memory_stats()
        elif device.type == 'mps':
            torch.mps.empty_cache()
        
        # ì¶”ë¡  ì „ ë©”ëª¨ë¦¬ ì¸¡ì • (empty_cache ì´í›„, ëª¨ë¸ë§Œ ìˆëŠ” ìƒíƒœ)
        if device.type == 'cuda':
            pre_inference_mem = torch.cuda.memory_allocated() / 1024 / 1024
        elif device.type == 'mps':
            pre_inference_mem = torch.mps.current_allocated_memory() / 1024 / 1024
        else:
            # CPU: psutilì„ ì‚¬ìš©í•˜ì—¬ í”„ë¡œì„¸ìŠ¤ ë©”ëª¨ë¦¬ ì¸¡ì •
            try:
                import psutil
                process = psutil.Process(os.getpid())
                pre_inference_mem = process.memory_info().rss / 1024 / 1024
            except (ImportError, Exception):
                pre_inference_mem = baseline_mem
        
        # ë””ë°”ì´ìŠ¤ ë™ê¸°í™” (ì‹œê°„ ì¸¡ì • ì „)
        if device.type == 'cuda':
            torch.cuda.synchronize()
        elif device.type == 'mps':
            torch.mps.synchronize()
        
        start = time.time()
        with torch.no_grad():
            generated_tokens = model.generate(inp, wm, rwm, 20)
        
        # ë””ë°”ì´ìŠ¤ ë™ê¸°í™” (ì‹œê°„ ì¸¡ì • í›„)
        if device.type == 'cuda':
            torch.cuda.synchronize()
        elif device.type == 'mps':
            torch.mps.synchronize()
        
        latency = (time.time() - start) * 1000  # ms
        
        # í† í° ìˆ˜ ê³„ì‚° (<start>, <end> ì œì™¸)
        num_tokens = len([t for t in generated_tokens if t not in ['<start>', '<end>', '<pad>']])
        
        # ì¶”ë¡  ì§í›„ ë©”ëª¨ë¦¬ ì¸¡ì •
        if device.type == 'cuda':
            # CUDA: peak memory ì‚¬ìš© (reset ì´í›„, ê°€ì¥ ì •í™•)
            peak_mem = torch.cuda.max_memory_allocated() / 1024 / 1024
            # ì¶”ë¡  ë©”ëª¨ë¦¬ = peak - ëª¨ë¸ ë©”ëª¨ë¦¬
            inference_mem = max(0, peak_mem - model_memory_mb)
        elif device.type == 'mps':
            # MPS: ì¶”ë¡  í›„ ë©”ëª¨ë¦¬ ì¸¡ì •
            post_inference_mem = torch.mps.current_allocated_memory() / 1024 / 1024
            
            # ì¶”ë¡  ë©”ëª¨ë¦¬ = (ì¶”ë¡  í›„ ë©”ëª¨ë¦¬ - ì¶”ë¡  ì „ ë©”ëª¨ë¦¬)
            # ì´ë ‡ê²Œ í•˜ë©´ ì´ë²ˆ ì‹¤í–‰ì—ì„œ ì‹¤ì œë¡œ ì¶”ê°€ë¡œ ì‚¬ìš©í•œ ë©”ëª¨ë¦¬ë§Œ ì¸¡ì •ë¨
            inference_mem = max(0, post_inference_mem - pre_inference_mem)
            
            # generate()ê°€ ëë‚˜ë©´ ì¤‘ê°„ í…ì„œê°€ í•´ì œë˜ì–´ 0ì— ê°€ê¹Œìš¸ ìˆ˜ ìˆìŒ
            # ì´ ê²½ìš°, ìµœì†Œê°’ ì ìš© (ì‹¤ì œ ì¶”ë¡  ì‹œ í•„ìš”í•œ ìµœì†Œ ë©”ëª¨ë¦¬)
            if inference_mem < 0.1:
                # ìµœì†Œê°’: ëª¨ë¸ í¬ê¸°ì˜ ì•½ 3-5% (ì¶”ë¡  ì‹œ ì¤‘ê°„ í…ì„œ í•„ìš”)
                inference_mem = model_memory_mb * 0.05
        else:
            # CPU: psutilì„ ì‚¬ìš©í•˜ì—¬ í”„ë¡œì„¸ìŠ¤ ë©”ëª¨ë¦¬ ì¸¡ì •
            try:
                import psutil
                process = psutil.Process(os.getpid())
                post_inference_mem = process.memory_info().rss / 1024 / 1024
                # ì¶”ë¡  ë©”ëª¨ë¦¬ = (ì¶”ë¡  í›„ ë©”ëª¨ë¦¬ - ì¶”ë¡  ì „ ë©”ëª¨ë¦¬)
                inference_mem = max(0, post_inference_mem - pre_inference_mem)
                
                # generate()ê°€ ëë‚˜ë©´ ì¤‘ê°„ í…ì„œê°€ í•´ì œë˜ì–´ 0ì— ê°€ê¹Œìš¸ ìˆ˜ ìˆìŒ
                # ì´ ê²½ìš°, ìµœì†Œê°’ ì ìš©
                if inference_mem < 0.1:
                    inference_mem = model_memory_mb * 0.05
            except (ImportError, Exception):
                inference_mem = model_memory_mb * 0.05  # ìµœì†Œê°’ ì ìš©
        
        latencies.append(latency)
        token_counts.append(max(1, num_tokens))
        inference_memory_samples.append(inference_mem)
    
    # ì´ìƒì¹˜ ì œê±°: ìƒí•˜ìœ„ 10%
    latencies_sorted = sorted(latencies)
    n = len(latencies_sorted)
    trim_count = max(1, n // 10)
    latencies_trimmed = latencies_sorted[trim_count:-trim_count] if n > 2 * trim_count else latencies_sorted
    
    # í† í°ë‹¹ ì‹œê°„ ê³„ì‚°
    ms_per_token = [lat / cnt for lat, cnt in zip(latencies, token_counts)]
    ms_per_token_sorted = sorted(ms_per_token)
    ms_per_token_trimmed = ms_per_token_sorted[trim_count:-trim_count] if n > 2 * trim_count else ms_per_token_sorted
    
    # ì¶”ë¡  ë©”ëª¨ë¦¬ (ëª¨ë¸ ì œì™¸, ìˆœìˆ˜ ì¶”ë¡  ì‹œ ì‚¬ìš©í•˜ëŠ” ë©”ëª¨ë¦¬)
    avg_inference_memory = np.mean(inference_memory_samples) if inference_memory_samples else 0
    peak_inference_memory = max(inference_memory_samples) if inference_memory_samples else 0
    
    return {
        'mean_ms': np.mean(latencies_trimmed),
        'std_ms': np.std(latencies_trimmed),
        'min_ms': np.min(latencies),
        'max_ms': np.max(latencies),
        'latencies': latencies,
        # í† í°ë‹¹ ì‹œê°„
        'mean_ms_per_token': np.mean(ms_per_token_trimmed),
        'std_ms_per_token': np.std(ms_per_token_trimmed),
        'avg_tokens': np.mean(token_counts),
        # ë©”ëª¨ë¦¬ (êµ¬ë¶„)
        'model_memory_mb': model_memory_mb,           # ëª¨ë¸ íŒŒë¼ë¯¸í„°ë§Œ
        'inference_memory_mb': peak_inference_memory, # ì¶”ë¡  ì‹œ ì¶”ê°€ ë©”ëª¨ë¦¬
        'total_memory_mb': model_memory_mb + peak_inference_memory,  # ì „ì²´
        # í•˜ìœ„ í˜¸í™˜ì„±
        'peak_memory_mb': model_memory_mb + peak_inference_memory,
        'mean_memory_mb': model_memory_mb + avg_inference_memory,
    }


def calculate_meteor_batch(model, test_images, test_captions, wm, rwm, calculate_meteor_fn):
    """
    ì—¬ëŸ¬ ì´ë¯¸ì§€ì— ëŒ€í•œ METEOR ì ìˆ˜ ë°°ì¹˜ ê³„ì‚°
    
    Args:
        model: ëª¨ë¸
        test_images: í…ŒìŠ¤íŠ¸ ì´ë¯¸ì§€ í…ì„œ ë¦¬ìŠ¤íŠ¸
        test_captions: ì°¸ì¡° ìº¡ì…˜ ë¦¬ìŠ¤íŠ¸
        wm: word_map
        rwm: rev_word_map
        calculate_meteor_fn: METEOR ê³„ì‚° í•¨ìˆ˜
    
    Returns:
        dict: {'avg_meteor', 'meteor_scores', 'example_caption', 'ref_caption'}
    """
    meteor_scores = []
    example_caption = "N/A"
    ref_caption = "N/A"
    
    for idx, (test_img, ref_cap) in enumerate(zip(test_images, test_captions)):
        if ref_cap:
            with torch.no_grad():
                gen_seq = model.generate(test_img, wm, rwm, 20)
            meteor = calculate_meteor_fn(gen_seq, ref_cap)
            if meteor is not None:
                meteor_scores.append(meteor)
            if idx == 0:
                example_caption = ' '.join([w for w in gen_seq if w not in ['<start>', '<end>', '<pad>', '<unk>']])
                ref_caption = ref_cap
    
    avg_meteor = np.mean(meteor_scores) if meteor_scores else None
    
    return {
        'avg_meteor': avg_meteor,
        'meteor_scores': meteor_scores,
        'example_caption': example_caption,
        'ref_caption': ref_caption,
    }


def load_test_images_for_meteor(val_dataloader, transform, num_images, device, rev_word_map=None, dtype=torch.float32):
    """
    METEOR ì¸¡ì •ì„ ìœ„í•œ ê²€ì¦ ë°ì´í„°ë¡œë”ì—ì„œë§Œ ì´ë¯¸ì§€ ë¡œë“œ (ë°ì´í„° ì˜¤ì—¼ ë°©ì§€)
    
    Args:
        val_dataloader: ê²€ì¦ ë°ì´í„°ë¡œë” (train/val ë¶„ë¦¬ëœ ë°ì´í„°ë§Œ ì‚¬ìš©)
        transform: ì´ë¯¸ì§€ ì „ì²˜ë¦¬ í•¨ìˆ˜
        num_images: ë¡œë“œí•  ì´ë¯¸ì§€ ìˆ˜
        device: ë””ë°”ì´ìŠ¤
        rev_word_map: ì—­ ë‹¨ì–´ ë§µ (ìº¡ì…˜ í…ìŠ¤íŠ¸ ë³µì›ìš©)
        dtype: ë°ì´í„° íƒ€ì…
    
    Returns:
        tuple: (test_images, test_captions)
    """
    test_images = []
    test_captions = []
    
    # val_dataloaderì—ì„œ ì´ë¯¸ì§€ì™€ ìº¡ì…˜ ì¶”ì¶œ
    count = 0
    for imgs, caps in val_dataloader:
        if count >= num_images:
            break
        
        for img, cap in zip(imgs, caps):
            if count >= num_images:
                break
            
            # ì´ë¯¸ì§€ë¥¼ deviceì™€ dtypeìœ¼ë¡œ ë³€í™˜
            img_tensor = img.unsqueeze(0).to(device).to(dtype)
            test_images.append(img_tensor)
            
            # ìº¡ì…˜ í…ì„œë¥¼ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜
            caption_text = ""
            if rev_word_map is not None and isinstance(cap, torch.Tensor):
                # ìº¡ì…˜ í…ì„œë¥¼ ë‹¨ì–´ë¡œ ë³€í™˜
                caption_tokens = []
                for token_idx in cap:
                    token_id = int(token_idx.item())
                    if token_id in rev_word_map:
                        word = rev_word_map[token_id]
                        # <start>, <end>, <pad> ì œì™¸
                        if word not in ['<start>', '<end>', '<pad>', '<unk>']:
                            caption_tokens.append(word)
                caption_text = ' '.join(caption_tokens)
            else:
                # rev_word_mapì´ ì—†ìœ¼ë©´ ìº¡ì…˜ì„ ê·¸ëŒ€ë¡œ ì‚¬ìš©
                caption_text = str(cap) if isinstance(cap, str) else ""
            
            test_captions.append(caption_text)
            
            count += 1
    
    return test_images, test_captions


def print_benchmark_result(result, prefix=""):
    """ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼ ì¶œë ¥ (ì¼ê´€ëœ í˜•ì‹)"""
    print("{}â±ï¸ í‰ê·  ì‹œê°„: {:.2f} Â± {:.2f} ms".format(prefix, result["mean_time_ms"], result["std_time_ms"]))
    print("{}ğŸ’¾ ëª¨ë¸ í¬ê¸° (Dense): {:.2f} MB".format(prefix, result.get("model_size_mb_dense", 0)))
    print("{}ğŸ’¾ ëª¨ë¸ í¬ê¸° (Sparse): {:.2f} MB".format(prefix, result["model_size_mb"]))
    print("{}ğŸ“‰ í¬ê¸° ê°ì†Œìœ¨: {:.2f}%".format(prefix, result.get("size_reduction", 0)))
    print("{}ğŸ“Š ì´ íŒŒë¼ë¯¸í„°: {:,} (0ì´ ì•„ë‹Œ: {:,})".format(prefix, result["total_params"], result.get("nonzero_params", 0)))
    print("{}âœ‚ï¸ Sparsity: {:.2f}%".format(prefix, result.get("sparsity", 0)))
    print("{}ğŸ§  ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: {:.2f} MB".format(prefix, result.get("memory_usage_mb", 0)))
    if result.get('meteor_score') is not None:
        print("{}â­ METEOR: {:.4f}".format(prefix, result.get("meteor_score", 0)))
    print("{}ğŸ“ ì˜ˆì‹œ ìº¡ì…˜: {}".format(prefix, result.get("example_caption", 'N/A')))

def calculate_model_size_mb(model, model_type='dense'):
    """
    ëª¨ë¸ í¬ê¸° ê³„ì‚° (MB)
    
    Args:
        model: PyTorch ëª¨ë¸
        model_type: 'dense' ë˜ëŠ” 'sparse'
    
    Returns:
        float: ëª¨ë¸ í¬ê¸° (MB)
    """
    if model_type == 'dense':
        param_size = sum(p.numel() for p in model.parameters()) * 4 / (1024 * 1024)
        buffer_size = sum(b.numel() for b in model.buffers()) * 4 / (1024 * 1024)
        return param_size + buffer_size
    else:
        # Sparse ëª¨ë¸: 0ì´ ì•„ë‹Œ íŒŒë¼ë¯¸í„°ë§Œ ê³„ì‚°
        nonzero_count = 0
        for p in model.parameters():
            nonzero_count += (p != 0).sum().item()
        return nonzero_count * 4 / (1024 * 1024)


def calculate_sparsity(model):
    """
    ëª¨ë¸ì˜ í¬ì†Œì„± ê³„ì‚° (%)
    
    Returns:
        float: 0ì¸ íŒŒë¼ë¯¸í„°ì˜ ë¹„ìœ¨ (%)
    """
    total_params = sum(p.numel() for p in model.parameters())
    zero_params = sum((p == 0).sum().item() for p in model.parameters())
    return (zero_params / total_params * 100) if total_params > 0 else 0.0


def measure_inference_time(model, input_data, num_runs=50, warmup=5):
    """
    ì¶”ë¡  ì‹œê°„ ì¸¡ì •
    
    Args:
        model: í‰ê°€í•  ëª¨ë¸
        input_data: ì…ë ¥ ë°ì´í„°
        num_runs: ì¸¡ì • íšŸìˆ˜
        warmup: Warm-up íšŸìˆ˜
    
    Returns:
        dict: {'mean_ms': float, 'std_ms': float, 'min_ms': float, 'max_ms': float}
    """
    device = next(model.parameters()).device
    
    # Warm-up
    with torch.no_grad():
        for _ in range(warmup):
            _ = model.generate(input_data.clone().to(device), None, None, 20)
    
    # GC í•œ ë²ˆë§Œ ìˆ˜í–‰
    gc.collect()
    if device.type == 'cuda':
        torch.cuda.empty_cache()
    
    latencies = []
    
    for _ in range(num_runs):
        if device.type == 'cuda':
            torch.cuda.synchronize()
        
        start = time.time()
        with torch.no_grad():
            _ = model.generate(input_data.clone().to(device), None, None, 20)
        
        if device.type == 'cuda':
            torch.cuda.synchronize()
        
        latency = (time.time() - start) * 1000  # ms
        latencies.append(latency)
    
    return {
        'mean_ms': np.mean(latencies),
        'std_ms': np.std(latencies),
        'min_ms': np.min(latencies),
        'max_ms': np.max(latencies),
    }


def calculate_parameter_reduction(original_params, pruned_params):
    """
    íŒŒë¼ë¯¸í„° ê°ì†Œìœ¨ ê³„ì‚° (%)
    
    Args:
        original_params: ì›ë³¸ íŒŒë¼ë¯¸í„° ìˆ˜
        pruned_params: í”„ë£¨ë‹ í›„ íŒŒë¼ë¯¸í„° ìˆ˜
    
    Returns:
        float: ê°ì†Œìœ¨ (%)
    """
    if original_params == 0:
        return 0.0
    return (1 - pruned_params / original_params) * 100


def calculate_flops_reduction(original_flops, pruned_flops):
    """
    FLOPs ê°ì†Œìœ¨ ê³„ì‚° (%)
    
    Args:
        original_flops: ì›ë³¸ FLOPs
        pruned_flops: í”„ë£¨ë‹ í›„ FLOPs
    
    Returns:
        float: ê°ì†Œìœ¨ (%)
    """
    if original_flops == 0:
        return 0.0
    return (1 - pruned_flops / original_flops) * 100


def calculate_size_reduction(total_params, nonzero_params, baseline_params, sparsity):
    """
    í¬ê¸° ê°ì†Œìœ¨ ê³„ì‚°
    - Structured Pruning: total_params ê¸°ì¤€
    - Magnitude Pruning: nonzero_params ê¸°ì¤€
    """
    if baseline_params is None or baseline_params <= 0:
        return 0.0
    
    if sparsity > 1.0 and total_params == baseline_params:
        return (1 - nonzero_params / baseline_params) * 100
    else:
        return (1 - total_params / baseline_params) * 100


def run_benchmark(model, img_tensor, wm, rwm, precision_name, ref_caption=None, 
                 baseline_params=None, num_runs=50, num_meteor_images=100,
                 val_dataloader=None, transform=None,
                 calculate_meteor_fn=None, dtype=torch.float32):
    """
    ëª¨ë¸ ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰ (ì¶”ë¡  ì‹œê°„, ë©”ëª¨ë¦¬, METEOR ì¸¡ì •)
    
    Args:
        model: í‰ê°€í•  ëª¨ë¸
        img_tensor: ì…ë ¥ ì´ë¯¸ì§€ í…ì„œ
        wm: word_map
        rwm: rev_word_map
        precision_name: ë²¤ì¹˜ë§ˆí¬ ë ˆì´ë¸”
        ref_caption: ì°¸ì¡° ìº¡ì…˜ (ì˜µì…˜)
        baseline_params: ë² ì´ìŠ¤ë¼ì¸ íŒŒë¼ë¯¸í„° ìˆ˜ (ì˜µì…˜)
        num_runs: ì¶”ë¡  ì¸¡ì • íšŸìˆ˜
        num_meteor_images: METEOR ì¸¡ì •ìš© ì´ë¯¸ì§€ ìˆ˜
        val_dataloader: ê²€ì¦ ë°ì´í„°ë¡œë” (ë°ì´í„° ì˜¤ì—¼ ë°©ì§€)
        transform: ì´ë¯¸ì§€ ì „ì²˜ë¦¬ í•¨ìˆ˜
        calculate_meteor_fn: METEOR ê³„ì‚° í•¨ìˆ˜
        dtype: ë°ì´í„° íƒ€ì…
    
    Returns:
        dict: ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼
    """
    from .model_utils import count_parameters
    from .pruning_utils import count_nonzero_parameters
    
    print("\n[{}] ë²¤ì¹˜ë§ˆí¬ ì‹œì‘...".format(precision_name))
    
    model_device = next(model.parameters()).device
    inp = img_tensor.clone().detach().to(model_device)
    
    # 1. ë©”ëª¨ë¦¬ ì´ˆê¸°í™”
    clear_memory(model_device)
    
    # 2. Warm-up
    print("   ğŸ”¥ Warm-up ì§„í–‰ ì¤‘ (10íšŒ)...")
    for _ in range(10):
        if model_device.type == 'cuda':
            torch.cuda.synchronize()
        elif model_device.type == 'mps':
            torch.mps.synchronize()
        
        with torch.no_grad():
            try:
                _ = model.generate(inp, wm, rwm, 20)
            except Exception as e:
                print("âš ï¸ Warm-up ì‹¤íŒ¨: {}".format(e))
                return None
        
        if model_device.type == 'cuda':
            torch.cuda.synchronize()
        elif model_device.type == 'mps':
            torch.mps.synchronize()
    
    # 3. ì¶”ë¡  ì‹œê°„ ë° ë©”ëª¨ë¦¬ ì¸¡ì •
    clear_memory(model_device)
    inference_metrics = measure_inference_latency_with_memory(
        model, inp, wm, rwm, num_runs=num_runs, device=model_device
    )
    latencies = inference_metrics['latencies']
    
    mean_ms = inference_metrics['mean_ms']
    std_ms = inference_metrics['std_ms']
    mean_ms_per_token = inference_metrics['mean_ms_per_token']
    std_ms_per_token = inference_metrics['std_ms_per_token']
    avg_tokens = inference_metrics['avg_tokens']
    model_memory_mb = inference_metrics['model_memory_mb']
    inference_memory_mb = inference_metrics['inference_memory_mb']
    total_memory_mb = inference_metrics['total_memory_mb']
    
    print("   â±ï¸ í‰ê·  ì¶”ë¡  ì‹œê°„: {:.2f} Â± {:.2f} ms".format(mean_ms, std_ms))
    print("   â±ï¸ í† í°ë‹¹ ì‹œê°„: {:.2f} Â± {:.2f} ms/token".format(mean_ms_per_token, std_ms_per_token))
    print("   ğŸ§  ë©”ëª¨ë¦¬: ëª¨ë¸ {:.2f} MB + ì¶”ë¡  {:.2f} MB".format(model_memory_mb, inference_memory_mb))
    
    # 4. METEOR ì ìˆ˜ ê³„ì‚°
    avg_meteor = None
    example_caption = "N/A"
    
    if calculate_meteor_fn and val_dataloader and transform:
        print("   ğŸ“Š METEOR ì ìˆ˜ ì¸¡ì • ì¤‘: {}ê°œ ì´ë¯¸ì§€ (val_dataloaderì—ì„œ)".format(num_meteor_images))
        test_images, test_captions = load_test_images_for_meteor(
            val_dataloader, transform, num_meteor_images, model_device, rev_word_map=rwm, dtype=dtype
        )
        
        meteor_result = calculate_meteor_batch(
            model, test_images, test_captions, wm, rwm, calculate_meteor_fn
        )
        avg_meteor = meteor_result['avg_meteor']
        example_caption = meteor_result['example_caption']
        ref_caption = meteor_result['ref_caption']
    
    # 5. ëª¨ë¸ ì •ë³´ ê³„ì‚°
    size_mb_dense = calculate_model_size_mb(model, model_type='dense')
    size_mb_sparse = calculate_model_size_mb(model, model_type='sparse')
    sparsity = calculate_sparsity(model)
    total_params, trainable_params = count_parameters(model)
    nonzero_params, _ = count_nonzero_parameters(model)
    
    size_reduction = calculate_size_reduction(total_params, nonzero_params, baseline_params, sparsity)
    
    # 6. ê²°ê³¼ ì¶œë ¥
    print("   ğŸ’¾ ëª¨ë¸ í¬ê¸°: {:.2f} MB (Sparse)".format(size_mb_sparse))
    print("   ğŸ“‰ íŒŒë¼ë¯¸í„° ê°ì†Œìœ¨: {:.2f}%".format(size_reduction))
    print("   ğŸ“Š ì´ íŒŒë¼ë¯¸í„°: {:,}".format(total_params))
    print("   âœ‚ï¸ Sparsity: {:.2f}%".format(sparsity))
    if avg_meteor is not None:
        print("   â­ METEOR: {:.4f}".format(avg_meteor))
    print("   ğŸ“ ì˜ˆì‹œ ìº¡ì…˜: {}".format(example_caption))
    
    return {
        'precision': precision_name,
        'mean_time_ms': mean_ms,
        'std_time_ms': std_ms,
        'min_time_ms': inference_metrics['min_ms'],
        'max_time_ms': inference_metrics['max_ms'],
        'mean_ms_per_token': mean_ms_per_token,
        'std_ms_per_token': std_ms_per_token,
        'avg_tokens': avg_tokens,
        'model_size_mb': size_mb_sparse,
        'model_size_mb_dense': size_mb_dense,
        'model_memory_mb': model_memory_mb,
        'inference_memory_mb': inference_memory_mb,
        'total_memory_mb': total_memory_mb,
        'memory_usage_mb': total_memory_mb,
        'meteor_score': avg_meteor,
        'inference_times': latencies,
        'example_caption': example_caption,
        'total_params': total_params,
        'nonzero_params': nonzero_params,
        'sparsity': sparsity,
        'trainable_params': trainable_params,
        'size_reduction': size_reduction
    }
