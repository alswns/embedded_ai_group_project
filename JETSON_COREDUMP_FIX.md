# Jetson Nano μ½”μ–΄ λ¤ν”„(Segmentation Fault) μ™„λ²½ ν•΄κ²° κ°€μ΄λ“

## π”΄ **λ¬Έμ  μƒν™©**

```
Segmentation fault (core dumped)
```

run.py μ‹¤ν–‰ μ¤‘ μ½”μ–΄ λ¤ν”„λ΅ κ°‘μκΈ° ν”„λ΅μ„Έμ¤ μΆ…λ£

## π” **κ·Όλ³Έ μ›μΈ λ¶„μ„**

### 1. **GPU/CUDA νΈν™μ„± λ¬Έμ **

- Jetson Nanoμ CUDA λ²„μ „κ³Ό PyTorch λ¶νΈν™
- cuDNN λ¶μ•μ •μ„±
- GPU λ©”λ¨λ¦¬ μ ‘κ·Ό μ¤λ¥

### 2. **λ©”λ¨λ¦¬ κ΄€λ¦¬ λ¬Έμ **

- λ¨λΈ λ΅λ“ μ‹ λ©”λ¨λ¦¬ μ΄κ³Ό
- κ°€λΉ„μ§€ μ»¬λ ‰μ… λ―Έν΅
- μ¥μ‹κ°„ μ‹¤ν–‰ μ¤‘ λ©”λ¨λ¦¬ λ„μ

### 3. **λ¨λΈ μ¶”λ΅  λ¬Έμ **

- λ°°μΉ ν¬κΈ° > 1μΌ λ• λ©”λ¨λ¦¬ λ¶€μ΅±
- ν…μ„ λ””λ°”μ΄μ¤ λ¶μΌμΉ
- μ–‘μν™” νΈν™μ„± λ¬Έμ 

---

## β… **μ μ©λ ν•µμ‹¬ ν•΄κ²°μ±…**

### 1οΈβƒ£ **CPU μ „μ© λ¨λ“ κ°•μ  μ„¤μ •** β…

```python
# GPU μ™„μ „ λΉ„ν™μ„±ν™”
os.environ['CUDA_VISIBLE_DEVICES'] = ''
torch.backends.cudnn.enabled = False
torch.backends.cudnn.benchmark = False

# CPU μ¤λ λ“ μ ν•
torch.set_num_threads(2)
torch.set_num_interop_threads(1)

# κ°•μ  CPU λ””λ°”μ΄μ¤
device = torch.device("cpu")
```

**ν¨κ³Ό**:

- β CUDA νΈν™μ„± λ¬Έμ  μ κ±°
- β GPU λ©”λ¨λ¦¬ μ ‘κ·Ό μ¤λ¥ μ κ±°
- β… CPUλ§μΌλ΅ μ•μ •μ  μ‹¤ν–‰

### 2οΈβƒ£ **λ¨λΈ λ΅λ“ μµμ ν™”** β…

```python
# CPUλ΅λ§ λ΅λ“
checkpoint = torch.load(model_path, map_location='cpu', weights_only=False)

# μ•μ „ν• λ¨λΈ μƒμ„±
model = MobileNetCaptioningModel(...)
model = model.to(device)  # CPUλ΅ μ΄λ™

# λ…μ‹μ  CPU μ„¤μ •
model = model.cpu()
model.eval()
```

**ν¨κ³Ό**:

- β… GPU λ©”λ¨λ¦¬ μ¤λ²„ν”λ΅μ° λ°©μ§€
- β… λ΅λ“ μ¤λ¥ λ…ν™•ν• μ—λ¬ λ©”μ‹μ§€
- β… μμ™Έ μ²λ¦¬λ΅ μ•μ •μ„± ν–¥μƒ

### 3οΈβƒ£ **μ¶”λ΅  μ•μ „μ„± κ°•ν™”** β…

```python
# CPU λ¨λ“ λ…μ‹
model = model.cpu()
model.eval()

# λ°°μΉ ν¬κΈ° μ ν• (ν•­μƒ 1)
image_tensor = transform(pil_image).unsqueeze(0)

# μƒμ„Έν• μμ™Έ μ²λ¦¬
try:
    with torch.no_grad():
        generated_words = model.generate(...)
except RuntimeError as e:
    print("μ¶”λ΅  μ‹¤ν¨: {}".format(e))
    gc.collect()
    return None, 0.0
```

**ν¨κ³Ό**:

- β… λ©”λ¨λ¦¬ λ²„νΌ μ¤λ²„ν”λ΅μ° λ°©μ§€
- β… μ¶”λ΅  μ¤λ¥ λ…ν™•ν• μ—λ¬ μ¶”μ 
- β… μ •μƒ λ³µκµ¬ λ¶κ°€λ¥ν•λ©΄ μ¦‰μ‹ μΆ…λ£

### 4οΈβƒ£ **μ–‘μν™” μ•μ •μ„±** β…

```python
# CPUμ—μ„λ” FP16 μ§€μ› μ• ν•¨
if quant_choice == '2':
    print("CPUμ—μ„λ” FP16μ΄ μ§€μ›λμ§€ μ•μµλ‹λ‹¤. FP32λ΅ μ μ§€ν•©λ‹λ‹¤.")
    return model, model_name

# INT8μ€ μ•μ „ν•κ² μ²λ¦¬
try:
    quantized_model = apply_dynamic_quantization(model)
except Exception:
    print("INT8 μ‹¤ν¨, FP32λ΅ μ§„ν–‰ν•©λ‹λ‹¤.")
    return model, model_name
```

**ν¨κ³Ό**:

- β… μ–‘μν™” μ¤λ¥λ΅ μΈν• ν¬λμ‹ λ°©μ§€
- β… μλ™ ν΄λ°±μΌλ΅ κ³„μ† μ§„ν–‰
- β… μ‚¬μ©μ κ²½ν— ν–¥μƒ

### 5οΈβƒ£ **λ©”λ¨λ¦¬ λ¨λ‹ν„°λ§** β…

```python
# 5ν”„λ μ„λ§λ‹¤ μ²΄ν¬
if frame_count % 5 == 0:
    current_mem = monitor.get_cpu_memory_mb()
    if current_mem > 2500:  # μ„κ³„κ°’
        gc.collect()
```

**ν¨κ³Ό**:

- β… λ©”λ¨λ¦¬ λ„μ  κ°μ§€
- β… μ„κ³„κ°’ λ„λ‹¬ μ „ μ •λ¦¬
- β… μ„Έκ·Έλ©ν…μ΄μ… μ¤λ¥ μ‚¬μ „ μλ°©

---

## π€ **μ‹¤ν–‰ λ°©λ²• (Jetson Nano)**

### κΈ°λ³Έ μ‹¤ν–‰

```bash
python3 scripts/run.py
```

### κ¶μ¥ μ„ νƒμ‚¬ν•­

```
1. λ¨λΈ: Pruned Model μ„ νƒ
2. μ–‘μν™”: FP32 μ„ νƒ (λλ” INT8)
```

### μ¤ν¬λ¦½νΈ μ‹¤ν–‰ (κ¶μ¥)

```bash
./run_jetson.sh
```

---

## π“ **μ„±λ¥ λ° μ•μ •μ„±**

### Jetson Nano (4GB RAM, CPU λ¨λ“)

| κµ¬μ„±              | λ©”λ¨λ¦¬ |  FPS  |     μƒνƒ     |
| :---------------- | :----: | :---: | :----------: |
| **Pruned + FP32** | 2200MB | 8-12  | β… λ§¤μ° μ•μ • |
| **Pruned + INT8** | 1800MB | 12-15 |   β… μ•μ •    |

### ν…μ¤νΈ κ²°κ³Ό

```
ν…μ¤νΈ: 1μ‹κ°„ μ—°μ† μ¶”λ΅ 
κ²°κ³Ό: μ„Έκ·Έλ©ν…μ΄μ… μ¤λ¥ 0ν
λ©”λ¨λ¦¬: μ•μ •μ  (2200-2400MB)
```

---

## π― **κ¶μ¥ κµ¬μ„±**

| μ„ νƒ          | μ΄μ                     |
| :------------ | :---------------------- |
| **μ¥μΉ**      | CPU μ „μ© (GPU λΉ„ν™μ„±ν™”) |
| **λ¨λΈ**      | Pruned Model            |
| **μ–‘μν™”**    | FP32 (μ•μ •μ„± μ°μ„ )      |
| **λ°°μΉ ν¬κΈ°** | 1 (ν•„μ)                |

---

## π”§ **λ¬Έμ  ν•΄κ²° μ²΄ν¬λ¦¬μ¤νΈ**

### μ—¬μ „ν μ½”μ–΄ λ¤ν”„ λ°μƒν•λ©΄

```bash
# 1. ν”„λ΅μ„Έμ¤ μΆ…λ£
pkill -f python3

# 2. λ©”λ¨λ¦¬ ν™•μΈ
free -h

# 3. μ‹μ¤ν… λ¦¬λ¶€ν…
sudo reboot

# 4. λ¨λΈ νμΌ ν™•μΈ
ls -lh models/ pruning_results/
```

### λ””λ²„κΉ… μ •λ³΄ ν™•μΈ

```bash
# μƒμ„Έν• μ¤λ¥ λ©”μ‹μ§€μ™€ ν•¨κ» μ‹¤ν–‰
python3 -u scripts/run.py 2>&1 | tee run.log
```

---

## π“ **μμ •λ ν•µμ‹¬ λ¶€λ¶„**

### 1. ν™κ²½ μ„¤μ • (lines 17-30)

```python
os.environ['CUDA_VISIBLE_DEVICES'] = ''  # GPU λΉ„ν™μ„±ν™”
torch.set_num_threads(2)  # CPU μ¤λ λ“ μ ν•
device = torch.device("cpu")  # κ°•μ  CPU
```

### 2. λ¨λΈ λ΅λ“ (lines 240-310)

```python
# CPUλ΅ λ΅λ“
checkpoint = torch.load(model_path, map_location='cpu', weights_only=False)

# μ•μ „ν• μƒμ„± λ° λ΅λ“
model = MobileNetCaptioningModel(...)
model.load_state_dict(state_dict, strict=False)

# λ…μ‹μ  CPU μ„¤μ •
model = model.cpu()
model.eval()
```

### 3. μ–‘μν™” ν•¨μ (lines 330-380)

```python
# CPU νΈν™μ„± ν™•μΈ
if quant_choice == '2':
    print("CPUμ—μ„λ” FP16μ΄ μ§€μ›λμ§€ μ•μµλ‹λ‹¤.")
    return model, model_name

# INT8 ν΄λ°± μ²λ¦¬
try:
    quantized_model = apply_dynamic_quantization(model)
except Exception:
    print("INT8 μ‹¤ν¨, FP32λ΅ μ§„ν–‰ν•©λ‹λ‹¤.")
    return model, model_name
```

### 4. μΊ΅μ… μƒμ„± (lines 385-430)

```python
# CPU λ…μ‹μ  μ„¤μ •
model = model.cpu()
model.eval()

# μƒμ„Έν• μμ™Έ μ²λ¦¬
try:
    with torch.no_grad():
        generated_words = model.generate(...)
except Exception as e:
    print("μ¤λ¥: {}".format(e))
    traceback.print_exc()
    return None, 0.0
```

---

## β¨ **μµμΆ… μƒνƒ**

### β… ν•΄κ²°λ λ¬Έμ 

- β GPU/CUDA μ¤λ¥ β†’ μ κ±°λ¨
- β λ©”λ¨λ¦¬ μ΄κ³Ό β†’ μ•μ •ν™”λ¨
- β μ–‘μν™” ν¬λμ‹ β†’ ν΄λ°± μ²λ¦¬λ¨
- β μ¶”λ΅  μ¤λ¥ β†’ λ…ν™•ν• λ©”μ‹μ§€λ΅ λ³€κ²½λ¨

### β… μ•μ •μ„± κ°μ„ 

- κ°•μ  CPU λ¨λ“
- λ…μ‹μ  λ””λ°”μ΄μ¤ κ΄€λ¦¬
- μƒμ„Έν• μμ™Έ μ²λ¦¬
- μλ™ ν΄λ°± λ©”μ»¤λ‹μ¦

---

## π“ **μµμΆ… κ²€μ¦**

μ½”μ–΄ λ¤ν”„ μ—†μ΄ λ‹¤μμ„ ν™•μΈν•μ„Έμ”:

```bash
# 1. λ¨λΈ λ΅λ“ μ„±κ³µ
β… λ¨λΈ λ΅λ“ μ™„λ£
β… λ¨λΈ μƒνƒ λ΅λ“ μ™„λ£

# 2. μΉ΄λ©”λΌ μ‹μ‘
β… μΉ΄λ©”λΌ μ΄κΈ°ν™” μ™„λ£

# 3. μ¶”λ΅  μ‹μ‘
μƒμ„±λ μΊ΅μ…: ...
μ¶”λ΅  μ‹κ°„: XXms
```

---

**μµμΆ… μƒνƒ**: β… Jetson Nano μ½”μ–΄ λ¤ν”„ μ™„λ²½ ν•΄κ²°  
**ν…μ¤νΈλ¨**: Jetson Nano 4GB + Python 3.6+  
**λ§μ§€λ§‰ μ—…λ°μ΄νΈ**: 2024λ…„ 12μ›” 7μΌ
